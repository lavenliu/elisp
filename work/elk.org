* ELK Stack
  官方文档：[[https://www.elastic.co/guide/index.html][ES官方指导文档]]
* 日志平台概述
  + 开发人员不能登录线上服务器查看详细日志
  + 各个系统都有日志，日志数据分散难以查找
  + 日志数据量大，查询速度慢，或者数据不够实时
  + 一个调用会涉及多个系统，难以在这些系统的日志中快速定位数据
* ELK Stack介绍
  ElasticSearch + LogStash + Kibana = ELK Stack
* ElasticSearch详解
  Elasticsearch is a highly scalable open-source full-text search and
  analytics engine. It allows you to store, search, and analyze big
  volumes of data quickly and in near real time.
** 基本概念
   There are a few concepts that are core to
   Elasticsearch. Understanding these concepts from the outset will
   tremendously help ease the learning process. 我的翻译：
   ElasticSearch有一些核心的概念，从一开始理解这些概念，可以极大地提
   高我们学习ElasticSearch的进度。
   
   下面是核心的概念：
   1. Near Realtime (NRT)
	  #+BEGIN_EXAMPLE
	  ElasticSearch is a near real time search platform. What this
	  means is there is a slight latency (normally one second) from
	  the time you index a document until the time it becomes
	  searchable.  [近乎实时，一秒延迟]
	  #+END_EXAMPLE
   2. Cluster 
	  #+BEGIN_EXAMPLE
	  A cluster is a collection of one or more nodes (servers) that
	  together holds your entire data and provides federated indexing
	  and search capabilities across all nodes. A cluster is
	  identified by a unique name which by default is
	  "elasticsearch". This name is important because a node can only
	  be part of a cluster if the node is set up to join the cluster
	  by its name.

	  Make sure that you don’t reuse the same cluster names in different environments,

	  Note that it is valid and perfectly fine to have a cluster with 
	  only a single node in it. Furthermore, you may also have multiple 
	  independent clusters each with its own unique cluster name.
	  #+END_EXAMPLE
   3. Node
	  #+BEGIN_EXAMPLE
	  A node is a single server that is part of your cluster, stores
	  your data, and participates in the cluster’s indexing and
	  search capabilities. Just like a cluster, a node is identified
	  by a name which by default is a random Marvel character name
	  that is assigned to the node at startup. You can define any
	  node name you want if you do not want the default. This name is
	  important for administration purposes where you want to
	  identify which servers in your network correspond to which
	  nodes in your Elasticsearch cluster.

	  A node can be configured to join a specific cluster by the
	  cluster name. By default, each node is set up to join a cluster
	  named elasticsearch which means that if you start up a number
	  of nodes on your network and—assuming they can discover each
	  other—they will all automatically form and join a single
	  cluster named elasticsearch.

	  In a single cluster, you can have as many nodes as you
	  want. Furthermore, if there are no other Elasticsearch nodes
	  currently running on your network, starting a single node will
	  by default form a new single-node cluster named elasticsearch.
	  #+END_EXAMPLE
   4. Index
	  #+BEGIN_EXAMPLE
	  An index is a collection of documents that have somewhat
	  similar characteristics. For example, you can have an index for
	  customer data, another index for a product catalog, and yet
	  another index for order data. An index is identified by a name
	  (that must be all lowercase) and this name is used to refer to
	  the index when performing indexing, search, update, and delete
	  operations against the documents in it.

	  In a single cluster, you can define as many indexes as you
	  want.
	  #+END_EXAMPLE
   5. Type
	  #+BEGIN_EXAMPLE
	  Within an index, you can define one or more types. A type is a
	  logical category/partition of your index whose semantics is
	  completely up to you. In general, a type is defined for
	  documents that have a set of common fields. For example, let’s
	  assume you run a blogging platform and store all your data in a
	  single index. In this index, you may define a type for user
	  data, another type for blog data, and yet another type for
	  comments data.
	  #+END_EXAMPLE
   6. Document
	  #+BEGIN_EXAMPLE
	  A document is a basic unit of information that can be
	  indexed. For example, you can have a document for a single
	  customer, another document for a single product, and yet
	  another for a single order. This document is expressed in JSON
	  (JavaScript Object Notation) which is an ubiquitous internet
	  data interchange format.

	  Within an index/type, you can store as many documents as you
	  want. Note that although a document physically resides in an
	  index, a document actually must be indexed/assigned to a type
	  inside an index.
	  #+END_EXAMPLE
   7. Shards & Replicas
	  #+BEGIN_EXAMPLE
	  An index can potentially store a large amount of data that can
	  exceed the hardware limits of a single node. For example, a
	  single index of a billion documents taking up 1TB of disk space
	  may not fit on the disk of a single node or may be too slow to
	  serve search requests from a single node alone.

	  To solve this problem, Elasticsearch provides the ability to
	  subdivide your index into multiple pieces called shards. When
	  you create an index, you can simply define the number of shards
	  that you want. Each shard is in itself a fully-functional and
	  independent "index" that can be hosted on any node in the
	  cluster.

	  Sharding is important for two primary reasons:
	  + It allows you to horizontally split/scale your content volume
	  + It allows you to distribute and parallelize operations across 
		shards (potentially on multiple nodes) thus increasing 
		performance/throughput

	  The mechanics of how a shard is distributed and also how its
	  documents are aggregated back into search requests are completely
	  managed by Elasticsearch and is transparent to you as the user.

	  In a network/cloud environment where failures can be expected anytime,
	  it is very useful and highly recommended to have a failover mechanism
	  in case a shard/node somehow goes offline or disappears for whatever
	  reason. To this end, Elasticsearch allows you to make one or more
	  copies of your index’s shards into what are called replica shards, or
	  replicas for short.

	  Replication is important for two primary reasons:

	  + It provides high availability in case a shard/node fails. 
		For this reason, it is important to note that a replica shard 
		is never allocated on the same node as the original/primary shard 
		that it was copied from.
	  + It allows you to scale out your search volume/throughput since 
		searches can be executed on all replicas in parallel. 

	  To summarize, each index can be split into multiple shards. An
	  index can also be replicated zero (meaning no replicas) or more
	  times. Once replicated, each index will have primary shards (the
	  original shards that were replicated from) and replica
	  shards (the copies of the primary shards). The number of shards
	  and replicas can be defined per index at the time the index is
	  created. After the index is created, you may change the number of
	  replicas dynamically anytime but you cannot change the number
	  shards after-the-fact.

	  By default, each index in Elasticsearch is allocated 5 primary
	  shards and 1 replica which means that if you have at least two
	  nodes in your cluster, your index will have 5 primary shards and
	  another 5 replica shards (1 complete replica) for a total of 10
	  shards per index.
	  #+END_EXAMPLE
	  
   + 索引：ES把数据存放在一个或多个为索引中
   + 文档：索引中存放的基本单位为文档
   + 分片：ES会把索引分片，可以横向扩展，也可以备份分片
   + 节点：一个ES实例就是一个节点
   + 集群：多个ES实例组成一个集群
** 安装
	ElasticSearch需要Java的环境。Java版本建议是1.8.0_25以上的版本，而
	且是Oracle的JDK。
    |       | 主机                    |             IP | 备注  |
    |-------+-------------------------+----------------+-------|
    | kvm01 | linux-node1.example.com | 192.168.20.141 | node1 |
    | kvm02 | linux-node2.example.com | 192.168.20.138 | node2 | 

	参考文档：[[https://www.elastic.co/guide/en/elasticsearch/guide/current/index.html][ES官方文档]]

	如果集群不想使用组播的方式，可以使用单播的方式进行通信。可以禁用多
	播的方式，具体修改ES的配置文件，
	#+BEGIN_SRC sh
# 第321行，取消注释
discovery.zen.ping.multicast.enabled: false
# 取消第326行注释，把集群中的机器添加到该列表中
discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]
	#+END_SRC
* 如何使用ES - ElasticSearch权威指南
   ES工作原理：
   默认使用多播进行节点发现，相同的多播组会组成一个集群。集群当中会选举主节点，

   1. Java API
	  + node client
	  + Transport client
   2. RESTful API
	  + Javascript
	  + .NET
	  + PHP
	  + Perl
	  + Python
	  + Ruby

   通过curl使用示例，
   #+BEGIN_SRC sh
curl -X<VERB> '<PROTOOL>://<HOST>/<PATH>?<QUERY_STRING>' -d '<BODY>'
+ VERB HTTP方法：GET,POST,PUT,HEAD,DELETE
+ PROTOCOL http或者https协议（只有在ElasticSearch使用https代理的时候可用）
+ HOST ElasticSearch集群中的任何一个节点的主机名，如果是在本地的节点，则为localhost
+ PORT ElasticSearch HTTP服务绑定的端口，默认为9200
+ QUERY_STRING 一些可选的查询请求参数，例如?pretty参数将使请求返回更加美观易读的JSON数据
+ BODY 一个JSON格式的请求主题（如果请求需要的话）
   #+END_SRC

   #+BEGIN_SRC sh
GET /_nodes/process
   #+END_SRC
得到：
#+BEGIN_SRC sh
{
   "cluster_name": "lavenliu",
   "nodes": {
      "YJvvthugQY-r-TSwbkoh7Q": {
         "name": "linux-node01",
         "transport_address": "inet[/192.168.19.130:9300]",
         "host": "gluster01.lavenliu.com",
         "ip": "192.168.20.141",
         "version": "1.7.0",
         "build": "929b973",
         "http_address": "inet[/192.168.19.130:9200]",
         "attributes": {
            "master": "true"
         },
         "process": {
            "refresh_interval_in_millis": 1000,
            "id": 19570,
            "max_file_descriptors": 4096,
            "mlockall": true
         }
      },
      "YMaBj3xDRNuuSBDWQwFAKg": {
         "name": "linux-node02",
         "transport_address": "inet[/192.168.19.135:9300]",
         "host": "gluster02.lavenliu.com",
         "ip": "192.168.20.138",
         "version": "1.7.0",
         "build": "929b973",
         "http_address": "inet[/192.168.19.135:9200]",
         "attributes": {
            "master": "true"
         },
         "process": {
            "refresh_interval_in_millis": 1000,
            "id": 15301,
            "max_file_descriptors": 4096,
            "mlockall": true
         }
      }
   }
}
#+END_SRC

可以动态修改配置，在marvel中设置
#+BEGIN_SRC sh
PUT /_cluster/settings
{
    "persistent" " {
        "discovery.zen.minimum_master_nodes" : 2
    },
    "transient" : {
        "indices.store.throttle.max_bytes_per_sec" : "50mb"
    }
}
#+END_SRC

升级ES，URL地址：“http://192.168.20.141:9200/_plugin/marvel/sense/index.html”
1. if possible, stop indexing new data. This is not always possible,
   but will help speed up recovery time.
2. Disable shard allocation. This prevents Elasticsearch from
   reblancing missing shards until you tell it otherwise. If you know
   the maintenance window will be short, this is a good idea. You can
   disable allocation as follows:
   #+BEGIN_SRC sh
PUT /_cluster/settings
{
    "transient" : {
        "cluster.routing.allocation.enable" : "none"
    }
}
   #+END_SRC
3. Shut down a single node, preferably using the shutdown API on that
   particular machine.
   #+BEGIN_SRC sh
POST /_cluster/nodes/_local/_shutdown
   #+END_SRC
4. Perform a maintenance/upgrade.
5. Restart the node, and confirm that it joins the cluster.
6. Reenable shard allocation as follows:
   #+BEGIN_SRC sh
PUT /_cluster/settings
{
    "transient" : {
        "cluster.routing.allocation.enable" : "all"
    }
}
   #+END_SRC
   Shard rebalancing may take some time. Wait until the cluster has
   returned to status green before continuing.
7. Repeat steps 2 through 6 for the rest of your nodes.
8. At this point you are safe to resume indexing (if you had
   previously stopped), but waiting unitl the cluster is fully
   balanced before resuming indexing will help to speed up the
   process.

备份ES，使用快照进行备份：
#+BEGIN_SRC sh
# let's setup a shared filesystem repository
PUT _snapshot/my_backup
{
    "type" : "fs",
    "settings" : {
        "location" : "/path/to/backup/my_backup"
    }
}
# We provide a name for our repository, in this case it is called my_backup.
# We specify that the type of the repository should be a shared filesystem.
# And finally, we provide a mounted drive as the destination.
#+END_SRC
*注意：The shared filesystem path must be accessable from all nodes in our cluster!*

This will create the repository and required metadata at the mount
point. There are also some other options that you may want to
configure, depending on the performance profile of your nodes,
network, and repository location:
* LogStash详解
   生产环境建议yum方式安装。

   LogStash运行流程：
   INPUTS        FILTERS     OUTPUT
   apache logs   date        ElasticSearch
   mail logs     xxxx        Graphite

   LogStash官网文档：[[https://www.elastic.co/guide/en/logstash/1.5/index.html][官网文档链接]]

   *生产环境建议使用yum进行安装。*

   #+BEGIN_SRC sh
   # 几种前台方式的启动
   /usr/local/logstash/bin/logstash -e 'input { stdin{} } output { stdout{} }' 

   # 打开调试模式
   /usr/local/logstash/bin/logstash -e 'input { stdin{} } output { stdout{codec => rubydebug} }' 

   # 把输出写入ElasticSearch
   /usr/local/logstash/bin/logstash -e 'input { stdin{} } output { elasticsearch { host => "192.168.20.129" protocol => "http"} }' 
   # 可以打开http://192.168.20.149:9200/_plugin/head进行查看，是否已把数据写入了ElasticSearch
   #+END_SRC

   这个时候，打开ElasticSearch的head插件，[[http://192.168.20.129:9200/_plugin/head/][ES之head插件页面]]

   LogStash配置文件：
   #+BEGIN_EXAMPLE
   input与output是必须配置的

   # 示例1：文件到文件
cat /etc/logstash.conf
input {
   file {
        path => "/var/log/messages"
   }
}

output {
    file {
         path => "/tmp/%{+YYYY-MM-dd}-messages.gz"
         gzip => true
    }
}
   #+END_EXAMPLE

   下载LogStash的启动脚本，

   如何使用呢？
   #+BEGIN_SRC sh
   /usr/local/logstash/bin/logstash -f /etc/logstash.conf
   cd /var/log/
   cat maillog >> messages
   ls /tmp
   #+END_SRC

   把文件写入到ElasticSearch，
   #+BEGIN_EXAMPLE
# cat /etc/logstash.conf 
input {
      file {
           path => "/var/log/messages"
      }
}
 
output {
       file {
            path => "/tmp/%{+YYYY-MM-dd}-messages.gz"
            gzip => true
       }
 
       elasticsearch {
            host => "192.168.20.129"
            protocol => "http"
            index => "system-messages-%{+YYYY.MM.dd}"
       }
}
   #+END_EXAMPLE
   
   一个常用的组合(解耦的架构)：两个LogStash实例，一个LogStash实例的
   input使用file模块，output使用redis模块。另一个LogStash实例的input使
   用redis模块，output使用elasticsearch模块。不直接往ES里写数据，如果
   其中ES服务宕了，数据也不会丢失，因为数据存在Redis的list中，只要
   Redis的List中的数据不弹出，数据就会存在，除非Redis中的数据把内存撑爆。

   
   #+BEGIN_SRC sh
   redis-cli -h 192.168.20.130 -p 6379
   redis 192.168.20.130:6379> info
   redis_version:2.4.10
   redis_git_sha1:00000000
   redis_git_dirty:0
   arch_bits:64
   multiplexing_api:epoll
   gcc_version:4.4.6
   process_id:10108
   uptime_in_seconds:91
   uptime_in_days:0
   lru_clock:598596
   used_cpu_sys:0.00
   used_cpu_user:0.03
   used_cpu_sys_children:0.00
   used_cpu_user_children:0.00
   connected_clients:1
   connected_slaves:0
   client_longest_output_list:0
   client_biggest_input_buf:0
   blocked_clients:0
   used_memory:726128
   used_memory_human:709.11K
   used_memory_rss:1593344
   used_memory_peak:726056
   used_memory_peak_human:709.04K
   mem_fragmentation_ratio:2.19
   mem_allocator:jemalloc-2.2.5
   loading:0
   aof_enabled:0
   changes_since_last_save:0
   bgsave_in_progress:0
   last_save_time:1453020756
   bgrewriteaof_in_progress:0
   total_connections_received:1
   total_commands_processed:0
   expired_keys:0
   evicted_keys:0
   keyspace_hits:0
   keyspace_misses:0
   pubsub_channels:0
   pubsub_patterns:0
   latest_fork_usec:0
   vm_enabled:0
   role:master
   redis 192.168.20.130:6379>
   #+END_SRC

   node1上的LogStash配置文件：
   #+BEGIN_EXAMPLE
input {
   file {
        path => "/var/log/messages"
   }
}

output {
    # file {
    #      path => "/tmp/%{+YYYY-MM-dd}-messages.gz"
    #      gzip => true
    # }

    # elasticsearch {
    #      host => "192.168.20.129"
    #      protocol => "http"
    #      index => "system-messages-%{+YYYY.MM.dd}"
    # }

    redis { 
          data_type => "list"
          key => "system-messages"
          host => "192.168.20.130"
          port => "6379"
          db => "1"
    }
}
   #+END_EXAMPLE

   *Redis中的db设置，默认16个db。一个Redis给一个业务使用。区分类型：db1系统日志，db2访问日志，db3错误日志，db4MySQL日志*


   在node1上启动LogStash，
   #+BEGIN_SRC sh
   /usr/local/logstash/bin/logstash -f /etc/logstash.conf
   # 往/var/log/messages文件里追加内容
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages

   # 登录redis，查看数据
   redis-cli -h 192.168.20.130 -p 6379
   redis 192.168.20.130:6379> select 1
   OK
   redis 192.168.20.130:6379[1]> KEYS *
   1) "system-messages"
   redis 192.168.20.130:6379[1]> LLEN system-messages
   (integer) 163
   redis 192.168.20.130:6379[1]> LINDEX  system-messages -1
   "{\"message\":\"}\",\"@version\":\"1\",\"@timestamp\":\"2016-01-17T09:09:38.650Z\",\"host\":\"linux-node1.example.com\",\"path\":\"/var/log/messages\"}"

   # 再次往messages文件里追加内容
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages
   cat /etc/logstash.conf >> /var/log/messages

   # 在redis里查看
   redis 192.168.20.130:6379[1]> LLEN system-messages
   (integer) 293
   #+END_SRC

   在node2上启动LogStash，node2上的LogStash配置文件，
   #+BEGIN_EXAMPLE
# cat /etc/logstash.conf 
input {
	redis {
		  data_type => "list"
		  key => "system-messages"
		  host => "192.168.20.130"
		  port => "6379"
		  db => "1"
	}
}
 
output {
	elasticsearch {
		  host => "192.168.20.129"
		  protocol => "http"
		  index => "system-redis-messages-%{+YYYY.MM.dd}"
	}
}
   #+END_EXAMPLE

   在node1上，登录redis
   #+BEGIN_SRC sh
   redis 192.168.20.130:6379[1]> LLEN system-messages
   (integer) 0
   #+END_SRC
* 日志需求分析
  有哪些日志需要收集呢？
  1. 系统日志 - 系统运行的状况，比如/var/log目录下的所有日志文件都是做什么的(可以使用syslog模块)
  2. 访问日志 - 业务的访问日志。比如500的错误有多少（使用json格式的Nginx访问日志）
  3. 错误日志 - 无论是WEB还是应用，都会有报错信息。
  4. 运行日志 - 运行日志的收集，以便做分析等。（codec => json grok）
  5. 其他日志

  处理流程：
  input -> decode -> filter -> encode -> output
** Nginx访问日志
	默认是按行处理的，没有字段的划分，这样的查询将是全文的检索。如果进
	行字段的划分，以后就可以根据字段查询了，而不是全文检索。
	#+BEGIN_EXAMPLE
	log_format logstash_json '{"@timestamp":"$time_iso8601",'
               '"host": "$server_addr",'
			   '"client": "$remote_addr",'
			   '"size": $body_bytes_sent,'
			   '"responsetime": $request_time,'
			   '"domain": "$host",'
			   '"url": "$uri",'
			   '"referer": "$http_referer",'
			   '"agent": "$http_user_agent",'
			   '"status": "$status"}';
	#+END_EXAMPLE
	在http配置内，添加如上的配置，在server配置内，
	#+BEGIN_EXAMPLE
	access_log logs/access_json.log logstash_json;
	#+END_EXAMPLE

	使用ab测试工具进行对Nginx测试：
	#+BEGIN_EXAMPLE
# 如果没有ab工具，可以进行安装
# yum install -y httpd-tools
	# ab -n1000 -c10 http://192.168.20.128/
This is ApacheBench, Version 2.3 <$Revision: 655654 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking 192.168.20.158 (be patient)
Completed 100 requests
Completed 200 requests
Completed 300 requests
Completed 400 requests
Completed 500 requests
Completed 600 requests
Completed 700 requests
Completed 800 requests
Completed 900 requests
Completed 1000 requests
Finished 1000 requests


Server Software:        nginx/1.9.4
Server Hostname:        192.168.20.158
Server Port:            81

Document Path:          /
Document Length:        612 bytes

Concurrency Level:      10
Time taken for tests:   0.130 seconds
Complete requests:      1000
Failed requests:        0
Write errors:           0
Total transferred:      844844 bytes
HTML transferred:       612612 bytes
Requests per second:    7664.95 [#/sec] (mean)
Time per request:       1.305 [ms] (mean)
Time per request:       0.130 [ms] (mean, across all concurrent requests)
Transfer rate:          6323.91 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    0   0.2      0       2
Processing:     0    1   2.7      1      25
Waiting:        0    1   2.7      1      25
Total:          0    1   2.7      1      25

Percentage of the requests served within a certain time (ms)
  50%      1
  66%      1
  75%      1
  80%      1
  90%      2
  95%      4
  98%      9
  99%     25
 100%     25 (longest request)
	#+END_EXAMPLE

	接下来修改es，
	#+BEGIN_SRC sh
# linux-node1:
cat /etc/logstash_nginx_access_log_to_redis.conf
input {
    file {
        path => "/usr/local/nginx/logs/access_json.log"
        codec => "json"
    }
}

output {
    redis {
        data_type => "list"
        key => "nginx-access-log"
        host => "192.168.20.159"
        port => "6379"
        db => "2"
    }
}

# linux-node2:
cat /etc/logstash_nginx_access_log_to_redis.conf
input {
	redis {
		data_type => "list"
		key => "nginx-access-log"
		host => "192.168.20.159"
		port => "6379"
		db => "2"
	}
}

output {
	elasticsearch {
		host => "192.168.20.158"
		protocol => "http"
		index => "nginx-access-log-%{+YYYY.MM.dd}"
	}
}
	#+END_SRC
* Kibana详解
  kibana.logstash.es

  Kibana是为ElasticSearch设计的可视化的分析平台。搜索ES的数据并进行可
  视化的展现。
* ELK Stack实践案例
   Logstash是按事件进行处理的，如果某个事件有多行错误日志信息，就会被
   Logstash认为是多个事件了。可以使用Logstash的multiline处理。

   使用codec => multiline需要练习。

   访问日志可以使用codec => json

   一些坑：
   1. 时区的问题，使用UTC。


   一些要掌握的东西，
   1. syslog
   2. multiline
   3. geoip
   4. if else(用在)
	  #+BEGIN_SRC sh
input {
	file {
		type => "apache"
		path => "/data/access_logs/*.lavenliu.com-access.log"
	}

	file {
		type => "php-error-log"
		path => "/tmp/php_errors.log"
	}

	file {
		type => "api-run-log"
		path => "/tmp/logs/*"
	}
}

filter {
	if [type] == "api-run-log" {
	       json {
			   source => "message"
			   remove_field => "message"
		   }
	   }

	   source => "message"
	   remove_field => "message"
}

output {
	if [type] == "apache" {
		   redis {
			   host => "192.168.20.159"
			   port => "6379"
			   db => "6"
			   data_type => "list"
			   key => "api-access-log-{{HOSTNAME}}"
		   }
	}

    if [type] == "php-error-log" {
		   redis {
			   host => "192.168.20.159"
			   port => "6379"
			   db => "8"
			   data_type => "list"
			   key => "php-error-log"
		   }
	}

    if [type] == "api-run-log" {
		   redis {
			   host => "192.168.20.159"
			   port => "6379"
			   db => "7"
			   data_type => "list"
			   key => "api-run-log"
		   }
	}
}
# 后面从redis写到ES的话，两边都要写判断。
# 注意：我们在上面设置了type字段，所以搜集的日志文件里就不能有type字段了，
# 会覆盖我们设置的type了，所以应用的日志数据里就不能有type字段了。
	  #+END_SRC
