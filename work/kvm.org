#+TITLE: KVM
#+AUTHOR: LavenLiu
#+DATE: 2013-08-09
#+EMAIL: ldczz2008@163.com 

#+STARTUP: OVERVIEW
#+TAGS: OFFICE(o) HOME(h) PROJECT(p) CHANGE(c) REPORT(r) MYSELF(m) 
#+TAGS: PROBLEM(P) INTERRUPTTED(i) RESEARCH(R)
#+SEQ_TODO: TODO(t)  STARTED(s) WAITING(W) | DONE(d) CANCELLED(C) DEFERRED(f)
#+COLUMNS: %40ITEM(Details) %TAGS(Context) %7TODO(To Do) %5Effort(Time){:} %6CLOCKSUM{Total}

#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper,11pt]
#+LaTeX_HEADER: \usepackage[top=2.1cm,bottom=2.1cm,left=2.1cm,right=2.1cm]{geometry}
#+LaTeX_HEADER: \setmainfont[Mapping=tex-text]{Times New Roman}
#+LaTeX_HEADER: \setsansfont[Mapping=tex-text]{Tahoma}
#+LaTeX_HEADER: \setmonofont{Courier New}
#+LaTeX_HEADER: \setCJKmainfont[BoldFont={Adobe Heiti Std},ItalicFont={Adobe Kaiti Std}]{Adobe Song Std}
#+LaTeX_HEADER: \setCJKsansfont{Adobe Heiti Std}
#+LaTeX_HEADER: \setCJKmonofont{Adobe Fangsong Std}
#+LaTeX_HEADER: \punctstyle{hangmobanjiao}
#+LaTeX_HEADER: \usepackage{color,graphicx}
#+LaTeX_HEADER: \usepackage[table]{xcolor}
#+LaTeX_HEADER: \usepackage{colortbl}
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage[bf,small,indentafter,pagestyles]{titlesec}

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/style2.css" />

#+OPTIONS: ^:nil
#+OPTIONS: tex:t

* 什么是KVM
  内核级虚拟化技术
  | 虚拟化产品 | 谁在用                         | 从业钱景如何 | 学习难度如何 |
  |------------+--------------------------------+--------------+--------------|
  | VMWare     | 政府、金融等传统行业使用比较多 | ****         | ****         |
  | XEN        | 使用虚拟化比较早的公司         | ***          | *****        |
  | HyperV     | 使用微软产品比较多的公司       | ****         | ****         |
  | KVM        | 互联网公司                     | *****        | *****        |

  检查服务器是否支持虚拟化
  #+BEGIN_SRC sh
grep -E --color '(vmx|svm)' /proc/cpuinfo
  #+END_SRC
* 硬件虚拟化与软件虚拟化对比
* 平台虚拟化与软件虚拟化对比
* 全虚拟化与半虚拟化对比
* without virtualization vs. with virtualization
  租用机柜，根据U数计算费用。可以购买一台性能强劲的物理服务器，上面跑
  虚拟机，可以少占用机柜空间，节省预算。
* KVM与QEMU
  KVM虚拟机：KVM内核模块+QEMU硬件模拟
* KVM介绍
  RHEL6.0以后已被整合到内核了。

  KVM：Kernel-based Virtual Machine

  结构简单，包括两部分：
  1. 设备驱动/dev/kvm
  2. 针对模拟PC硬件的用户空间组件

  KVM需要CPU的虚拟化功能支持，只有在具有虚拟化支持的CPU上运行，即具有
  VT功能的Intel CPU和具有AMD-V功能的AMD CPU。
* 安装KVM
** 测试环境
   | 主机名                  | IP地址                           | 备注  |
   |-------------------------+----------------------------------+-------|
   | linux-node1.example.com | 192.168.20.129 - eth1 (HostOnly) | kvm01 |
   |                         | 192.168.19.129 - eth0 (NAT)      |       |
   |-------------------------+----------------------------------+-------|
   | linux-node2.example.com | 192.168.20.130 - eth1 (HostOnly) | kvm02 |
   |                         | 192.168.19.130 - eth0 (NAT)      |       |
** 安装EPEL源
   在所有节点安装EPEL源，
   #+BEGIN_SRC sh
rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel//6/x86_64/epel-release-6-8.noarch.rpm
   #+END_SRC
** 安装QEMU
   #+BEGIN_SRC sh
yum install -y qemu-kvm
rpm -ql qemu-kvm
/etc/sysconfig/modules/kvm.modules
lsmod | grep kvm
   #+END_SRC
** 安装KVM管理工具
   #+BEGIN_SRC sh
yum install -y virt-manager python-virtinst qemu-kvm-tools libvirt libvirt-python
/etc/init.d/libvirtd start
chkconfig libvirted on
[root@kvm01 opt]# libvirtd --version
libvirtd (libvirt) 0.10.2
   #+END_SRC

   启动libvirtd服务，
   #+BEGIN_SRC sh
/etc/init.d/libvirtd start
   #+END_SRC
   此时，会生成一个virbr0的虚拟网卡，可以使用"brctl show"。

  首先qemu-img创建一块硬盘，
  #+BEGIN_SRC sh
qemu-img create -f raw /opt/kvm.raw 8G # raw格式占用宿主机的磁盘空间，分多少空间，实际就占多少空间
qemu-img info /opt/kvm.raw
image: /opt/kvm.raw
file format: raw
virtual size: 8.0G (8589934592 bytes)
disk size: 1.5G
  #+END_SRC
** 创建虚拟机
  #+BEGIN_SRC sh
virt-install --virt-type kvm \
--name kvm-demo \
--ram 512M \
--cdrom /opt/centos6u5.iso \
--network network=default \
--graphics vnc,listen=0.0.0.0 \
--noautoconsole \
--os-type=linux \
--os-variant=rhel6 \
--disk path=/opt/kvm.raw

############################
ifconfig -a
eth0      Link encap:Ethernet  HWaddr 00:0C:29:1C:1F:8E  
          inet addr:192.168.19.134  Bcast:192.168.19.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:fe1c:1f8e/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:100868 errors:0 dropped:0 overruns:0 frame:0
          TX packets:38436 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:126169479 (120.3 MiB)  TX bytes:2098905 (2.0 MiB)

eth1      Link encap:Ethernet  HWaddr 00:0C:29:1C:1F:98  
          inet addr:192.168.20.129  Bcast:192.168.20.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:fe1c:1f98/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:11209 errors:0 dropped:0 overruns:0 frame:0
          TX packets:17181 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:834591 (815.0 KiB)  TX bytes:13139578 (12.5 MiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

virbr0    Link encap:Ethernet  HWaddr 52:54:00:F0:29:34  
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:10 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:2353 (2.2 KiB)  TX bytes:2413 (2.3 KiB)

virbr0-nic Link encap:Ethernet  HWaddr 52:54:00:F0:29:34  
          BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:500 
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

vnet0     Link encap:Ethernet  HWaddr FE:54:00:08:8E:88  
          inet6 addr: fe80::fc54:ff:fe08:8e88/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:40 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:500 
          RX bytes:0 (0.0 b)  TX bytes:2236 (2.1 KiB)
###################################
ps -ef |grep dns
nobody     1334      1  0 17:44 ?        00:00:00 /usr/sbin/dnsmasq --strict-order \
--pid-file=/var/run/libvirt/network/default.pid \
--conf-file= --except-interface lo --bind-interfaces --listen-address 192.168.122.1 \
--dhcp-range 192.168.122.2,192.168.122.254 \
--dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases \
--dhcp-lease-max=253 --dhcp-no-override \
--dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile \
--addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
  #+END_SRC

   如果是qcow2格式的虚拟机镜像文件的话，上面的命令行要修改为如下形式：
   #+BEGIN_SRC sh
virt-install --virt-type kvm \
--name kvm-demo \
--ram 512M \
--cdrom /opt/centos6u5.iso \
--network network=default \
--graphics vnc,listen=0.0.0.0 \
--noautoconsole \
--os-type=linux \
--os-variant=rhel6 \
--disk path=/opt/kvm.qcow2,format=qcow2
   #+END_SRC

  qemu-kvm可以模拟哪些CPU，
  #+BEGIN_SRC sh
/usr/libexec/qemu-kvm -cpu ?
x86       Opteron_G5  AMD Opteron 63xx class CPU                      
x86       Opteron_G4  AMD Opteron 62xx class CPU                      
x86       Opteron_G3  AMD Opteron 23xx (Gen 3 Class Opteron)          
x86       Opteron_G2  AMD Opteron 22xx (Gen 2 Class Opteron)          
x86       Opteron_G1  AMD Opteron 240 (Gen 1 Class Opteron)           
x86        Broadwell  Intel Core Processor (Broadwell)                
x86          Haswell  Intel Core Processor (Haswell)                  
x86      SandyBridge  Intel Xeon E312xx (Sandy Bridge)                
x86         Westmere  Westmere E56xx/L56xx/X56xx (Nehalem-C)          
x86          Nehalem  Intel Core i7 9xx (Nehalem Class Core i7)       
x86           Penryn  Intel Core 2 Duo P9xxx (Penryn Class Core 2)    
x86           Conroe  Intel Celeron_4x0 (Conroe/Merom Class Core 2)   
x86      cpu64-rhel5  QEMU Virtual CPU version (cpu64-rhel5)          
x86      cpu64-rhel6  QEMU Virtual CPU version (cpu64-rhel6)          
x86             n270  Intel(R) Atom(TM) CPU N270   @ 1.60GHz          
x86           athlon  QEMU Virtual CPU version 0.12.1                 
x86         pentium3                                                  
x86         pentium2                                                  
x86          pentium                                                  
x86              486                                                  
x86          coreduo  Genuine Intel(R) CPU           T2600  @ 2.16GHz 
x86           qemu32  QEMU Virtual CPU version 0.12.1                 
x86            kvm64  Common KVM processor                            
x86         core2duo  Intel(R) Core(TM)2 Duo CPU     T7700  @ 2.40GHz 
x86           phenom  AMD Phenom(tm) 9550 Quad-Core Processor         
x86           qemu64  QEMU Virtual CPU version 0.12.1                 

Recognized CPUID flags:
  f_edx: pbe ia64 tm ht ss sse2 sse fxsr mmx acpi ds clflush pn pse36 pat cmov mca pge mtrr sep apic cx8 mce pae msr tsc pse de vme fpu
  f_ecx: hypervisor rdrand f16c avx osxsave xsave aes tsc-deadline popcnt movbe x2apic sse4.2|sse4_2 sse4.1|sse4_1 dca pcid pdcm xtpr cx16 fma cid ssse3 tm2 est smx vmx ds_cpl monitor dtes64 pclmulqdq|pclmuldq pni|sse3
  extf_edx: 3dnow 3dnowext lm|i64 rdtscp pdpe1gb fxsr_opt|ffxsr fxsr mmx mmxext nx|xd pse36 pat cmov mca pge mtrr syscall apic cx8 mce pae msr tsc pse de vme fpu
  extf_ecx: perfctr_nb perfctr_core topoext tbm nodeid_msr tce fma4 lwp wdt skinit xop ibs osvw 3dnowprefetch misalignsse sse4a abm cr8legacy extapic svm cmp_legacy lahf_lm
  #+END_SRC
* KVM虚拟机管理
** libvirt简介
   libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和应用程序接口
   （API），而且一些常用的虚拟机管理工具（如virsh、virt-install、
   virt-manager等）和云计算框架平台（如OpenStack、OpenNebula等）都在底
   层使用libvirt的应用程序接口来管理虚拟机。

   libvirt是为了更方便地管理平台虚拟化技术而设计的开放源代码的应用程序
   接口、守护进程和管理工具，它不仅提供了对虚拟化客户机的管理，也提供
   了对虚拟化网络和存储的管理。尽管libvirt项目最初是为Xen设计的一套API，
   但是目前对KVM等其他Hypervisor的支持也非常的好。libvirt支持多种虚拟
   化方案，既支持包括KVM、QEMU、Xen、VMware、VirtualBox等在内的平台虚
   拟化方案，又支持OpenVZ、LXC等Linux容器虚拟化系统，还支持用户态Linux
   （UML）的虚拟化。libvirt是一个免费的开源的软件，使用的许可证是LGPL
   （GNU宽松的通用公共许可证），使用libvirt库进行链接的软件程序不需要
   一定选择开源和遵守GPL许可证。和KVM、Xen等开源项目类似，libvirt也有
   自己的开发者社区，而且随着虚拟化、云计算等成为近年来的技术热点，
   libvirt项目的社区也比较活跃。

   libvirt本身提供了一套较为稳定的C语言应用程序接口，目前，在其他一些
   流行的编程语言中也提供了对libvirt的绑定，在Python、Perl、Java、Ruby、
   PHP、OCaml等高级编程语言中已经有libvirt的程序库可以直接使用。
   libvirt还提供了为基于AMQP（高级消息队列协议）的消息系统（如Apache
   Qpid）提供QMF代理，这可以让云计算管理系统中宿主机与客户机、客户机与
   客户机之间的消息通信变得更易于实现。libvirt还为安全的远程管理虚拟客
   户机提供了加密和认证等安全措施。正是由于libvirt拥有这些强大的功能和
   较为稳定的应用程序接口，而且它的许可证（license）也比较宽松，
   libvirt的应用程序接口已被广泛地用在基于虚拟化和云计算的解决方案中，
   主要作为连接底层Hypervisor和上层应用程序的一个中间适配层。

   libvirt对多种不同的Hypervisor的支持是通过一种基于驱动程序的架构来实
   现的。libvirt对不同的Hypervisor提供了不同的驱动：对Xen有Xen的驱动，
   对QEMU/KVM有QEMU驱动，对VMware有VMware驱动。在libvirt源代码中，可以
   很容易找到qemu_driver.c、xen_driver.c、xenapi_driver.c、
   vmware_driver.c、vbox_driver.c这样的驱动程序源代码文件。

   libvirt作为中间适配层，让底层Hypervisor对上层用户空间的管理工具是可
   以做到完全透明的，因为libvirt屏蔽了底层各种Hypervisor的细节，为上层
   管理工具提供了一个统一的、较稳定的接口（API）。通过libvirt，一些用
   户空间管理工具可以管理各种不同的Hypervisor和上面运行的客户机

   在libvirt中涉及到几个重要的概念，解释如下：

   1. 节点（Node）：一个物理机器，上面可能运行着多个虚拟客户机。
      Hypervisor和Domain都运行在Node之上。
   2. Hypervisor：也称虚拟机监控器（VMM），如KVM、Xen、VMware、Hyper-V
      等，是虚拟化中的一个底层软件层，它可以虚拟化一个节点让其运行多个
      虚拟客户机（不同客户机可能有不同的配置和操作系统）。
   3. 域（Domain）：是在Hypervisor上运行的一个客户机操作系统实例。域也
      被称为实例（instance，如亚马逊的AWS云计算服务中客户机就被称为实
      例）、客户机操作系统（guest OS）、虚拟机（virtual machine），它
      们都是指同一个概念。

   在了解了节点、Hypervisor和域的概念之后，用一句话概括libvirt的目标，
   就是：为了安全高效的管理节点上的各个域，而提供一个公共的稳定的软件层。
   当然，这里的管理，既包括本地的管理，也包含远程的管理。具体地讲，
   libvirt的管理功能主要包含如下五个部分：

   （1）域的管理：包括对节点上的域的各个生命周期的管理，如：启动、停止、
   暂停、保存、恢复和动态迁移。也包括对多种设备类型的热插拔操作，包括：
   磁盘、网卡、内存和CPU，当然不同的Hypervisor上对这些热插拔的支持程度
   有所不同。

   （2）远程节点的管理：只要物理节点上运行了libvirtd这个守护进程，远程
   的管理程序就可以连接到该节点进程管理操作，经过认证和授权之后，所有的
   libvirt功能都可以被访问和使用。libvirt支持多种网络远程传输类型，如
   SSH、TCP套接字、Unix domain socket、支持TLS的加密传输等。假设使用最
   简单的SSH，则不需要额外配置工作，比如：example.com节点上运行了
   libvirtd，而且允许SSH访问，在远程的某台管理机器上就可以用如下的命令
   行来连接到example.com上，从而管理其上的域。

   virsh -c qemu+ssh://root@example.com/system

   （3）存储的管理：任何运行了libvirtd守护进程的主机，都可以通过
   libvirt来管理不同类型的存储，如：创建不同格式的客户机镜像（qcow2、
   raw、qde、vmdk等）、挂载NFS共享存储系统、查看现有的LVM卷组、创建新的
   LVM卷组和逻辑卷、对磁盘设备分区、挂载iSCSI共享存储，等等。当然
   libvirt中，对存储的管理也是支持远程管理的。

   （4）网络的管理：任何运行了libvirtd守护进程的主机，都可以通过
   libvirt来管理物理的和逻辑的网络接口。包括：列出现有的网络接口卡，配
   置网络接口，创建虚拟网络接口，网络接口的桥接，VLAN管理，NAT网络设置，
   为客户机分配虚拟网络接口，等等。

   （5）提供一个稳定、可靠、高效的应用程序接口（API）以便可以完成前面的
   4个管理功能。

   libvirt主要由三个部分组成，它们分别是：应用程序编程接口（API）库、
   一个守护进程（libvirtd）和一个默认命令行管理工具（virsh）。应用程序
   接口（API）是为了其他虚拟机管理工具（如virsh、virt-manager等）提供
   虚拟机管理的程序库支持。libvirtd守护进程负责执行对节点上的域的管理
   工作，在用各种工具对虚拟机进行管理之时，这个守护进程一定要处于运行
   状态中，而且这个守护进程可以分为两种：一种是root权限的libvirtd，其
   权限较大，可以做所有支持的管理工作；一种是普通用户权限的libvirtd，
   只能做比较受限的管理工作。virsh是libvirt项目中默认的对虚拟机管理的
   一个命令行工具。
** libvirt的配置和使用
   以CentOS6.5为例，libvirt相关的配置文件在/etc/libvirt目录中，
   #+BEGIN_SRC sh
   ls -l /etc/libvirt
total 48
-rw-r--r-- 1 root root   518 Mar 23 00:59 libvirt.conf
-rw-r--r-- 1 root root 12963 Mar 23 00:59 libvirtd.conf
-rw-r--r-- 1 root root  1176 Mar 23 00:59 lxc.conf
drwx------ 2 root root  4096 Mar 27 17:44 nwfilter
drwx------ 3 root root  4096 Apr  6 09:25 qemu
-rw-r--r-- 1 root root 14998 Mar 23 00:59 qemu.conf
   #+END_SRC

   /etc/libvirt目录下的几个文件及目录介绍，
   | 文件名        | 作用                                                                                     |
   |---------------+------------------------------------------------------------------------------------------|
   | libvirt.conf  | 用于配置一些常用libvirt连接（通常是远程连接）的别名                                      |
   |---------------+------------------------------------------------------------------------------------------|
   | libvirtd.conf | libvirt的守护进程libvirtd的配置文件，用来设置启动libvirtd时的一些启动参数，              |
   |               | 如是否建立TCP、UNIX domain socket等连接方式及其最大连接数，以及这些连接的认证机制等等    |
   |---------------+------------------------------------------------------------------------------------------|
   | qemu.conf     | qemu.conf是libvirt对QEMU的驱动的配置文件，包括VNC、SPICE等和连接它们时采用的权限认证方式 |
   |               | 的配置，也包括内存大页、SELinux、CGroup等相关配置                                        |
   |---------------+------------------------------------------------------------------------------------------|
   | qemu          | qemu目录下是存放使用QEME驱动的域（具体的虚拟机）的配置文件                               |

   *libvirt.conf的简单配置:*
   #+BEGIN_SRC sh
uri_aliases = [
"remote129=qemu+ssh://root@192.168.20.129/system",
]
   #+END_SRC
   其中，配置了remote129这个别名用于指代
   qemu+ssh://root@192.168.20.129/system这个远程的libvirt连接，有了这
   个别名后，就可以在virsh等工具或者自己写代码调用libvirt API时使用这
   个别名而不需要写完整的、冗长的URI连接标识了。用virsh使用这个别名，
   连接到远程的libvirt上查询当前已经启动的客户机状态，然后退出，命令行
   操作如下：
   #+BEGIN_SRC sh
# 修改完毕libvirt.conf后，重启或不重启libvirtd进程都是生效的，
# 为了规范，这里重新加载libvirtd进程
/etc/init.d/libvirtd reload
[root@kvm01 opt]# virsh -c remote129
root@192.168.20.129's password: 
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list
 Id    Name                           State
----------------------------------------------------
 5     new                            running

virsh # quit
#
   #+END_SRC

   在代码中调用 libvirt API 也可以使用这个别名用于建立连接，如下的
   python 代码行就是使用这个别名来建立连接：
   #+BEGIN_SRC python
conn = libvirt.openReadOnly('remote129')
   #+END_SRC

   *libvirtd.conf的简单配置：*

   例如，下面的几个配置项，表示关闭TLS安全认证的连接（默认值是打开的）、
   打开 TCP 连接（默认是关闭TCP连接的），设置TCP监听的端口，TCP连接不
   使用认证授权方式，设置UNIX domain socket 的保存目录等。
   #+BEGIN_SRC sh
listen_tls = 0
listen_tcp = 1
tcp_port = "16509"
unix_sock_dir = "/var/run/libvirt"
auth_tcp = "none"
   #+END_SRC

   注意：要让TCP、TLS等连接的生效，需要在启动libvirtd时加上--listen参
   数（简写为-l ）。而默认的service libvirtd start命令启动libvirtd服务
   时，并没带--listen参数，所以如果要使用TCP等连接方式，可以使用
   libvirtd --listen -d命令来启动libvirtd。
   #+BEGIN_SRC sh
/etc/init.d/libvirtd stop
libvirtd --listen -d
virsh -c qemu+tcp://localhost:16509/system
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list
 Id    Name                           State
----------------------------------------------------
 5     new                            running

virsh #
   #+END_SRC

   查看一下libvirtd的套接字，
   #+BEGIN_SRC sh
[root@kvm01 libvirt]# ll /var/run/libvirt/libvirt-sock*
srwxrwxrwx 1 root root 0 Apr  6 13:38 /var/run/libvirt/libvirt-sock
srwxrwxrwx 1 root root 0 Apr  6 13:38 /var/run/libvirt/libvirt-sock-ro
   #+END_SRC

   *qemu目录：存放域的配置文件的目录*  
   #+BEGIN_SRC sh
ll /etc/libvirt/qemu
total 12
-rw------- 1 root root 3029 Mar 27 19:26 kvm-demo.xml
drwx------ 3 root root 4096 Mar 27 17:44 networks
-rw------- 1 root root 3025 Apr  6 09:25 new.xml
   #+END_SRC

   其中包括了两个域的XML配置文件（kvm-demo.xml和new.xml），这就是笔者
   用virt-manager工具创建的两个域，默认会将其配置文件保存到
   /etc/libvirt/qemu/目录下。而其中的networks目录是保存了创建一个域时
   默认使用的网络配置。
** libvirtd的使用
   libvirtd是一个作为libvirt虚拟化管理系统中的服务器端的守护进程，如果
   要让某个节点能够用libvirt进行管理（无论是本地管理还是远程管理），都
   需要在这个节点上运行着libvirtd这个守护进程，以便让其他上层管理工具
   可以连接到该节点，libvirtd负责执行其他管理工具发送到它的虚拟化管理
   操作指令。而libvirt的客户端工具（包括virsh、virt-manager等）可以连
   接到本地货远程的libvirtd进程，以便管理节点上的客户机（启动、停止、
   重启、迁移等）、收集节点上的宿主机和客户机的配置和资源使用状态。

   默认情况下，libvirtd监听在本地的Unix Domain Socket上，并没有监听基
   于网络的TCP/IP socket，需要使用"-l"或"--listen"的命令行参数来开启对
   libvirtd.conf配置文件中对TCP/IP socket的配置。另外，libvirtd进程的
   启动或停止，并不会直接影响正在运行的客户机。libvirtd在启动或重启完
   成时，只要客户机的XML配置文件是存在的，libvirtd会自动加载这些客户机
   的配置文件，获取它们的信息；当然，如果客户机没有基于libvirt格式的
   XML文件来运行，libvirtd则不能发现它。
** virsh
   该工具是由libvirt包得到，虚拟机的XML文件存放在/etc/libvirt/qemu目录
	下。硬性修改XML文件并不能影响虚拟机。

	1. 启动虚拟机
	   #+BEGIN_SRC sh
	   virsh start CentOS-6.5-x86_64
	   #+END_SRC
	2. 关闭虚拟机
	   #+BEGIN_SRC sh
	   virsh shutdown CentOS-6.5-x86_64
	   #+END_SRC
	3. 查看已有的虚拟机
	   #+BEGIN_SRC sh
	   virsh list --all
	   #+END_SRC
	4. define与undefine
	   #+BEGIN_SRC sh
virsh undefine CentOS-6.5-x86_64
virsh define /opt/CentOS-6.5-x86_64.xml
	   #+END_SRC
** brctl的使用
   创建br0网桥
   #+BEGIN_SRC sh
# brctl addbr br0                        #添加br0网桥
# brctl addif br0 eth1                   #把eth1加入br0网桥
# ip addr del dev eth1 192.168.20.129/24 #删除eth1上的IP地址
# ifconfig br0 192.168.20.129/24 up      #把eth1上的IP地址设置在br0上
   #+END_SRC

   创建思路：
   #+BEGIN_EXAMPLE
   首先创建桥接网卡br0，把桥接网卡关联到eth1网卡上，删除eth1上的IP地址，
   把eth1上的IP地址添加到桥接网卡br0上
   #+END_EXAMPLE

   编辑KVM虚拟机的XML配置文件，修改其网络连接方式：
   
   #+BEGIN_SRC sh
   # 编辑之前，关闭KVM虚拟机
   # virsh shutdown CentOS-6.5-x86_64
   # virsh edit CentOS-6.5-x86_64
     52     <interface type='bridge'>  # 此行需要修改，由"default"改为"bridge"
     53       <mac address='52:54:00:68:56:c8'/>
     54       <source bridge='br0'/>  # 此行需要修改，由"network"改为"br0"
     55       <model type='virtio'/>
     56       <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
     57     </interface>
   #+END_SRC
  
   编辑完毕，启动KVM虚拟机：
   #+BEGIN_SRC sh
   # virsh start CentOS-6.5-x86_64
   # brctl show
     bridge name	bridge id		STP enabled	interfaces
     br0		8000.000c291c1f98	no		eth1
							                vnet0 # KVM虚拟机的vnet0网卡已经附加在br0上了
     virbr0		8000.525400d07d1f	yes		virbr0-nic
   #+END_SRC
** 虚拟机拷贝
   #+BEGIN_SRC sh
virsh destroy kvm-demo # 这一步可以不用操作
virsh dumpxml kvm-demo > new.xml
cp /opt/kvm.raw /opt/new.raw
sed -i 's#kvm-demo#new#g' new.xml
sed -i 's#kvm\.raw#new\.raw#g' new.xml
virsh define new.xml
####
# 这种方式要修改new.xml
   #+END_SRC
** 虚拟机克隆
** 增加虚拟机硬盘空间
   对硬盘做操作要谨慎。要做好备份再进行操作比较可靠。
   #+BEGIN_SRC sh
qemu-img resize new.raw +1G
   #+END_SRC
** 虚拟机硬盘格式转换
   把raw格式的硬盘转换为qcow2格式的，
   #+BEGIN_SRC sh
qemu-img convert -c -f raw -O qcow2 new.raw new.qcow2
qemu-img check new.qcow2
   #+END_SRC

   然后直接使用vim修改new.xml文件后，reboot虚拟机后，其磁盘格式任然是
   raw格式。彻底更改使用"virsh edit new"。

   *在生产环境中使用virsh edit xxx来修改设置*
** 虚拟机迁移
   尽量避免使用动态迁移。

   可以使用复制的静态方式来迁移虚拟机，然后重新定义虚拟机的XML文件。
** 创建快照
   创建快照的命令，
   #+BEGIN_SRC sh
qemu-img snapshot -c backup /opt/new.qcow2
   #+END_SRC

   查看快照的命令，
   #+BEGIN_SRC sh
qemu-img snapshot -l /opt/new.qcow2
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         backup                    0 2016-04-06 10:06:47   00:00:00.000
   #+END_SRC
   
   可以在虚拟机里创建一个文件，如/root/test.iso；然后恢复到标签为
   backup时的快照，恢复过去的话，就不会看到test.iso文件了。
   #+BEGIN_SRC sh
# 在虚拟机里进行操作
dd if=/dev/zero of=/root/test.iso bs=1M count=512
   #+END_SRC

   恢复到标签为backup时的快照，
   #+BEGIN_SRC sh
qemu-img snapshot -a backup /opt/new.qcow2
   #+END_SRC
* KVM镜像制作
** 使用OZ制作镜像
   安装相应的工具包，
   #+BEGIN_SRC sh
yum install -y oz libguestfs-tools
   #+END_SRC

   虚拟机分区的时候，只划分一个/分区，不划分其他的比如/root和交换分区。

   安装完毕后的设置，
   1. 网络设置
	  #+BEGIN_EXAMPLE
	  删除HWADDR=""
	  ONBOOT=no修改为ONBOOT=yes
	  #+END_EXAMPLE
   2. 删除已经生成的网络设备相关的udev规则
	  #+BEGIN_SRC sh
	  rm -f /etc/udev/rules.d/70-persistent-net.rules
	  #+END_SRC
   3. 关闭防火墙和selinux
	  #+BEGIN_SRC sh
	  sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/sysconfig/selinux
      /etc/init.d/iptables stop && chkconfig iptables stop
      /etc/init.d/ip6tables stop && chkconfig ip6tables stop
	  #+END_SRC
* KVM虚拟机监控
** virt-top
   virt-top是一个用于展示虚拟机运行状态和性能的工具，它和Linux系统的
   top工具类似。virt-top是使用libvirt API来进行工作的。所以virt-top可
   以监控所有libvirt支持的Hypervisor上的虚拟机的状态。
   #+BEGIN_SRC sh
   yum install -y virt-top
   #+END_SRC
* KVM优化
** CPU
   如何让KVM高效使用CPU。

   CPU有用户态和系统态。

   在虚拟机里也有用户态和系统态。

   如何优化，让QEMU进程绑定到具体的CPU核上运行。充分利用CPU的缓存。如
   果QEMU进程被系统调度，经常切换CPU核心，CPU的缓存将不起作用。
** 内存
   大小的优化，寻址的优化。
** IO
   IO分为存储IO、网络IO。

   使用virtio；还可以使用vt-d工具（把具体的硬件分配个虚拟机使用）。

   #+BEGIN_SRC sh
cat /sys/block/sda/queue/scheduler
   #+END_SRC
* Libvirt API
** libvirt API简介
   libvirt的核心价值和主要目标就是提供了一套管理虚拟机的、稳定的、高效
   的应用程序接口（API）。libvirt API本身是用C语言实现的，

   libvirt API大致可划分为如下8个大的部分：
*** 连接Hypervisor相关的API
	以virConnect开头的一系列函数。
	
	只有与Hypervisor建立了连接之后，才能进行虚拟机管理操作，所以连接
	Hypervisor的API是其他所有API使用的前提条件。与Hypervisor建立的连接
	是为其他API的执行提供了路径，是其他虚拟化管理功能的基础。通过调用
	virConnectOpen函数可以建立一个连接，其返回值是一个virConnectPtr对
	象，该对象就代表到Hypervisor的一个连接；如果连接出错，则返回空值
	（NULL）。而virConnectOpenReadOnly函数会建立一个只读的连接，在该连
	接上可以使用一些查询的功能，而不使用创建、修改等功能。
	virConnectOpenAuth函数提供了更具认证建立的连接。
	virConnectGetCapabilities函数是返回对Hypervisor和驱动的功能的描述
	的 XML 格式的字符串。virConnectListDomains函数返回一列域标识符，它
	们代表该Hypervisor上的活动域。
*** 域管理的API
	以virDomain开头的一系列函数。

	虚拟机的管理，最基本的职能就是对各个节点上的域的管理，故libvirt
	API 中实现了很多针对域管理的函数。要管理域，首先就要获取
	virDomainPtr这个域对象，然后才能对域进行操作。有很多种方式来获取域
	对象，如virDomainPtr virDomainLookupByID(virConnectPtr conn, int
	id)函数是根据域的id值到conn这个连接上去查找相应的域。类似地，
	virDomainLookupByName、virDomainLookupByUUID等函数分别是根据域的名
	称和UUID去查找相应的域。在得到了某个域的对象后，就可以进行很多的操
	作，可以是查询域的信息（如：virDomainGetHostname、virDomainGetInfo、
	virDomainGetVcpus、virDomainGetVcpusFlags、virDomainGetCPUStats，
	等等），也可以是控制域的生命周期（如：virDomainCreate 、
	virDomainSuspend 、virDomainResume 、virDomainDestroy 、
	virDomainMigrate，等等）。
*** 节点管理的API
	以virNode开头的一系列函数。

	域是运行在物理节点之上，libvirt也提供了对节点的信息查询和控制的功
	能。节点管理的多数函数都需要使用一个连接Hypervisor的对象作为其中的
	一个传入参数，以便可以查询或修改到该连接上的节点的信息。
	virNodeGetInfo函数是获取节点的物理硬件信息，virNodeGetCPUStats函数
	可以获取节点上各个CPU的使用统计信息，virNodeGetMemoryStats函数可以
	获取节点上的内存的使用统计信息，virNodeGetFreeMemory函数可以获取节
	点上可用的空闲内存大小。也有一些设置或者控制节点的函数，如
	virNodeSetMemoryParameters函数可以设置节点上的内存调度的参数，
	virNodeSuspendForDuration函数可以让节点（宿主机）暂停运行一段时间。
*** 网络管理的API
	以virNetwork开头的一系列函数和部分以virInterface开头的函数。

	libvirt对虚拟化环境中的网络管理也提供了丰富的API。libvirt首先需要
	创建virNetworkPtr对象，然后才能查询或控制虚拟网络。一些查询网络相
	关信息的函数，如：virNetworkGetName函数可以获取网络的名称，
	virNetworkGetBridgeName函数可以获取该网络中网桥的名称，
	virNetworkGetUUID函数可以获取网络的UUID标识，virNetworkGetXMLDesc
	函数可以获取网络的以XML格式的描述信息，virNetworkIsActive函数可以
	查询网络是否正在使用中。一些控制或更改网络设置的函数，有：
	virNetworkCreateXML函数可以根据提供的XML格式的字符串创建一个网络
	（返回 virNetworkPtr对象），virNetworkDestroy函数可以销毁一个网络
	（同时也会关闭使用该网络的域），virNetworkFree函数可以回收一个网络
	（但不会关闭正在运行的域），virNetworkUpdate函数可根据提供的 XML
	格式的网络配置来更新一个已存在的网络。另外，virInterfaceCreate、
	virInterfaceFree、virInterfaceDestroy、virInterfaceGetName、
	virInterfaceIsActive等函数可以用于创建、释放和销毁网络接口，以及查
	询网络接口的名称和激活状态。
*** 存储卷管理的API
	以virStorageVol开头的一系列数函。

	libvirt对存储卷（volume）的管理，主要是对域的镜像文件的管理，这些
	镜像文件可能是 raw、qcow2、vmdk、qed等各种格式。libvirt对存储卷的
	管理，首先需要创建virStorageVolPtr这个存储卷的对象，然后才能对其进
	行查询或控制操作。libvirt提供了3个函数来分别通过不同的方式来获取存
	储卷对象，如：virStorageVolLookupByKey函数可以根据全局唯一的键值来
	获得一个存储卷对象，virStorageVolLookupByName 函数可以根据名称在一
	个存储资源池（storage pool）中获取一个存储卷对象，
	virStorageVolLookupByPath函数可以根据它在节点上路径来获取一个存储
	卷对象。有一些函数用于查询存储卷的信息，如：virStorageVolGetInfo
	函数可以查询某个存储卷的使用情况，virStorageVolGetName函数可以获取
	存储卷的名称，virStorageVolGetPath函数可以获取存储卷的路径，
	virStorageVolGetConnect函数可以查询存储卷的连接。一些函数用于创建
	和修改存储卷，如：virStorageVolCreateXML函数可以根据提供的XML描述
	来创建一个存储卷，virStorageVolFree函数可以释放存储卷的句柄（但是
	存储卷依然存在），virStorageVolDelete函数可以删除一个存储卷，
	virStorageVolResize函数可以调整存储卷的大小。
*** 存储池管理的API
	以virStoragePool开头的一系列函数。

	libvirt对存储池（pool）的管理，包括对本地的基本文件系统、普通网络
	共享文件系统、iSCSI共享文件系统、LVM分区等的管理。libvirt需要基于
	virStoragePoolPtr 这个存储池对象才能进行查询和控制操作。一些函数可
	以通过查询获取一个存储池对象，如：virStoragePoolLookupByName函数可
	以根据存储池的名称来获取一个存储池对象，
	virStoragePoolLookupByVolume可以根据一个存储卷返回其对应的存储池对
	象。virStoragePoolCreateXML函数可以根据XML描述来创建一个存储池（默
	认已激活），virStoragePoolDefineXML函数可以根据XML描述信息静态地定
	义个存储池（尚未激活），virStoragePoolCreate函数可以激活一个存储池。
	virStoragePoolGetInfo、virStoragePoolGetName、
	virStoragePoolGetUUID等函数可以分别获取存储池的信息、名称和UUID标
	识。virStoragePoolIsActive函数可以查询存储池是否处于使用中状态。
	virStoragePoolFree函数可以释放存储池相关的内存（但是不改变其在宿主
	机中的状态），virStoragePoolDestroy函数可以用于销毁一个存储池（但
	并没有释放virStoragePoolPtr对象，之后还可以用virStoragePoolCreate
	函数重新激活它），virStoragePoolDelete函数可以物理删除一个存储池资
	源（该操作不可恢复）。
*** 事件管理的API
	以virEvent开头的一系列函数。

	libvirt支持事件机制，使用该机制注册之后，可以在发生特定的事件（如：
	域的启动、暂停、恢复、停止等）之时，得到自己定义的一些通知。
*** 数据流管理的API	
	以virStream开头的一系列函数。

	libvirt 还提供了一系列函数用于数据流的传输。
** C API示例
   在使用libvirt API之前，必须要在远程或本地的节点上启动libvirtd守护进
   程。在使用libvirt的客户端，先安装libvirt-devel软件包，
   #+BEGIN_SRC sh
   yum install -y libvirt-devel
   #+END_SRC

   编写源代码时，需要在源码的开头引入<libvirt/libvirt.h>头文件。编写完
   毕源代码，如何编译呢，
   #+BEGIN_SRC sh
   gcc test.c -o test -lvirt
   #+END_SRC
*** 例1：显示某个域的信息
	#+BEGIN_SRC sh
[root@kvm01 lavenliu]# cat dominfo.c 
/**
 * Get domain information via libvirt C API.
 * Tested with libvirt-devel- on CentOS6.5 host system
 */

#include <stdio.h>
#include <libvirt/libvirt.h>

int getDomainInfo(int id)
{
    virConnectPtr conn = NULL; /* the hypervisior connection */
    virDomainPtr dom = NULL; /* the domain being checked */
    virDomainInfo info; /* the information being fetched */

    /* NULL means connect to local QEMU/KVM hypervisor*/
    conn = virConnectOpenReadOnly(NULL);
    if (conn == NULL) {
      fprintf(stderr, "Failed to connect to hypervisor\n");
      return 1;
    }

    /* find the Domain by its ID*/
    dom = virDomainLookupByID(conn, id);
    if (dom == NULL) {
      fprintf(stderr, "Failed to find Domain %d\n", id);
      virConnectClose(conn);
      return 1;
    }

    /* Get virDomainInfo structure of the domain */
    if (virDomainGetInfo(dom, &info) < 0) {
      fprintf(stderr, "Failed to get information for Domain %d\n", id);
      virDomainFree(dom);
      virConnectClose(conn);
      return 1;
    }

    /* Print some info of the domain*/
    printf("Domain ID: %d\n", id);
    printf("  vCPUs: %d\n", info.nrVirtCpu);
    printf("  maxMem: %d KB\n", info.maxMem);
    printf("  memory: %d KB\n", info.memory);

    if (dom != NULL)
      virDomainFree(dom);
    if (conn != NULL)
      virConnectClose(conn);

    return 0;
}

int main(int argc, char *argv[])
{

    int dom_id = 5;
    printf("-- Get Domain info by ID via libvirt C API --\n");
    getDomainInfo(dom_id);
    return 0;
}
	#+END_SRC

	编译并运行，
	#+BEGIN_SRC sh
gcc dominfo.c -o dominfo -lvirt
./dominfo
-- Get Domain info by ID via libvirt C API --
Domain ID: 5
  vCPUs: 1
  maxMem: 524288 KB
  memory: 524288 KB
	#+END_SRC
** Python API示例
   许多种编程语言都提供了libvirt的绑定。Python作为一种在Linux上比较流
   行的编程语言，它也提供了libvirt API的绑定。在使用Python调用libvirt
   之前，需要安装libvirt-python软件包，
   #+BEGIN_SRC sh
yum install -y libvirt-python
   #+END_SRC
*** 例1：简单的程序
	#+BEGIN_SRC sh
[root@kvm01 lavenliu]# cat libvirt_test.py 
#!/usr/bin/env python
# coding: utf-8

# Get domain info via libvirt python API
# Tested with python2.6 and libvirt-python-0.10.2 on KVM host.

import libvirt
import sys


def createConnection():
    """
    """
    conn = libvirt.openReadOnly(None)
    if conn is None:
        print 'Failed to open connection to QEMU/KVM'
        sys.exit(1)
    else:
        print '-- Connection is created successfully --'
    return conn


def closeConnection(conn):
    """
    Arguments:
    - `conn`:
    """
    print
    try:
        conn.close()
    except:
        print 'Failed to close the connection'
        return 1
    print 'Connection is closed'


def getDomInfoByName(conn, name):
    """
    Arguments:
    - `conn`:
    - `NameError`:
    """
    print
    print '---- get domain info by name ----'
    
    try:
        myDom = conn.lookupByName(name)
    except:
        print 'Failed to find the domain with name "%s"' % name
        return 1
    
    print "Dom id: %d name: %s" % (myDom.ID(), myDom.name())
    print "Dom state: %s" % myDom.state(0)
    print "Dom info: %s" % myDom.info()
    print "memory: %d MB" % (myDom.maxMemory() / 1024)
    print "memory status: %s" % myDom.memoryStats()
    print "vCPUs: %d" % myDom.maxVcpus()


def getDomInfoByID(conn, id):
    """
    Arguments:
    - `conn`:
    - `id`:
    """
    print
    print '---- get domain info by ID ----'
    try:
        myDom = conn.lookupByID(id)
    except:
        print 'Failed to find the domain with ID "%d"' % id
        return 1
    print "Domain id is %d; Name is %s" % (myDom.ID(), myDom.name())


if __name__ == '__main__':
    name1 = "kvm-demo"
    name2 = "new"
    id1 = 5
    id2 = 6
    print '-- Get domain info via libvirt python API --'
    conn = createConnection()
    getDomInfoByName(conn, name1)
    getDomInfoByName(conn, name2)
    getDomInfoByID(conn, id1)
    getDomInfoByID(conn, id2)
    closeConnection(conn)
	#+END_SRC
* Misc
  VirtualBox虚拟机镜像文件转换为KVM格式，首先必须安装VirtualBox软件，
  并且要有相应的内核开发包，这样才可以正常启动。先将VirtualBox格式转换
  为普通的RAW格式，注意硬盘空间，由VirtualBox的镜像转换为RAW格式后，容
  量一般是原来的1.5倍左右。
  #+BEGIN_SRC sh
VBoxManage cloned -format RAW /path/to/xxx.vdi /path/to/xxx.raw
  #+END_SRC

  再将RAW格式转换为QCOW2格式，
  #+BEGIN_SRC sh
qemu-img convert -f raw /path/to/xxx.raw -O qcow2 /path/to/xxx.qcow2
  #+END_SRC

  测试虚拟机，
  #+BEGIN_SRC sh
kvm -m 1024 -usbdevice tablet -hda /path/to/xxx.qcow2
  #+END_SRC

  KVM虚拟机网卡与物理机网卡桥接

由于物理机网卡启用VLAN模式，因此需要针对不同VLAN设置桥接。关于Linux下面单网卡启用802.1Q以支持多个VLAN的知识，请参加其它相关文章。这里以VLAN 2为例
ifconfig eth0.2 0.0.0.0 promisc up

执行这条命令之前，请先记录eth0.2的相关IP设置。此条命令将启用物理网卡VLAN 2混杂工作模式，使其监听接收交换机发送来的所有的数据包，不论目的地是否是自己。但不用担心网卡的工作效率，因为交换机或者路由器在缓存了MAC地址表后，只会把与该机相关的数据包才转发到对应端口。 值得注意的是，这一步很重要，而当使用无线网卡时，还必须确认其自身是否支持网卡混杂模式。
brctl addbr br0

建立桥接设备br0, brctl命令由bridge-utils包提供。
brctl addif br0 eth0.2

将br0设备与eth0.2关联。
ifconfig br0 up

激活桥接设备br0
brctl stp br0 on

启用桥接设备br0生成树协议
ifconfig br0 10.0.0.20 netmask 255.0.0.0 up

将之前应用在eth0.2的相关IP设置，重新应用在网桥br0
route add default gw 10.0.0.2 br0

添加静态路由，重新设置默认网关。

重新启动libvirtd服务，打开virtual machine manager就可以对虚拟机设置桥接网卡了。
* Problems
  # /etc/init.d/libvirtd start
  Starting libvirtd daemon: 2016-04-29 06:23:39.512+0000: 1189: info : libvirt version: 0.10.2, package: 54.el6_7.6 (CentOS BuildSystem <http://bugs.centos.org>, 2016-03-22-16:55:18, c6b8.bsys.dev.centos.org)
  2016-04-29 06:23:39.512+0000: 1189: warning : virGetHostname:2279 : getaddrinfo failed for 'basic': Name or service not known
